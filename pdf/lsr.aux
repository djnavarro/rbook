\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{Preface}{ix}{chapter*.1}}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Background}{1}{part.1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Why do we learn statistics?\nobreakspace  {}}{3}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:whystats}{{1}{3}{Why do we learn statistics?~}{chapter.1}{}}
\newlabel{sec:whywhywhy}{{1.1}{3}{On the psychology of statistics~\label {sec:whywhywhy}}{section.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}On the psychology of statistics\nobreakspace  {}}{3}{section.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}The curse of belief bias}{4}{subsection.1.1.1}}
\citation{Evans1983}
\@writefile{brf}{\backcite{Evans1983}{{5}{1.1}{subsection.1.1.1}}}
\@writefile{brf}{\backcite{Evans1983}{{5}{1.1}{subsection.1.1.1}}}
\@writefile{brf}{\backcite{Evans1983}{{5}{1.1}{subsection.1.1.1}}}
\citation{Bickel1975}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}The cautionary tale of Simpson's paradox}{6}{section.1.2}}
\@writefile{brf}{\backcite{Bickel1975}{{6}{1.2}{section.1.2}}}
\@writefile{brf}{\backcite{Bickel1975}{{6}{1.2}{section.1.2}}}
\@writefile{brf}{\backcite{Bickel1975}{{6}{1.2}{section.1.2}}}
\citation{Bickel1975}
\citation{Bickel1975}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces The Berkeley 1973 college admissions data. This figure plots the admission rate for the 85 departments that had at least one female applicant, as a function of the percentage of applicants that were female. The plot is a redrawing of Figure 1 from \citeA {Bickel1975}. Circles plot departments with more than 40 applicants; the area of the circle is proportional to the total number of applicants. The crosses plot department with fewer than 40 applicants.}}{7}{figure.1.1}}
\@writefile{brf}{\backcite{Bickel1975}{{7}{1.1}{figure.1.1}}}
\@writefile{brf}{\backcite{Bickel1975}{{7}{1.1}{figure.1.1}}}
\@writefile{brf}{\backcite{Bickel1975}{{7}{1.1}{figure.1.1}}}
\newlabel{fig:berkeley}{{1.1}{7}{The Berkeley 1973 college admissions data. This figure plots the admission rate for the 85 departments that had at least one female applicant, as a function of the percentage of applicants that were female. The plot is a redrawing of Figure 1 from \protect \citeA {Bickel1975}. Circles plot departments with more than 40 applicants; the area of the circle is proportional to the total number of applicants. The crosses plot department with fewer than 40 applicants}{figure.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Statistics in psychology}{8}{section.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Statistics in everyday life}{9}{section.1.4}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}There's more to research methods than statistics}{10}{section.1.5}}
\citation{Campbell1963}
\citation{Stevens1946}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}A brief introduction to research design}{11}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:studydesign}{{2}{11}{A brief introduction to research design}{chapter.2}{}}
\@writefile{brf}{\backcite{Campbell1963}{{11}{2}{chapter.2}}}
\@writefile{brf}{\backcite{Campbell1963}{{11}{2}{chapter.2}}}
\@writefile{brf}{\backcite{Campbell1963}{{11}{2}{chapter.2}}}
\@writefile{brf}{\backcite{Stevens1946}{{11}{2}{chapter.2}}}
\@writefile{brf}{\backcite{Stevens1946}{{11}{2}{chapter.2}}}
\@writefile{brf}{\backcite{Stevens1946}{{11}{2}{chapter.2}}}
\newlabel{sec:measurement}{{2.1}{11}{Introduction to psychological measurement~\label {sec:measurement}}{section.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction to psychological measurement\nobreakspace  {}}{11}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Some thoughts about psychological measurement}{11}{subsection.2.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Operationalisation: defining your measurement}{13}{subsection.2.1.2}}
\newlabel{sec:scales}{{2.2}{14}{Scales of measurement\label {sec:scales}}{section.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Scales of measurement}{14}{section.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Nominal scale}{14}{subsection.2.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Ordinal scale}{15}{subsection.2.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Interval scale}{16}{subsection.2.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Ratio scale}{16}{subsection.2.2.4}}
\newlabel{sec:continuousdiscrete}{{2.2.5}{16}{Continuous versus discrete variables~\label {sec:continuousdiscrete}}{subsection.2.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Continuous versus discrete variables\nobreakspace  {}}{16}{subsection.2.2.5}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces The relationship between the scales of measurement and the discrete/continuity distinction. Cells with a tick mark correspond to things that are possible.}}{17}{table.2.1}}
\newlabel{tab:scalescont}{{2.1}{17}{The relationship between the scales of measurement and the discrete/continuity distinction. Cells with a tick mark correspond to things that are possible}{table.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.6}Some complexities}{17}{subsection.2.2.6}}
\newlabel{sec:reliability}{{2.3}{18}{Assessing the reliability of a measurement~\label {sec:reliability}}{section.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Assessing the reliability of a measurement\nobreakspace  {}}{18}{section.2.3}}
\newlabel{sec:ivdv}{{2.4}{19}{The ``role'' of variables: predictors and outcomes \label {sec:ivdv}}{section.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}The ``role'' of variables: predictors and outcomes }{19}{section.2.4}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces The terminology used to distinguish between different roles that a variable can play when analysing a data set. Note that this book will tend to avoid the classical terminology in favour of the newer names.}}{20}{table.2.2}}
\newlabel{tab:ivdv}{{2.2}{20}{The terminology used to distinguish between different roles that a variable can play when analysing a data set. Note that this book will tend to avoid the classical terminology in favour of the newer names}{table.2.2}{}}
\newlabel{sec:researchdesigns}{{2.5}{20}{Experimental and non-experimental research~\label {sec:researchdesigns}}{section.2.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Experimental and non-experimental research\nobreakspace  {}}{20}{section.2.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Experimental research}{20}{subsection.2.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Non-experimental research}{21}{subsection.2.5.2}}
\newlabel{sec:validity}{{2.6}{22}{Assessing the validity of a study~\label {sec:validity}}{section.2.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Assessing the validity of a study\nobreakspace  {}}{22}{section.2.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Internal validity}{22}{subsection.2.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}External validity}{23}{subsection.2.6.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}Construct validity}{23}{subsection.2.6.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.4}Face validity}{24}{subsection.2.6.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.5}Ecological validity}{24}{subsection.2.6.5}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Confounds, artifacts and other threats to validity}{25}{section.2.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.1}History effects}{26}{subsection.2.7.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.2}Maturation effects}{26}{subsection.2.7.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.3}Repeated testing effects}{27}{subsection.2.7.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.4}Selection bias}{27}{subsection.2.7.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.5}Differential attrition}{27}{subsection.2.7.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.6}Non-response bias}{28}{subsection.2.7.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.7}Regression to the mean}{28}{subsection.2.7.7}}
\citation{Kahneman1973}
\citation{Pfungst1911}
\citation{Hothersall2004}
\@writefile{brf}{\backcite{Kahneman1973}{{29}{2.7}{subsection.2.7.7}}}
\@writefile{brf}{\backcite{Kahneman1973}{{29}{2.7}{subsection.2.7.7}}}
\@writefile{brf}{\backcite{Kahneman1973}{{29}{2.7}{subsection.2.7.7}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.8}Experimenter bias}{29}{subsection.2.7.8}}
\@writefile{brf}{\backcite{Pfungst1911}{{29}{2.7}{subsection.2.7.8}}}
\@writefile{brf}{\backcite{Pfungst1911}{{29}{2.7}{subsection.2.7.8}}}
\@writefile{brf}{\backcite{Pfungst1911}{{29}{2.7}{subsection.2.7.8}}}
\@writefile{brf}{\backcite{Hothersall2004}{{29}{2.7}{subsection.2.7.8}}}
\@writefile{brf}{\backcite{Hothersall2004}{{29}{2.7}{subsection.2.7.8}}}
\@writefile{brf}{\backcite{Hothersall2004}{{29}{2.7}{subsection.2.7.8}}}
\citation{Rosenthal1966}
\citation{Adair1984}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.9}Demand effects and reactivity}{30}{subsection.2.7.9}}
\@writefile{brf}{\backcite{Rosenthal1966}{{30}{2.7}{subsection.2.7.9}}}
\@writefile{brf}{\backcite{Rosenthal1966}{{30}{2.7}{subsection.2.7.9}}}
\@writefile{brf}{\backcite{Rosenthal1966}{{30}{2.7}{subsection.2.7.9}}}
\@writefile{brf}{\backcite{Adair1984}{{30}{2.7}{subsection.2.7.9}}}
\@writefile{brf}{\backcite{Adair1984}{{30}{2.7}{subsection.2.7.9}}}
\@writefile{brf}{\backcite{Adair1984}{{30}{2.7}{subsection.2.7.9}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.10}Placebo effects}{30}{subsection.2.7.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.11}Situation, measurement and subpopulation effects}{31}{subsection.2.7.11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.12}Fraud, deception and self-deception}{31}{subsection.2.7.12}}
\citation{Ioannidis2005}
\citation{Kuhberger2014}
\citation{Campbell1963}
\@writefile{brf}{\backcite{Ioannidis2005}{{33}{2.7}{subsection.2.7.12}}}
\@writefile{brf}{\backcite{Ioannidis2005}{{33}{2.7}{subsection.2.7.12}}}
\@writefile{brf}{\backcite{Ioannidis2005}{{33}{2.7}{subsection.2.7.12}}}
\@writefile{brf}{\backcite{Kuhberger2014}{{33}{2.7}{subsection.2.7.12}}}
\@writefile{brf}{\backcite{Kuhberger2014}{{33}{2.7}{subsection.2.7.12}}}
\@writefile{brf}{\backcite{Kuhberger2014}{{33}{2.7}{subsection.2.7.12}}}
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Summary}{33}{section.2.8}}
\@writefile{brf}{\backcite{Campbell1963}{{33}{2.8}{section.2.8}}}
\@writefile{brf}{\backcite{Campbell1963}{{33}{2.8}{section.2.8}}}
\@writefile{brf}{\backcite{Campbell1963}{{33}{2.8}{section.2.8}}}
\@writefile{toc}{\contentsline {part}{II\hspace  {1em}An introduction to \textsf  {R}}{35}{part.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Getting started with \textsf  {R}\nobreakspace  {}}{37}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:introR}{{3}{37}{Getting started with \R ~}{chapter.3}{}}
\newlabel{sec:gettingR}{{3.1}{38}{Installing \R ~\label {sec:gettingR}}{section.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Installing \textsf  {R}\nobreakspace  {}}{38}{section.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Installing \textsf  {R}\ on a Windows computer}{39}{subsection.3.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Installing \textsf  {R}\ on a Mac}{39}{subsection.3.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Installing \textsf  {R}\ on a Linux computer}{39}{subsection.3.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Downloading and installing Rstudio}{39}{subsection.3.1.4}}
\newlabel{sec:startingR}{{3.1.5}{40}{Starting up \R ~\label {sec:startingR}}{subsection.3.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}Starting up \textsf  {R}\nobreakspace  {}}{40}{subsection.3.1.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces An R session in progress running through Rstudio. The picture shows Rstudio running on a Mac, but the Windows interface is almost identical.}}{41}{figure.3.1}}
\newlabel{fig:rstudio}{{3.1}{41}{An R session in progress running through Rstudio. The picture shows Rstudio running on a Mac, but the Windows interface is almost identical}{figure.3.1}{}}
\newlabel{sec:firstcommand}{{3.2}{42}{Typing commands at the \R \ console\label {sec:firstcommand}}{section.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Typing commands at the \textsf  {R}\ console}{42}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Be very careful to avoid typos}{42}{subsection.3.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}\textsf  {R}\ is (a bit) flexible with spacing}{43}{subsection.3.2.2}}
\citation{R2013}
\@writefile{brf}{\backcite{R2013}{{44}{3.2}{subsection.3.2.2}}}
\@writefile{brf}{\backcite{R2013}{{44}{3.2}{subsection.3.2.2}}}
\@writefile{brf}{\backcite{R2013}{{44}{3.2}{subsection.3.2.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}\textsf  {R}\ can sometimes tell that you're not finished yet (but not often)}{44}{subsection.3.2.3}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Basic arithmetic operations in \textsf  {R}. These five operators are used very frequently throughout the text, so it's important to be familiar with them at the outset. There are others as well, which I'll discuss in Chapter\nobreakspace  {}\ref  {ch:datahandling}.}}{46}{table.3.1}}
\newlabel{tab:arithmetic1}{{3.1}{46}{Basic arithmetic operations in \R . These five operators are used very frequently throughout the text, so it's important to be familiar with them at the outset. There are others as well, which I'll discuss in Chapter~\ref {ch:datahandling}}{table.3.1}{}}
\newlabel{sec:arithmetic}{{3.3}{46}{Doing simple calculations with \R ~\label {sec:arithmetic}}{section.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Doing simple calculations with \textsf  {R}\nobreakspace  {}}{46}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Adding, subtracting, multiplying and dividing}{46}{subsection.3.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Taking powers}{46}{subsection.3.3.2}}
\newlabel{sec:bedmas}{{3.3.3}{47}{Doing calculations in the right order\label {sec:bedmas}}{subsection.3.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Doing calculations in the right order}{47}{subsection.3.3.3}}
\newlabel{sec:assign}{{3.4}{48}{Storing a number as a variable~\label {sec:assign}}{section.3.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Storing a number as a variable\nobreakspace  {}}{48}{section.3.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Variable assignment using \texttt  {\relax \fontsize  {10.95}{14}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}<-}} and \texttt  {\relax \fontsize  {10.95}{14}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}->}}}{48}{subsection.3.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Doing calculations using variables}{49}{subsection.3.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Rules and conventions for naming variables}{50}{subsection.3.4.3}}
\newlabel{sec:usingfunctions}{{3.5}{51}{Using functions to do calculations\label {sec:usingfunctions}}{section.3.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Using functions to do calculations}{51}{section.3.5}}
\newlabel{sec:functionarguments}{{3.5.1}{53}{Function arguments, their names and their defaults~\label {sec:functionarguments}}{subsection.3.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Function arguments, their names and their defaults\nobreakspace  {}}{53}{subsection.3.5.1}}
\newlabel{sec:Rstudio1}{{3.6}{54}{Letting Rstudio help you with your commands\label {sec:Rstudio1}}{section.3.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Letting Rstudio help you with your commands}{54}{section.3.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Start typing the name of a function or a variable, and hit the ``tab'' key. Rstudio brings up a little dialog box like this one that lets you select the one you want, and even prints out a little information about it.}}{55}{figure.3.2}}
\newlabel{fig:Rstudiotab}{{3.2}{55}{Start typing the name of a function or a variable, and hit the ``tab'' key. Rstudio brings up a little dialog box like this one that lets you select the one you want, and even prints out a little information about it}{figure.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Autocomplete using ``tab''}{55}{subsection.3.6.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces If you've typed the name of a function already along with the left parenthesis and then hit the ``tab'' key, Rstudio brings up a different window to the one shown in Figure\nobreakspace  {}\ref {fig:Rstudiotab}. This one lists all the arguments to the function on the left, and information about each argument on the right.}}{56}{figure.3.3}}
\newlabel{fig:Rstudiotab2}{{3.3}{56}{If you've typed the name of a function already along with the left parenthesis and then hit the ``tab'' key, Rstudio brings up a different window to the one shown in Figure~\protect \ref {fig:Rstudiotab}. This one lists all the arguments to the function on the left, and information about each argument on the right}{figure.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Browsing your command history}{56}{subsection.3.6.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces The history panel is located in the top right hand side of the Rstudio window. Click on the word ``History'' and it displays this panel. }}{57}{figure.3.4}}
\newlabel{fig:Rstudiohistory}{{3.4}{57}{The history panel is located in the top right hand side of the Rstudio window. Click on the word ``History'' and it displays this panel}{figure.3.4}{}}
\newlabel{sec:vectors}{{3.7}{57}{Storing many numbers as a vector~\label {sec:vectors}}{section.3.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Storing many numbers as a vector\nobreakspace  {}}{57}{section.3.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.1}Creating a vector}{57}{subsection.3.7.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.2}A handy digression}{58}{subsection.3.7.2}}
\newlabel{sec:vectorsubset}{{3.7.3}{58}{Getting information out of vectors\label {sec:vectorsubset}}{subsection.3.7.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.3}Getting information out of vectors}{58}{subsection.3.7.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.4}Altering the elements of a vector}{59}{subsection.3.7.4}}
\newlabel{sec:veclength}{{3.7.5}{59}{Useful things to know about vectors~\label {sec:veclength}}{subsection.3.7.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.5}Useful things to know about vectors\nobreakspace  {}}{59}{subsection.3.7.5}}
\newlabel{sec:text}{{3.8}{60}{Storing text data\label {sec:text}}{section.3.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Storing text data}{60}{section.3.8}}
\newlabel{sec:simpletext}{{3.8.1}{61}{Working with text~\label {sec:simpletext}}{subsection.3.8.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.1}Working with text\nobreakspace  {}}{61}{subsection.3.8.1}}
\newlabel{sec:logicals}{{3.9}{61}{Storing ``true or false'' data\label {sec:logicals}}{section.3.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.9}Storing ``true or false'' data}{61}{section.3.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.1}Assessing mathematical truths}{61}{subsection.3.9.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.2}Logical operations}{62}{subsection.3.9.2}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Some logical operators. Technically I should be calling these ``binary relational operators'', but quite frankly I don't want to. It's my book so no-one can make me.}}{63}{table.3.2}}
\newlabel{tab:logicals}{{3.2}{63}{Some logical operators. Technically I should be calling these ``binary relational operators'', but quite frankly I don't want to. It's my book so no-one can make me}{table.3.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Some more logical operators.}}{64}{table.3.3}}
\newlabel{tab:logicals2}{{3.3}{64}{Some more logical operators}{table.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.3}Storing and using logical data}{64}{subsection.3.9.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.4}Vectors of logicals}{65}{subsection.3.9.4}}
\newlabel{sec:logictext}{{3.9.5}{66}{Applying logical operation to text~\label {sec:logictext}}{subsection.3.9.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.5}Applying logical operation to text\nobreakspace  {}}{66}{subsection.3.9.5}}
\newlabel{sec:indexing}{{3.10}{66}{Indexing vectors~\label {sec:indexing}}{section.3.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.10}Indexing vectors\nobreakspace  {}}{66}{section.3.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.10.1}Extracting multiple elements}{66}{subsection.3.10.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.10.2}Logical indexing}{67}{subsection.3.10.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The dialog box that shows up when you try to close Rstudio.}}{69}{figure.3.5}}
\newlabel{fig:quitR}{{3.5}{69}{The dialog box that shows up when you try to close Rstudio}{figure.3.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.11}Quitting \textsf  {R}}{69}{section.3.11}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces The options window in Rstudio. On a Mac, you can open this window by going to the ``Rstudio'' menu and selecting ``Preferences''. On a Windows machine you go to the ``Tools'' menu and select ``Global Options''.}}{70}{figure.3.6}}
\newlabel{fig:Rstudiooptions}{{3.6}{70}{The options window in Rstudio. On a Mac, you can open this window by going to the ``Rstudio'' menu and selecting ``Preferences''. On a Windows machine you go to the ``Tools'' menu and select ``Global Options''}{figure.3.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.12}Summary}{70}{section.3.12}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Additional \textsf  {R}\ concepts\nobreakspace  {}}{73}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:mechanics}{{4}{73}{Additional \R \ concepts~}{chapter.4}{}}
\newlabel{sec:comments}{{4.1}{73}{Using comments~\label {sec:comments}}{section.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Using comments\nobreakspace  {}}{73}{section.4.1}}
\newlabel{sec:packageinstall}{{4.2}{74}{Installing and loading packages~\label {sec:packageinstall}}{section.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Installing and loading packages\nobreakspace  {}}{74}{section.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}The package panel in Rstudio}{74}{subsection.4.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces The packages panel.}}{75}{figure.4.1}}
\newlabel{fig:packagepanel}{{4.1}{75}{The packages panel}{figure.4.1}{}}
\newlabel{sec:packageload}{{4.2.2}{75}{Loading a package\label {sec:packageload}}{subsection.4.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Loading a package}{75}{subsection.4.2.2}}
\newlabel{sec:packageunload}{{4.2.3}{76}{Unloading a package\label {sec:packageunload}}{subsection.4.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Unloading a package}{76}{subsection.4.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}A few extra comments}{77}{subsection.4.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Downloading new packages}{78}{subsection.4.2.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.6}Updating \textsf  {R}\ and \textsf  {R}\ packages}{78}{subsection.4.2.6}}
\citation{Fox2011}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces The package installation dialog box in Rstudio (panel a). When you start typing, you'll see a dropdown menu suggest a list of possible packages that you might want to install (panel b)}}{79}{figure.4.2}}
\newlabel{fig:packageinstall}{{4.2}{79}{The package installation dialog box in Rstudio (panel a). When you start typing, you'll see a dropdown menu suggest a list of possible packages that you might want to install (panel b)}{figure.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.7}What packages does this book use?}{79}{subsection.4.2.7}}
\@writefile{brf}{\backcite{Fox2011}{{79}{4.2}{subsection.4.2.7}}}
\@writefile{brf}{\backcite{Fox2011}{{79}{4.2}{subsection.4.2.7}}}
\@writefile{brf}{\backcite{Fox2011}{{79}{4.2}{subsection.4.2.7}}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces The Rstudio dialog box for updating packages.}}{80}{figure.4.3}}
\newlabel{fig:updatepackages}{{4.3}{80}{The Rstudio dialog box for updating packages}{figure.4.3}{}}
\newlabel{sec:workspace}{{4.3}{80}{Managing the workspace\label {sec:workspace}}{section.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Managing the workspace}{80}{section.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Listing the contents of the workspace}{80}{subsection.4.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces The Rstudio ``Environment'' panel shows you the contents of the workspace. The view shown above is the ``list'' view. To switch to the grid view, click on the menu item on the top right that currently reads ``list''. Select ``grid'' from the dropdown menu, and then it will switch to a view like the one shown in Figure\nobreakspace  {}\ref {fig:workspace2}.}}{81}{figure.4.4}}
\newlabel{fig:workspace}{{4.4}{81}{The Rstudio ``Environment'' panel shows you the contents of the workspace. The view shown above is the ``list'' view. To switch to the grid view, click on the menu item on the top right that currently reads ``list''. Select ``grid'' from the dropdown menu, and then it will switch to a view like the one shown in Figure~\protect \ref {fig:workspace2}}{figure.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces The Rstudio ``Environment'' panel shows you the contents of the workspace. Compare this ``grid'' view to the ``list'' view in Figure\nobreakspace  {}\ref {fig:workspace}}}{82}{figure.4.5}}
\newlabel{fig:workspace2}{{4.5}{82}{The Rstudio ``Environment'' panel shows you the contents of the workspace. Compare this ``grid'' view to the ``list'' view in Figure~\protect \ref {fig:workspace}}{figure.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Removing variables from the workspace}{82}{subsection.4.3.2}}
\newlabel{sec:navigation}{{4.4}{83}{Navigating the file system\label {sec:navigation}}{section.4.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Navigating the file system}{83}{section.4.4}}
\newlabel{sec:filesystem}{{4.4.1}{83}{The file system itself~\label {sec:filesystem}}{subsection.4.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}The file system itself\nobreakspace  {}}{83}{subsection.4.4.1}}
\newlabel{sec:navigationR}{{4.4.2}{84}{Navigating the file system using the \R \ console~\label {sec:navigationR}}{subsection.4.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Navigating the file system using the \textsf  {R}\ console\nobreakspace  {}}{84}{subsection.4.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Why do the Windows paths use the wrong slash?}{85}{subsection.4.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces The ``file panel'' is the area shown in the lower right hand corner. It provides a very easy way to browse and navigate your computer using \textsf  {R}. See main text for details.}}{86}{figure.4.6}}
\newlabel{fig:filepanel}{{4.6}{86}{The ``file panel'' is the area shown in the lower right hand corner. It provides a very easy way to browse and navigate your computer using \R . See main text for details}{figure.4.6}{}}
\newlabel{sec:nav3}{{4.4.4}{86}{Navigating the file system using the Rstudio file panel\label {sec:nav3}}{subsection.4.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Navigating the file system using the Rstudio file panel}{86}{subsection.4.4.4}}
\newlabel{sec:load}{{4.5}{87}{Loading and saving data\label {sec:load}}{section.4.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Loading and saving data}{87}{section.4.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Loading workspace files using \textsf  {R}}{87}{subsection.4.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Loading workspace files using Rstudio}{88}{subsection.4.5.2}}
\newlabel{sec:loadingcsv}{{4.5.3}{88}{Importing data from CSV files using \R \label {sec:loadingcsv}}{subsection.4.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Importing data from CSV files using \textsf  {R}}{88}{subsection.4.5.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces The \texttt  {booksales.csv} data file. On the left, I've opened the file in using a spreadsheet program (OpenOffice), which shows that the file is basically a table. On the right, the same file is open in a standard text editor (the TextEdit program on a Mac), which shows how the file is formatted. The entries in the table are wrapped in quote marks and separated by commas.}}{89}{figure.4.7}}
\newlabel{fig:booksalescsv}{{4.7}{89}{The \filename {booksales.csv} data file. On the left, I've opened the file in using a spreadsheet program (OpenOffice), which shows that the file is basically a table. On the right, the same file is open in a standard text editor (the TextEdit program on a Mac), which shows how the file is formatted. The entries in the table are wrapped in quote marks and separated by commas}{figure.4.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.4}Importing data from CSV files using Rstudio}{90}{subsection.4.5.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces A dialog box on a Mac asking you to select the CSV file \textsf  {R}\ should try to import. Mac users will recognise this immediately: it's the usual way in which a Mac asks you to find a file. Windows users won't see this: they'll see the usual explorer window that Windows always gives you when it wants you to select a file.}}{91}{figure.4.8}}
\newlabel{fig:fileopen}{{4.8}{91}{A dialog box on a Mac asking you to select the CSV file \R \ should try to import. Mac users will recognise this immediately: it's the usual way in which a Mac asks you to find a file. Windows users won't see this: they'll see the usual explorer window that Windows always gives you when it wants you to select a file}{figure.4.8}{}}
\newlabel{sec:save}{{4.5.5}{91}{Saving a workspace file using \R \label {sec:save}}{subsection.4.5.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.5}Saving a workspace file using \textsf  {R}}{91}{subsection.4.5.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces The Rstudio window for importing a CSV file into \textsf  {R}.}}{92}{figure.4.9}}
\newlabel{fig:import}{{4.9}{92}{The Rstudio window for importing a CSV file into \R }{figure.4.9}{}}
\newlabel{sec:save1}{{4.5.6}{92}{Saving a workspace file using Rstudio\label {sec:save1}}{subsection.4.5.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.6}Saving a workspace file using Rstudio}{92}{subsection.4.5.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.7}Other things you might want to save}{93}{subsection.4.5.7}}
\newlabel{sec:useful}{{4.6}{93}{Useful things to know about variables~\label {sec:useful}}{section.4.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Useful things to know about variables\nobreakspace  {}}{93}{section.4.6}}
\newlabel{sec:specials}{{4.6.1}{94}{Special values\label {sec:specials}}{subsection.4.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Special values}{94}{subsection.4.6.1}}
\newlabel{sec:names}{{4.6.2}{94}{Assigning names to vector elements~\label {sec:names}}{subsection.4.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}Assigning names to vector elements\nobreakspace  {}}{94}{subsection.4.6.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.3}Variable classes}{95}{subsection.4.6.3}}
\newlabel{sec:factors}{{4.7}{97}{Factors\label {sec:factors}}{section.4.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Factors}{97}{section.4.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.1}Introducing factors}{98}{subsection.4.7.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.2}Labelling the factor levels}{98}{subsection.4.7.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.3}Moving on...}{99}{subsection.4.7.3}}
\newlabel{sec:dataframes}{{4.8}{100}{Data frames\label {sec:dataframes}}{section.4.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Data frames}{100}{section.4.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.1}Introducing data frames}{100}{subsection.4.8.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.2}Pulling out the contents of the data frame using \texttt  {\relax \fontsize  {10.95}{14}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}\$}}}{101}{subsection.4.8.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.3}Getting information about a data frame}{101}{subsection.4.8.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.4}Looking for more on data frames?}{102}{subsection.4.8.4}}
\newlabel{sec:lists}{{4.9}{102}{Lists~\label {sec:lists}}{section.4.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.9}Lists\nobreakspace  {}}{102}{section.4.9}}
\newlabel{sec:formulas}{{4.10}{103}{Formulas\label {sec:formulas}}{section.4.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.10}Formulas}{103}{section.4.10}}
\newlabel{sec:generics}{{4.11}{104}{Generic functions~\label {sec:generics}}{section.4.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.11}Generic functions\nobreakspace  {}}{104}{section.4.11}}
\newlabel{sec:help}{{4.12}{105}{Getting help~\label {sec:help}}{section.4.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.12}Getting help\nobreakspace  {}}{105}{section.4.12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.1}How to read the help documentation}{105}{subsection.4.12.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.2}Other resources}{108}{subsection.4.12.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4.13}Summary}{109}{section.4.13}}
\@writefile{toc}{\contentsline {part}{III\hspace  {1em}Working with data}{111}{part.3}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Descriptive statistics}{113}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:descriptives}{{5}{113}{Descriptive statistics}{chapter.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces A histogram of the AFL 2010 winning margin data (the \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}afl.margins}} variable). As you might expect, the larger the margin the less frequently you tend to see it.}}{114}{figure.5.1}}
\newlabel{fig:histogram1}{{5.1}{114}{A histogram of the AFL 2010 winning margin data (the \rtext {afl.margins} variable). As you might expect, the larger the margin the less frequently you tend to see it}{figure.5.1}{}}
\newlabel{sec:centraltendency}{{5.1}{114}{Measures of central tendency~\label {sec:centraltendency}}{section.5.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Measures of central tendency\nobreakspace  {}}{114}{section.5.1}}
\newlabel{sec:mean}{{5.1.1}{114}{The mean\label {sec:mean}}{subsection.5.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}The mean}{114}{subsection.5.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Calculating the mean in \textsf  {R}}{116}{subsection.5.1.2}}
\newlabel{sec:median}{{5.1.3}{117}{The median\label {sec:median}}{subsection.5.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}The median}{117}{subsection.5.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces An illustration of the difference between how the mean and the median should be interpreted. The mean is basically the ``centre of gravity'' of the data set: if you imagine that the histogram of the data is a solid object, then the point on which you could balance it (as if on a see-saw) is the mean. In contrast, the median is the middle observation. Half of the observations are smaller, and half of the observations are larger.}}{118}{figure.5.2}}
\newlabel{fig:meanmedian}{{5.2}{118}{An illustration of the difference between how the mean and the median should be interpreted. The mean is basically the ``centre of gravity'' of the data set: if you imagine that the histogram of the data is a solid object, then the point on which you could balance it (as if on a see-saw) is the mean. In contrast, the median is the middle observation. Half of the observations are smaller, and half of the observations are larger}{figure.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.4}Mean or median? What's the difference?}{118}{subsection.5.1.4}}
\newlabel{sec:housingpriceexample}{{5.1.5}{119}{A real life example~\label {sec:housingpriceexample}}{subsection.5.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.5}A real life example\nobreakspace  {}}{119}{subsection.5.1.5}}
\newlabel{sec:trimmedmean}{{5.1.6}{120}{Trimmed mean\label {sec:trimmedmean}}{subsection.5.1.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.6}Trimmed mean}{120}{subsection.5.1.6}}
\newlabel{sec:mode}{{5.1.7}{121}{Mode\label {sec:mode}}{subsection.5.1.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.7}Mode}{121}{subsection.5.1.7}}
\newlabel{sec:var}{{5.2}{123}{Measures of variability\label {sec:var}}{section.5.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Measures of variability}{123}{section.5.2}}
\newlabel{sec:range}{{5.2.1}{123}{Range\label {sec:range}}{subsection.5.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Range}{123}{subsection.5.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Interquartile range}{123}{subsection.5.2.2}}
\newlabel{sec:aad}{{5.2.3}{124}{Mean absolute deviation~\label {sec:aad}}{subsection.5.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Mean absolute deviation\nobreakspace  {}}{124}{subsection.5.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.4}Variance}{125}{subsection.5.2.4}}
\newlabel{sec:sd}{{5.2.5}{128}{Standard deviation\label {sec:sd}}{subsection.5.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.5}Standard deviation}{128}{subsection.5.2.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces An illustration of the standard deviation, applied to the AFL winning margins data. The shaded bars in the histogram show how much of the data fall within one standard deviation of the mean. In this case, 65.3\% of the data set lies within this range, which is pretty consistent with the ``approximately 68\% rule'' discussed in the main text.}}{129}{figure.5.3}}
\newlabel{fig:aflsd}{{5.3}{129}{An illustration of the standard deviation, applied to the AFL winning margins data. The shaded bars in the histogram show how much of the data fall within one standard deviation of the mean. In this case, 65.3\% of the data set lies within this range, which is pretty consistent with the ``approximately 68\% rule'' discussed in the main text}{figure.5.3}{}}
\newlabel{sec:mad}{{5.2.6}{129}{Median absolute deviation\label {sec:mad} \advanced }{subsection.5.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.6}Median absolute deviation }{129}{subsection.5.2.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.7}Which measure to use?}{130}{subsection.5.2.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces An illustration of skewness. On the left we have a negatively skewed data set (skewness $= -.93$), in the middle we have a data set with no skew (technically, skewness $= -.006$), and on the right we have a positively skewed data set (skewness $= .93$). }}{131}{figure.5.4}}
\newlabel{fig:skewness}{{5.4}{131}{An illustration of skewness. On the left we have a negatively skewed data set (skewness $= -.93$), in the middle we have a data set with no skew (technically, skewness $= -.006$), and on the right we have a positively skewed data set (skewness $= .93$)}{figure.5.4}{}}
\newlabel{sec:skew}{{5.3}{131}{Skew and kurtosis \label {sec:skew}\label {sec:kurtosis}}{section.5.3}{}}
\newlabel{sec:kurtosis}{{5.3}{131}{Skew and kurtosis \label {sec:skew}\label {sec:kurtosis}}{section.5.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Skew and kurtosis }{131}{section.5.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces An illustration of kurtosis. On the left, we have a ``platykurtic'' data set (kurtosis = $-.95$), meaning that the data set is ``too flat''. In the middle we have a ``mesokurtic'' data set (kurtosis is almost exactly 0), which means that the pointiness of the data is just about right. Finally, on the right, we have a ``leptokurtic'' data set (kurtosis $= 2.12$) indicating that the data set is ``too pointy''. Note that kurtosis is measured with respect to a normal curve (black line).}}{133}{figure.5.5}}
\newlabel{fig:kurtosis}{{5.5}{133}{An illustration of kurtosis. On the left, we have a ``platykurtic'' data set (kurtosis = $-.95$), meaning that the data set is ``too flat''. In the middle we have a ``mesokurtic'' data set (kurtosis is almost exactly 0), which means that the pointiness of the data is just about right. Finally, on the right, we have a ``leptokurtic'' data set (kurtosis $= 2.12$) indicating that the data set is ``too pointy''. Note that kurtosis is measured with respect to a normal curve (black line)}{figure.5.5}{}}
\newlabel{sec:summary}{{5.4}{133}{Getting an overall summary of a variable~\label {sec:summary}}{section.5.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Getting an overall summary of a variable\nobreakspace  {}}{133}{section.5.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}``Summarising'' a variable}{133}{subsection.5.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}``Summarising'' a data frame}{134}{subsection.5.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.3}``Describing'' a data frame}{135}{subsection.5.4.3}}
\newlabel{sec:groupdescriptives}{{5.5}{136}{Descriptive statistics separately for each group~\label {sec:groupdescriptives}}{section.5.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Descriptive statistics separately for each group\nobreakspace  {}}{136}{section.5.5}}
\newlabel{sec:zscore}{{5.6}{138}{Standard scores~\label {sec:zscore}}{section.5.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Standard scores\nobreakspace  {}}{138}{section.5.6}}
\newlabel{sec:correl}{{5.7}{139}{Correlations\label {sec:correl}}{section.5.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Correlations}{139}{section.5.7}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Descriptive statistics for the parenthood data.}}{140}{table.5.1}}
\newlabel{tab:parenthood}{{5.1}{140}{Descriptive statistics for the parenthood data}{table.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.1}The data}{140}{subsection.5.7.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Histograms for the three interesting variables in the \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}parenthood}} data set.}}{141}{figure.5.6}}
\newlabel{fig:parenthood}{{5.6}{141}{Histograms for the three interesting variables in the \rtext {parenthood} data set}{figure.5.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.2}The strength and direction of a relationship}{141}{subsection.5.7.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Scatterplots showing the relationship between \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}dan.sleep}} and \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}dan.grump}} (left) and the relationship between \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}baby.sleep}} and \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}dan.grump}} (right).}}{142}{figure.5.7}}
\newlabel{fig:scatterparent}{{5.7}{142}{Scatterplots showing the relationship between \rtext {dan.sleep} and \rtext {dan.grump} (left) and the relationship between \rtext {baby.sleep} and \rtext {dan.grump} (right)}{figure.5.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.3}The correlation coefficient}{142}{subsection.5.7.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Scatterplots showing the relationship between \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}baby.sleep}} and \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}dan.grump}} (left), as compared to the relationship between \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}baby.sleep}} and \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}dan.sleep}} (right).}}{143}{figure.5.8}}
\newlabel{fig:scatterparent2}{{5.8}{143}{Scatterplots showing the relationship between \rtext {baby.sleep} and \rtext {dan.grump} (left), as compared to the relationship between \rtext {baby.sleep} and \rtext {dan.sleep} (right)}{figure.5.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.4}Calculating correlations in \textsf  {R}}{143}{subsection.5.7.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Illustration of the effect of varying the strength and direction of a correlation. In the left hand column, the correlations are 0, .33, .66 and 1. In the right hand column, the correlations are 0, -.33, -.66 and -1.}}{144}{figure.5.9}}
\newlabel{fig:corr}{{5.9}{144}{Illustration of the effect of varying the strength and direction of a correlation. In the left hand column, the correlations are 0, .33, .66 and 1. In the right hand column, the correlations are 0, -.33, -.66 and -1}{figure.5.9}{}}
\citation{Anscombe1973}
\newlabel{sec:interpretingcorrelations}{{5.7.5}{145}{Interpreting a correlation~\label {sec:interpretingcorrelations}}{subsection.5.7.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.5}Interpreting a correlation\nobreakspace  {}}{145}{subsection.5.7.5}}
\@writefile{brf}{\backcite{Anscombe1973}{{145}{5.7}{figure.5.10}}}
\@writefile{brf}{\backcite{Anscombe1973}{{145}{5.7}{figure.5.10}}}
\@writefile{brf}{\backcite{Anscombe1973}{{145}{5.7}{figure.5.10}}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces A rough guide to interpreting correlations. Note that I say a {\it  rough} guide. There aren't hard and fast rules for what counts as strong or weak relationships. It depends on the context.}}{146}{table.5.2}}
\newlabel{tab:interpretingcorrelations}{{5.2}{146}{A rough guide to interpreting correlations. Note that I say a {\it rough} guide. There aren't hard and fast rules for what counts as strong or weak relationships. It depends on the context}{table.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.6}Spearman's rank correlations}{146}{subsection.5.7.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Anscombe's quartet. All four of these data sets have a Pearson correlation of $r = .816$, but they are qualitatively different from one another.}}{147}{figure.5.10}}
\newlabel{fig:anscombe}{{5.10}{147}{Anscombe's quartet. All four of these data sets have a Pearson correlation of $r = .816$, but they are qualitatively different from one another}{figure.5.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces The relationship between hours worked and grade received, for a toy data set consisting of only 10 students (each circle corresponds to one student). The dashed line through the middle shows the linear relationship between the two variables. This produces a strong Pearson correlation of $r = .91$. However, the interesting thing to note here is that there's actually a perfect monotonic relationship between the two variables: in this toy example at least, increasing the hours worked always increases the grade received, as illustrated by the solid line. This is reflected in a Spearman correlation of $\rho = 1$. With such a small data set, however, it's an open question as to which version better describes the actual relationship involved. }}{148}{figure.5.11}}
\newlabel{fig:rankcorrpic}{{5.11}{148}{The relationship between hours worked and grade received, for a toy data set consisting of only 10 students (each circle corresponds to one student). The dashed line through the middle shows the linear relationship between the two variables. This produces a strong Pearson correlation of $r = .91$. However, the interesting thing to note here is that there's actually a perfect monotonic relationship between the two variables: in this toy example at least, increasing the hours worked always increases the grade received, as illustrated by the solid line. This is reflected in a Spearman correlation of $\rho = 1$. With such a small data set, however, it's an open question as to which version better describes the actual relationship involved}{figure.5.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.7}The \texttt  {\relax \fontsize  {10.95}{14}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}correlate()}} function}{149}{subsection.5.7.7}}
\newlabel{sec:missing}{{5.8}{151}{Handling missing values~\label {sec:missing}}{section.5.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.8}Handling missing values\nobreakspace  {}}{151}{section.5.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8.1}The single variable case}{151}{subsection.5.8.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8.2}Missing values in pairwise calculations}{152}{subsection.5.8.2}}
\@writefile{toc}{\contentsline {section}{\numberline {5.9}Summary}{154}{section.5.9}}
\citation{Ellman2002}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.9.1}Epilogue: Good descriptive statistics are descriptive!}{155}{subsection.5.9.1}}
\@writefile{brf}{\backcite{Ellman2002}{{155}{5.9.1}{subsection.5.9.1}}}
\@writefile{brf}{\backcite{Ellman2002}{{155}{5.9.1}{subsection.5.9.1}}}
\@writefile{brf}{\backcite{Ellman2002}{{155}{5.9.1}{subsection.5.9.1}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Drawing graphs}{157}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:graphics}{{6}{157}{Drawing graphs}{chapter.6}{}}
\citation{Friendly2011}
\citation{Friendly2011}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces A stylised redrawing of John Snow's original cholera map. Each small dot represents the location of a cholera case, and each large circle shows the location of a well. As the plot makes clear, the cholera outbreak is centred very closely on the Broad St pump. This image uses the data from the \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}HistData}} package \cite {Friendly2011}, and was drawn using minor alterations to the commands provided in the help files. Note that Snow's original hand drawn map used different symbols and labels, but you get the idea.}}{158}{figure.6.1}}
\@writefile{brf}{\backcite{Friendly2011}{{158}{6.1}{figure.6.1}}}
\@writefile{brf}{\backcite{Friendly2011}{{158}{6.1}{figure.6.1}}}
\@writefile{brf}{\backcite{Friendly2011}{{158}{6.1}{figure.6.1}}}
\newlabel{fig:snowmap1}{{6.1}{158}{A stylised redrawing of John Snow's original cholera map. Each small dot represents the location of a cholera case, and each large circle shows the location of a well. As the plot makes clear, the cholera outbreak is centred very closely on the Broad St pump. This image uses the data from the \rtext {HistData} package \protect \cite {Friendly2011}, and was drawn using minor alterations to the commands provided in the help files. Note that Snow's original hand drawn map used different symbols and labels, but you get the idea}{figure.6.1}{}}
\newlabel{sec:rgraphics}{{6.1}{158}{An overview of \R \ graphics\label {sec:rgraphics}}{section.6.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}An overview of \textsf  {R}\ graphics}{158}{section.6.1}}
\newlabel{sec:introplotting}{{6.2}{160}{An introduction to plotting~\label {sec:introplotting}}{section.6.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}An introduction to plotting\nobreakspace  {}}{160}{section.6.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}A tedious digression}{160}{subsection.6.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Our first plot}}{161}{figure.6.2}}
\newlabel{fig:firstplot}{{6.2}{161}{Our first plot}{figure.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces How to add your own title, subtitle, x-axis label and y-axis label to the plot. I've drawn this figure a bit larger than the last one so that everything fits. Generally, you don't want your titles and subtitle to extend beyond the range of the actual plot. The results aren't pretty when that happens.}}{162}{figure.6.3}}
\newlabel{fig:secondplot}{{6.3}{162}{How to add your own title, subtitle, x-axis label and y-axis label to the plot. I've drawn this figure a bit larger than the last one so that everything fits. Generally, you don't want your titles and subtitle to extend beyond the range of the actual plot. The results aren't pretty when that happens}{figure.6.3}{}}
\newlabel{sec:figtitles}{{6.2.2}{162}{Customising the title and the axis labels~\label {sec:figtitles}}{subsection.6.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Customising the title and the axis labels\nobreakspace  {}}{162}{subsection.6.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces How to customise the appearance of the titles and labels.}}{164}{figure.6.4}}
\newlabel{fig:thirdplot}{{6.4}{164}{How to customise the appearance of the titles and labels}{figure.6.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}Changing the plot type}{164}{subsection.6.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Changing the \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}type}} of the plot.}}{165}{figure.6.5}}
\newlabel{fig:simpleplots}{{6.5}{165}{Changing the \rtext {type} of the plot}{figure.6.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Changing the \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}pch}} parameter (panel a) or the \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}lty}} parameter (panel b).}}{166}{figure.6.6}}
\newlabel{fig:pch}{{6.6}{166}{Changing the \rtext {pch} parameter (panel a) or the \rtext {lty} parameter (panel b)}{figure.6.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.4}Changing other features of the plot}{166}{subsection.6.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Customising various aspects to the plot itself.}}{167}{figure.6.7}}
\newlabel{fig:fifthplot}{{6.7}{167}{Customising various aspects to the plot itself}{figure.6.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.5}Changing the appearance of the axes}{167}{subsection.6.2.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces Altering the scale and appearance of the plot axes.}}{169}{figure.6.8}}
\newlabel{fig:fourthplot}{{6.8}{169}{Altering the scale and appearance of the plot axes}{figure.6.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces Other ways to customise the axes.}}{169}{figure.6.9}}
\newlabel{fig:sixthplot}{{6.9}{169}{Other ways to customise the axes}{figure.6.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.6}Don't panic}{170}{subsection.6.2.6}}
\newlabel{sec:hist}{{6.3}{170}{Histograms\label {sec:hist}}{section.6.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Histograms}{170}{section.6.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces Four different histograms of the \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}afl.margins}} variable: (a) the default histogram that \textsf  {R}\ produces, (b) a histogram with too few bins, (c) a histogram with too many bins, and (d) a ``prettier'' histogram making use of various optional arguments to \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}hist()}}. Overall I think that panel d makes a pretty good picture, though if I were to start being picky I wouldn't have the title in boldface text (the eye is drawn to the title and not to the picture), and I'd try to make the labels above the bars a little less prominent as well.}}{172}{figure.6.10}}
\newlabel{fig:hist1}{{6.10}{172}{Four different histograms of the \rtext {afl.margins} variable: (a) the default histogram that \R \ produces, (b) a histogram with too few bins, (c) a histogram with too many bins, and (d) a ``prettier'' histogram making use of various optional arguments to \rtext {hist()}. Overall I think that panel d makes a pretty good picture, though if I were to start being picky I wouldn't have the title in boldface text (the eye is drawn to the title and not to the picture), and I'd try to make the labels above the bars a little less prominent as well}{figure.6.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Visual style of your histogram}{173}{subsection.6.3.1}}
\newlabel{sec:stem}{{6.4}{173}{Stem and leaf plots~\label {sec:stem}}{section.6.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Stem and leaf plots\nobreakspace  {}}{173}{section.6.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces A basic boxplot (panel a), plus the same plot with annotations added to explain what aspect of the data set each part of the boxplot corresponds to (panel b).}}{175}{figure.6.11}}
\newlabel{fig:boxplot1}{{6.11}{175}{A basic boxplot (panel a), plus the same plot with annotations added to explain what aspect of the data set each part of the boxplot corresponds to (panel b)}{figure.6.11}{}}
\newlabel{sec:boxplots}{{6.5}{175}{Boxplots~\label {sec:boxplots}}{section.6.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Boxplots\nobreakspace  {}}{175}{section.6.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Visual style of your boxplot}{176}{subsection.6.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.12}{\ignorespaces By default, \textsf  {R}\ will only extent the whiskers a distance of 1.5 times the interquartile range, and will plot any points that fall outside that range separately (panel a). As we've seen in earlier graphics, the \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}boxplot()}} function lets you customise the plot fairly extensively. This is illustrated in panel b, which shows a much more minimalist boxplot, and attaches informative labels to the graphic.}}{177}{figure.6.12}}
\newlabel{fig:boxplot2}{{6.12}{177}{By default, \R \ will only extent the whiskers a distance of 1.5 times the interquartile range, and will plot any points that fall outside that range separately (panel a). As we've seen in earlier graphics, the \rtext {boxplot()} function lets you customise the plot fairly extensively. This is illustrated in panel b, which shows a much more minimalist boxplot, and attaches informative labels to the graphic}{figure.6.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.13}{\ignorespaces A boxplot showing one very suspicious outlier! I've drawn this plot in a similar, minimalist style to the one in Figure\nobreakspace  {}\ref  {fig:boxplot2}b, but I've used the \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}horizontal}} argument to draw it sideways in order to save space.}}{179}{figure.6.13}}
\newlabel{fig:boxplotoutlier}{{6.13}{179}{A boxplot showing one very suspicious outlier! I've drawn this plot in a similar, minimalist style to the one in Figure~\ref {fig:boxplot2}b, but I've used the \rtext {horizontal} argument to draw it sideways in order to save space}{figure.6.13}{}}
\newlabel{sec:boxplotoutliers}{{6.5.2}{179}{Using box plots to detect outliers~\label {sec:boxplotoutliers}}{subsection.6.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}Using box plots to detect outliers\nobreakspace  {}}{179}{subsection.6.5.2}}
\newlabel{sec:multipleboxplots}{{6.5.3}{181}{Drawing multiple boxplots~\label {sec:multipleboxplots}}{subsection.6.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.3}Drawing multiple boxplots\nobreakspace  {}}{181}{subsection.6.5.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.14}{\ignorespaces Boxplots showing the AFL winning margins for the 24 years from 1987 to 2010 inclusive. This is the default plot created by \textsf  {R}, with no annotations added and no changes to the visual design. It's pretty readable, though at a minimum you'd want to include some basic annotations labelling the axes. Compare and contrast with Figure\nobreakspace  {}\ref  {fig:multipleboxplots2}}}{182}{figure.6.14}}
\newlabel{fig:multipleboxplots}{{6.14}{182}{Boxplots showing the AFL winning margins for the 24 years from 1987 to 2010 inclusive. This is the default plot created by \R , with no annotations added and no changes to the visual design. It's pretty readable, though at a minimum you'd want to include some basic annotations labelling the axes. Compare and contrast with Figure~\ref {fig:multipleboxplots2}}{figure.6.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.15}{\ignorespaces A cleaned up version of Figure\nobreakspace  {}\ref  {fig:multipleboxplots}. Notice that I've used a very minimalist design for the boxplots, so as to focus the eye on the medians. I've also converted the medians to solid dots, to convey a sense that year to year variation in the median should be thought of as a single coherent plot (similar to what we did when plotting the \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}Fibonacci}} variable earlier). The size of outliers has been shrunk, because they aren't actually very interesting. In contrast, I've added a fill colour to the boxes, to make it easier to look at the trend in the interquartile range across years. }}{183}{figure.6.15}}
\newlabel{fig:multipleboxplots2}{{6.15}{183}{A cleaned up version of Figure~\ref {fig:multipleboxplots}. Notice that I've used a very minimalist design for the boxplots, so as to focus the eye on the medians. I've also converted the medians to solid dots, to convey a sense that year to year variation in the median should be thought of as a single coherent plot (similar to what we did when plotting the \rtext {Fibonacci} variable earlier). The size of outliers has been shrunk, because they aren't actually very interesting. In contrast, I've added a fill colour to the boxes, to make it easier to look at the trend in the interquartile range across years}{figure.6.15}{}}
\newlabel{sec:scatterplots}{{6.6}{183}{Scatterplots\label {sec:scatterplots}}{section.6.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Scatterplots}{183}{section.6.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.16}{\ignorespaces Two different scatterplots: (a) the default scatterplot that \textsf  {R}\ produces, (b) one that makes use of several options for fancier display.}}{184}{figure.6.16}}
\newlabel{fig:scatter}{{6.16}{184}{Two different scatterplots: (a) the default scatterplot that \R \ produces, (b) one that makes use of several options for fancier display}{figure.6.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.17}{\ignorespaces A fancy scatterplot drawn using the \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}scatterplot()}} function in the \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}car}} package.}}{186}{figure.6.17}}
\newlabel{fig:fancyscatter}{{6.17}{186}{A fancy scatterplot drawn using the \rtext {scatterplot()} function in the \rtext {car} package}{figure.6.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.1}More elaborate options}{186}{subsection.6.6.1}}
\newlabel{sec:bargraph}{{6.7}{187}{Bar graphs\label {sec:bargraph}}{section.6.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.7}Bar graphs}{187}{section.6.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.18}{\ignorespaces A matrix of scatterplots produced using \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}pairs()}}.}}{188}{figure.6.18}}
\newlabel{fig:pairs}{{6.18}{188}{A matrix of scatterplots produced using \rtext {pairs()}}{figure.6.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.19}{\ignorespaces Four bargraphs. Panel a shows the simplest version of a bargraph, containing the data but no labels. In panel b we've added the labels, but because the text runs horizontally \textsf  {R}\ only includes a few of them. In panel c we've rotated the labels, but now the text is too long to fit. Finally, in panel d we fix this by expanding the margin at the bottom, and add several other customisations to make the chart a bit nicer.}}{189}{figure.6.19}}
\newlabel{fig:bar1}{{6.19}{189}{Four bargraphs. Panel a shows the simplest version of a bargraph, containing the data but no labels. In panel b we've added the labels, but because the text runs horizontally \R \ only includes a few of them. In panel c we've rotated the labels, but now the text is too long to fit. Finally, in panel d we fix this by expanding the margin at the bottom, and add several other customisations to make the chart a bit nicer}{figure.6.19}{}}
\newlabel{sec:par}{{6.7.1}{190}{Changing global settings using \rtext {par()}\label {sec:par}}{subsection.6.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.1}Changing global settings using \texttt  {\relax \fontsize  {10.95}{14}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}par()}}}{190}{subsection.6.7.1}}
\newlabel{sec:saveimage}{{6.8}{191}{Saving image files using \R \ and Rstudio~\label {sec:saveimage}}{section.6.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.8}Saving image files using \textsf  {R}\ and Rstudio\nobreakspace  {}}{191}{section.6.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.8.1}The ugly details }{192}{subsection.6.8.1}}
\citation{Wilkinson2006}
\@writefile{toc}{\contentsline {section}{\numberline {6.9}Summary}{193}{section.6.9}}
\@writefile{brf}{\backcite{Wilkinson2006}{{193}{6.9}{section.6.9}}}
\@writefile{brf}{\backcite{Wilkinson2006}{{193}{6.9}{section.6.9}}}
\@writefile{brf}{\backcite{Wilkinson2006}{{193}{6.9}{section.6.9}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Pragmatic matters\nobreakspace  {}}{195}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:datahandling}{{7}{195}{Pragmatic matters~}{chapter.7}{}}
\newlabel{sec:freqtables}{{7.1}{196}{Tabulating and cross-tabulating data\label {sec:freqtables}}{section.7.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Tabulating and cross-tabulating data}{196}{section.7.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Creating tables from vectors}{196}{subsection.7.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Creating tables from data frames}{197}{subsection.7.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.3}Converting a table of counts to a table of proportions}{198}{subsection.7.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.4}Low level tabulation}{199}{subsection.7.1.4}}
\newlabel{sec:transform}{{7.2}{199}{Transforming and recoding a variable~\label {sec:transform}}{section.7.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Transforming and recoding a variable\nobreakspace  {}}{199}{section.7.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Creating a transformed variable}{199}{subsection.7.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Cutting a numeric variable into categories}{201}{subsection.7.2.2}}
\newlabel{sec:mathfunc}{{7.3}{203}{A few more mathematical functions and operations~\label {sec:mathfunc}}{section.7.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}A few more mathematical functions and operations\nobreakspace  {}}{203}{section.7.3}}
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces Some of the mathematical functions available in \textsf  {R}.}}{204}{table.7.1}}
\newlabel{tab:mathfunc}{{7.1}{204}{Some of the mathematical functions available in \R }{table.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Rounding a number}{204}{subsection.7.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {7.2}{\ignorespaces Two more arithmetic operations that sometimes come in handy.}}{205}{table.7.2}}
\newlabel{tab:arithmetic2}{{7.2}{205}{Two more arithmetic operations that sometimes come in handy}{table.7.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Modulus and integer division}{205}{subsection.7.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.3}Logarithms and exponentials}{206}{subsection.7.3.3}}
\newlabel{sec:subset}{{7.4}{207}{Extracting a subset of a vector~\label {sec:subset}}{section.7.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Extracting a subset of a vector\nobreakspace  {}}{207}{section.7.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.1}Refresher}{208}{subsection.7.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.2}Using \texttt  {\relax \fontsize  {10.95}{14}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}\%in\%}} to match multiple cases}{208}{subsection.7.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.3}Using negative indices to drop elements}{208}{subsection.7.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.4}Splitting a vector by group}{209}{subsection.7.4.4}}
\newlabel{sec:subsetdataframe}{{7.5}{210}{Extracting a subset of a data frame~\label {sec:subsetdataframe}}{section.7.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Extracting a subset of a data frame\nobreakspace  {}}{210}{section.7.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.1}Using the \texttt  {\relax \fontsize  {10.95}{14}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}subset()}} function}{211}{subsection.7.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.2}Using square brackets: I. Rows and columns}{212}{subsection.7.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.3}Using square brackets: II. Some elaborations}{214}{subsection.7.5.3}}
\newlabel{sec:dropping}{{7.5.4}{215}{Using square brackets: III. Understanding ``dropping''~\label {sec:dropping}}{subsection.7.5.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.4}Using square brackets: III. Understanding ``dropping''\nobreakspace  {}}{215}{subsection.7.5.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.5}Using square brackets: IV. Columns only}{216}{subsection.7.5.5}}
\newlabel{sec:sort}{{7.6}{218}{Sorting, flipping and merging data~\label {sec:sort}}{section.7.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.6}Sorting, flipping and merging data\nobreakspace  {}}{218}{section.7.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.1}Sorting a numeric or character vector}{218}{subsection.7.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.2}Sorting a factor}{219}{subsection.7.6.2}}
\newlabel{sec:sortframe}{{7.6.3}{219}{Sorting a data frame~\label {sec:sortframe}}{subsection.7.6.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.3}Sorting a data frame\nobreakspace  {}}{219}{subsection.7.6.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.4}Binding vectors together}{220}{subsection.7.6.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.5}Binding multiple copies of the same vector together}{221}{subsection.7.6.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.6}Transposing a matrix or data frame}{222}{subsection.7.6.6}}
\newlabel{sec:reshape}{{7.7}{224}{Reshaping a data frame\label {sec:reshape}}{section.7.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.7}Reshaping a data frame}{224}{section.7.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.1}Long form and wide form data}{224}{subsection.7.7.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.2}Reshaping data using \texttt  {\relax \fontsize  {10.95}{14}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}wideToLong()}}}{225}{subsection.7.7.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.3}Reshaping data using \texttt  {\relax \fontsize  {10.95}{14}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}longToWide()}}}{226}{subsection.7.7.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.4}Reshaping with multiple within-subject factors}{227}{subsection.7.7.4}}
\citation{Wickham2007}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.5}What other options are there?}{228}{subsection.7.7.5}}
\@writefile{brf}{\backcite{Wickham2007}{{228}{7.7}{subsection.7.7.5}}}
\@writefile{brf}{\backcite{Wickham2007}{{228}{7.7}{subsection.7.7.5}}}
\@writefile{brf}{\backcite{Wickham2007}{{228}{7.7}{subsection.7.7.5}}}
\newlabel{sec:textprocessing}{{7.8}{229}{Working with text~\label {sec:textprocessing}}{section.7.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.8}Working with text\nobreakspace  {}}{229}{section.7.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.1}Shortening a string}{229}{subsection.7.8.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.2}Pasting strings together}{230}{subsection.7.8.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.3}Splitting strings}{231}{subsection.7.8.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.4}Making simple conversions}{232}{subsection.7.8.4}}
\newlabel{sec:logictext2}{{7.8.5}{232}{Applying logical operations to text~\label {sec:logictext2}}{subsection.7.8.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.5}Applying logical operations to text\nobreakspace  {}}{232}{subsection.7.8.5}}
\@writefile{lot}{\contentsline {table}{\numberline {7.3}{\ignorespaces The ordering of various text characters used by the \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}<}} and \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}>}} operators, as well as by the \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}sort()}} function. Not shown is the ``space'' character, which actually comes first on the list.}}{233}{table.7.3}}
\newlabel{tab:asciiorder}{{7.3}{233}{The ordering of various text characters used by the \rtext {<} and \rtext {>} operators, as well as by the \rtext {sort()} function. Not shown is the ``space'' character, which actually comes first on the list}{table.7.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.6}Concatenating and printing with \texttt  {\relax \fontsize  {10.95}{14}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}cat()}}}{233}{subsection.7.8.6}}
\newlabel{sec:escapechars}{{7.8.7}{234}{Using escape characters in text~\label {sec:escapechars}}{subsection.7.8.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.7}Using escape characters in text\nobreakspace  {}}{234}{subsection.7.8.7}}
\@writefile{lot}{\contentsline {table}{\numberline {7.4}{\ignorespaces Standard escape characters that are evaluated by some text processing commands, including \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}cat()}}. This convention dates back to the development of the C programming language in the 1970s, and as a consequence a lot of these characters make most sense if you pretend that \textsf  {R}\ is actually a typewriter, as explained in the main text. Type \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}?Quotes}} for the corresponding \textsf  {R}\ help file.}}{235}{table.7.4}}
\newlabel{tab:textescape}{{7.4}{235}{Standard escape characters that are evaluated by some text processing commands, including \rtext {cat()}. This convention dates back to the development of the C programming language in the 1970s, and as a consequence a lot of these characters make most sense if you pretend that \R \ is actually a typewriter, as explained in the main text. Type \rtext {?Quotes} for the corresponding \R \ help file}{table.7.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.8}Matching and substituting text}{236}{subsection.7.8.8}}
\newlabel{sec:regex}{{7.8.9}{237}{Regular expressions (not really)~\label {sec:regex}}{subsection.7.8.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.9}Regular expressions (not really)\nobreakspace  {}}{237}{subsection.7.8.9}}
\newlabel{sec:importing}{{7.9}{237}{Reading unusual data files~\label {sec:importing}}{section.7.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.9}Reading unusual data files\nobreakspace  {}}{237}{section.7.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.9.1}Loading data from text files}{238}{subsection.7.9.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces The \texttt  {booksales2.csv} data file. It contains more or less the same data as the original \texttt  {booksales.csv} data file, but has a lot of very quirky features.}}{239}{figure.7.1}}
\newlabel{fig:booksales2csv}{{7.1}{239}{The \filename {booksales2.csv} data file. It contains more or less the same data as the original \filename {booksales.csv} data file, but has a lot of very quirky features}{figure.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.9.2}Loading data from SPSS (and other statistics packages)}{239}{subsection.7.9.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.9.3}Loading Excel files}{240}{subsection.7.9.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.9.4}Loading Matlab (\& Octave) files}{240}{subsection.7.9.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.9.5}Saving other kinds of data}{241}{subsection.7.9.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.9.6}Are we done yet?}{241}{subsection.7.9.6}}
\newlabel{sec:coercion}{{7.10}{241}{Coercing data from one class to another\label {sec:coercion}}{section.7.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.10}Coercing data from one class to another}{241}{section.7.10}}
\newlabel{sec:datastructures}{{7.11}{242}{Other useful data structures~\label {sec:datastructures}}{section.7.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.11}Other useful data structures\nobreakspace  {}}{242}{section.7.11}}
\newlabel{sec:matrix}{{7.11.1}{242}{Matrices~\label {sec:matrix} (and arrays)}{subsection.7.11.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.11.1}Matrices\nobreakspace  {} (and arrays)}{242}{subsection.7.11.1}}
\@writefile{lot}{\contentsline {table}{\numberline {7.5}{\ignorespaces An illustration of two different ways of indexing a $2 \times 3$ matrix. On the left we see the row and column version, which is identical to the corresponding indexing scheme for a data frame of the same size. On the right we see the single-index version, which is quite different to what we would get with a data frame. The reason for this is that, for both data frames and matrices, the ``row and column'' version exists to allow the human user to interact with the object in the psychologically meaningful way: since both data frames and matrices are basically just tables of data, it's the same in each case. However, the single-index version is really a method for you to interact with the object in terms of its internal structure, and the internals for data frames and matrices are quite different.}}{244}{table.7.5}}
\newlabel{tab:matrixindex}{{7.5}{244}{An illustration of two different ways of indexing a $2 \times 3$ matrix. On the left we see the row and column version, which is identical to the corresponding indexing scheme for a data frame of the same size. On the right we see the single-index version, which is quite different to what we would get with a data frame. The reason for this is that, for both data frames and matrices, the ``row and column'' version exists to allow the human user to interact with the object in the psychologically meaningful way: since both data frames and matrices are basically just tables of data, it's the same in each case. However, the single-index version is really a method for you to interact with the object in terms of its internal structure, and the internals for data frames and matrices are quite different}{table.7.5}{}}
\newlabel{sec:orderedfactors}{{7.11.2}{245}{Ordered factors~\label {sec:orderedfactors}}{subsection.7.11.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.11.2}Ordered factors\nobreakspace  {}}{245}{subsection.7.11.2}}
\newlabel{sec:dates}{{7.11.3}{246}{Dates and times \label {sec:dates}}{subsection.7.11.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.11.3}Dates and times }{246}{subsection.7.11.3}}
\newlabel{sec:miscdatahandling}{{7.12}{247}{Miscellaneous topics~\label {sec:miscdatahandling}}{section.7.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.12}Miscellaneous topics\nobreakspace  {}}{247}{section.7.12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.1}The problems with floating point arithmetic}{248}{subsection.7.12.1}}
\newlabel{sec:recycling}{{7.12.2}{249}{The recycling rule~\label {sec:recycling}}{subsection.7.12.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.2}The recycling rule\nobreakspace  {}}{249}{subsection.7.12.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces The environment panel in Rstudio can actually show you the contents of any loaded package: each package defines a separate environment, so you can select the one you want to look at in this panel.}}{250}{figure.7.2}}
\newlabel{fig:envs}{{7.2}{250}{The environment panel in Rstudio can actually show you the contents of any loaded package: each package defines a separate environment, so you can select the one you want to look at in this panel}{figure.7.2}{}}
\newlabel{sec:environments}{{7.12.3}{250}{An introduction to environments~\label {sec:environments}}{subsection.7.12.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.3}An introduction to environments\nobreakspace  {}}{250}{subsection.7.12.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.4}Attaching a data frame}{251}{subsection.7.12.4}}
\citation{Spector2008}
\citation{Teetor2011}
\@writefile{toc}{\contentsline {section}{\numberline {7.13}Summary}{252}{section.7.13}}
\@writefile{brf}{\backcite{Spector2008}{{252}{7.13}{section.7.13}}}
\@writefile{brf}{\backcite{Spector2008}{{252}{7.13}{section.7.13}}}
\@writefile{brf}{\backcite{Spector2008}{{252}{7.13}{section.7.13}}}
\@writefile{brf}{\backcite{Teetor2011}{{252}{7.13}{section.7.13}}}
\@writefile{brf}{\backcite{Teetor2011}{{252}{7.13}{section.7.13}}}
\@writefile{brf}{\backcite{Teetor2011}{{252}{7.13}{section.7.13}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Basic programming\nobreakspace  {}}{253}{chapter.8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:scripting}{{8}{253}{Basic programming~}{chapter.8}{}}
\newlabel{sec:scripts}{{8.1}{253}{Scripts\label {sec:scripts}}{section.8.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Scripts}{253}{section.8.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.1}Why use scripts?}{254}{subsection.8.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.2}Our first script}{255}{subsection.8.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces A screenshot showing the \texttt  {hello.R} script if you open in using the default text editor (TextEdit) on a Mac. Using a simple text editor like TextEdit on a Mac or Notepad on Windows isn't actually the best way to write your scripts, but it is the simplest. More to the point, it highlights the fact that a script really is just an ordinary text file.}}{256}{figure.8.1}}
\newlabel{fig:script}{{8.1}{256}{A screenshot showing the \filename {hello.R} script if you open in using the default text editor (TextEdit) on a Mac. Using a simple text editor like TextEdit on a Mac or Notepad on Windows isn't actually the best way to write your scripts, but it is the simplest. More to the point, it highlights the fact that a script really is just an ordinary text file}{figure.8.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.3}Using Rstudio to write scripts}{256}{subsection.8.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces A screenshot showing the \texttt  {hello.R} script open in Rstudio. Assuming that you're looking at this document in colour, you'll notice that the ``hello world'' text is shown in green. This isn't something that you do yourself: that's Rstudio being helpful. Because the text editor in Rstudio ``knows'' something about how \textsf  {R}\ commands work, it will highlight different parts of your script in different colours. This is useful, but it's not actually part of the script itself.}}{257}{figure.8.2}}
\newlabel{fig:script2}{{8.2}{257}{A screenshot showing the \filename {hello.R} script open in Rstudio. Assuming that you're looking at this document in colour, you'll notice that the ``hello world'' text is shown in green. This isn't something that you do yourself: that's Rstudio being helpful. Because the text editor in Rstudio ``knows'' something about how \R \ commands work, it will highlight different parts of your script in different colours. This is useful, but it's not actually part of the script itself}{figure.8.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.4}Commenting your script}{258}{subsection.8.1.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.5}Differences between scripts and the command line}{259}{subsection.8.1.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.6}Done!}{259}{subsection.8.1.6}}
\newlabel{sec:loops}{{8.2}{259}{Loops\label {sec:loops}}{section.8.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Loops}{259}{section.8.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}The \texttt  {\relax \fontsize  {10.95}{14}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}while}} loop}{259}{subsection.8.2.1}}
\newlabel{sec:for}{{8.2.2}{260}{The \rtext {for} loop~\label {sec:for}}{subsection.8.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.2}The \texttt  {\relax \fontsize  {10.95}{14}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}for}} loop\nobreakspace  {}}{260}{subsection.8.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.3}A more realistic example of a loop}{261}{subsection.8.2.3}}
\newlabel{sec:if}{{8.3}{263}{Conditional statements\label {sec:if}}{section.8.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Conditional statements}{263}{section.8.3}}
\newlabel{sec:functions}{{8.4}{264}{Writing functions\label {sec:functions}}{section.8.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.4}Writing functions}{264}{section.8.4}}
\newlabel{sec:dotsargument}{{8.4.1}{265}{Function arguments revisited~\label {sec:dotsargument}}{subsection.8.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.1}Function arguments revisited\nobreakspace  {}}{265}{subsection.8.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.2}There's more to functions than this}{266}{subsection.8.4.2}}
\newlabel{sec:vectorised}{{8.5}{267}{Implicit loops\label {sec:vectorised}}{section.8.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.5}Implicit loops}{267}{section.8.5}}
\citation{Braun2007}
\citation{Matloff2011}
\@writefile{toc}{\contentsline {section}{\numberline {8.6}Summary}{268}{section.8.6}}
\@writefile{brf}{\backcite{Braun2007}{{268}{8.6}{section.8.6}}}
\@writefile{brf}{\backcite{Braun2007}{{268}{8.6}{section.8.6}}}
\@writefile{brf}{\backcite{Braun2007}{{268}{8.6}{section.8.6}}}
\@writefile{brf}{\backcite{Matloff2011}{{268}{8.6}{section.8.6}}}
\@writefile{brf}{\backcite{Matloff2011}{{268}{8.6}{section.8.6}}}
\@writefile{brf}{\backcite{Matloff2011}{{268}{8.6}{section.8.6}}}
\@writefile{toc}{\contentsline {part}{IV\hspace  {1em}Statistical theory}{269}{part.4}}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Introduction to probability}{275}{chapter.9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:probability}{{9}{275}{Introduction to probability}{chapter.9}{}}
\newlabel{sec:probstats}{{9.1}{276}{How are probability and statistics different?~\label {sec:probstats}}{section.9.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}How are probability and statistics different?\nobreakspace  {}}{276}{section.9.1}}
\newlabel{sec:probmeaning}{{9.2}{277}{What does probability mean?\label {sec:probmeaning}}{section.9.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}What does probability mean?}{277}{section.9.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.1}The frequentist view}{277}{subsection.9.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces An illustration of how frequentist probability works. If you flip a fair coin over and over again, the proportion of heads that you've seen eventually settles down, and converges to the true probability of 0.5. Each panel shows four different simulated experiments: in each case, we pretend we flipped a coin 1000 times, and kept track of the proportion of flips that were heads as we went along. Although none of these sequences actually ended up with an exact value of .5, if we'd extended the experiment for an infinite number of coin flips they would have.}}{279}{figure.9.1}}
\newlabel{fig:frequentistprobability}{{9.1}{279}{An illustration of how frequentist probability works. If you flip a fair coin over and over again, the proportion of heads that you've seen eventually settles down, and converges to the true probability of 0.5. Each panel shows four different simulated experiments: in each case, we pretend we flipped a coin 1000 times, and kept track of the proportion of flips that were heads as we went along. Although none of these sequences actually ended up with an exact value of .5, if we'd extended the experiment for an infinite number of coin flips they would have}{figure.9.1}{}}
\citation{Fisher1922b}
\citation{Meehl1967}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.2}The Bayesian view}{280}{subsection.9.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.3}What's the difference? And who is right?}{280}{subsection.9.2.3}}
\@writefile{brf}{\backcite{Fisher1922b}{{281}{9.2}{subsection.9.2.3}}}
\@writefile{brf}{\backcite{Fisher1922b}{{281}{9.2}{subsection.9.2.3}}}
\@writefile{brf}{\backcite{Fisher1922b}{{281}{9.2}{subsection.9.2.3}}}
\@writefile{brf}{\backcite{Meehl1967}{{281}{9.2}{subsection.9.2.3}}}
\@writefile{brf}{\backcite{Meehl1967}{{281}{9.2}{subsection.9.2.3}}}
\@writefile{brf}{\backcite{Meehl1967}{{281}{9.2}{subsection.9.2.3}}}
\newlabel{sec:basicprobability}{{9.3}{281}{Basic probability theory~\label {sec:basicprobability}}{section.9.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.3}Basic probability theory\nobreakspace  {}}{281}{section.9.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.1}Introducing probability distributions}{281}{subsection.9.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces A visual depiction of the ``pants'' probability distribution. There are five ``elementary events'', corresponding to the five pairs of pants that I own. Each event has some probability of occurring: this probability is a number between 0 to 1. The sum of these probabilities is 1.}}{282}{figure.9.2}}
\newlabel{fig:pantsprob}{{9.2}{282}{A visual depiction of the ``pants'' probability distribution. There are five ``elementary events'', corresponding to the five pairs of pants that I own. Each event has some probability of occurring: this probability is a number between 0 to 1. The sum of these probabilities is 1}{figure.9.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9.1}{\ignorespaces Some basic rules that probabilities must satisfy. You don't really need to know these rules in order to understand the analyses that we'll talk about later in the book, but they are important if you want to understand probability theory a bit more deeply.}}{283}{table.9.1}}
\newlabel{tab:probrules}{{9.1}{283}{Some basic rules that probabilities must satisfy. You don't really need to know these rules in order to understand the analyses that we'll talk about later in the book, but they are important if you want to understand probability theory a bit more deeply}{table.9.1}{}}
\newlabel{sec:binomial}{{9.4}{283}{The binomial distribution\label {sec:binomial}}{section.9.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.4}The binomial distribution}{283}{section.9.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.1}Introducing the binomial}{283}{subsection.9.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.2}Working with the binomial distribution in \textsf  {R}}{284}{subsection.9.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces The binomial distribution with size parameter of $N=20$ and an underlying success probability of $\theta = 1/6$. Each vertical bar depicts the probability of one specific outcome (i.e., one possible value of $X$). Because this is a probability distribution, each of the probabilities must be a number between 0 and 1, and the heights of the bars must sum to 1 as well.}}{285}{figure.9.3}}
\newlabel{fig:binomial1}{{9.3}{285}{The binomial distribution with size parameter of $N=20$ and an underlying success probability of $\theta = 1/6$. Each vertical bar depicts the probability of one specific outcome (i.e., one possible value of $X$). Because this is a probability distribution, each of the probabilities must be a number between 0 and 1, and the heights of the bars must sum to 1 as well}{figure.9.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.4}{\ignorespaces Two binomial distributions, involving a scenario in which I'm flipping a fair coin, so the underlying success probability is $\theta = 1/2$. In panel (a), we assume I'm flipping the coin $N=20$ times. In panel (b) we assume that the coin is flipped $N=100$ times.}}{286}{figure.9.4}}
\newlabel{fig:binomial2}{{9.4}{286}{Two binomial distributions, involving a scenario in which I'm flipping a fair coin, so the underlying success probability is $\theta = 1/2$. In panel (a), we assume I'm flipping the coin $N=20$ times. In panel (b) we assume that the coin is flipped $N=100$ times}{figure.9.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9.2}{\ignorespaces Formulas for the binomial and normal distributions. We don't really use these formulas for anything in this book, but they're pretty important for more advanced work, so I thought it might be best to put them here in a table, where they can't get in the way of the text. In the equation for the binomial, $X!$ is the factorial function (i.e., multiply all whole numbers from 1 to $X$), and for the normal distribution ``exp'' refers to the exponential function, which we discussed in Chapter\nobreakspace  {}\ref  {ch:datahandling}. If these equations don't make a lot of sense to you, don't worry too much about them. }}{287}{table.9.2}}
\newlabel{tab:distformulas}{{9.2}{287}{Formulas for the binomial and normal distributions. We don't really use these formulas for anything in this book, but they're pretty important for more advanced work, so I thought it might be best to put them here in a table, where they can't get in the way of the text. In the equation for the binomial, $X!$ is the factorial function (i.e., multiply all whole numbers from 1 to $X$), and for the normal distribution ``exp'' refers to the exponential function, which we discussed in Chapter~\ref {ch:datahandling}. If these equations don't make a lot of sense to you, don't worry too much about them}{table.9.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9.3}{\ignorespaces The naming system for \textsf  {R}\ probability distribution functions. Every probability distribution implemented in \textsf  {R}\ is actually associated with four separate functions, and there is a pretty standardised way for naming these functions.}}{287}{table.9.3}}
\newlabel{tab:pdistnames}{{9.3}{287}{The naming system for \R \ probability distribution functions. Every probability distribution implemented in \R \ is actually associated with four separate functions, and there is a pretty standardised way for naming these functions}{table.9.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.5}{\ignorespaces The normal distribution with mean $\mu = 0$ and standard deviation $\sigma = 1$. The $x$-axis corresponds to the value of some variable, and the $y$-axis tells us something about how likely we are to observe that value. However, notice that the $y$-axis is labelled ``Probability Density'' and not ``Probability''. There is a subtle and somewhat frustrating characteristic of continuous distributions that makes the $y$ axis behave a bit oddly: the height of the curve here isn't actually the probability of observing a particular $x$ value. On the other hand, it {\it  is} true that the heights of the curve tells you which $x$ values are more likely (the higher ones!). (see Section\nobreakspace  {}\ref  {sec:density} for all the annoying details)}}{289}{figure.9.5}}
\newlabel{fig:normdist}{{9.5}{289}{The normal distribution with mean $\mu = 0$ and standard deviation $\sigma = 1$. The $x$-axis corresponds to the value of some variable, and the $y$-axis tells us something about how likely we are to observe that value. However, notice that the $y$-axis is labelled ``Probability Density'' and not ``Probability''. There is a subtle and somewhat frustrating characteristic of continuous distributions that makes the $y$ axis behave a bit oddly: the height of the curve here isn't actually the probability of observing a particular $x$ value. On the other hand, it {\it is} true that the heights of the curve tells you which $x$ values are more likely (the higher ones!). (see Section~\ref {sec:density} for all the annoying details)}{figure.9.5}{}}
\newlabel{sec:normal}{{9.5}{289}{The normal distribution\label {sec:normal}}{section.9.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.5}The normal distribution}{289}{section.9.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.6}{\ignorespaces An illustration of what happens when you change the mean of a normal distribution. The solid line depicts a normal distribution with a mean of $\mu =4$. The dashed line shows a normal distribution with a mean of $\mu =7$. In both cases, the standard deviation is $\sigma =1$. Not surprisingly, the two distributions have the same shape, but the dashed line is shifted to the right.}}{290}{figure.9.6}}
\newlabel{fig:normmean}{{9.6}{290}{An illustration of what happens when you change the mean of a normal distribution. The solid line depicts a normal distribution with a mean of $\mu =4$. The dashed line shows a normal distribution with a mean of $\mu =7$. In both cases, the standard deviation is $\sigma =1$. Not surprisingly, the two distributions have the same shape, but the dashed line is shifted to the right}{figure.9.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.7}{\ignorespaces An illustration of what happens when you change the the standard deviation of a normal distribution. Both distributions plotted in this figure have a mean of $\mu = 5$, but they have different standard deviations. The solid line plots a distribution with standard deviation $\sigma =1$, and the dashed line shows a distribution with standard deviation $\sigma = 2$. As a consequence, both distributions are ``centred'' on the same spot, but the dashed line is wider than the solid one.}}{291}{figure.9.7}}
\newlabel{fig:normsd}{{9.7}{291}{An illustration of what happens when you change the the standard deviation of a normal distribution. Both distributions plotted in this figure have a mean of $\mu = 5$, but they have different standard deviations. The solid line plots a distribution with standard deviation $\sigma =1$, and the dashed line shows a distribution with standard deviation $\sigma = 2$. As a consequence, both distributions are ``centred'' on the same spot, but the dashed line is wider than the solid one}{figure.9.7}{}}
\newlabel{sec:density}{{9.5.1}{291}{Probability density~\label {sec:density} \advanced }{subsection.9.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.1}Probability density\nobreakspace  {} }{291}{subsection.9.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.8}{\ignorespaces The area under the curve tells you the probability that an observation falls within a particular range. The solid lines plot normal distributions with mean $\mu =0$ and standard deviation $\sigma =1$. The shaded areas illustrate ``areas under the curve'' for two important cases. In panel a, we can see that there is a 68.3\% chance that an observation will fall within one standard deviation of the mean. In panel b, we see that there is a 95.4\% chance that an observation will fall within two standard deviations of the mean.}}{292}{figure.9.8}}
\newlabel{fig:sdnorm}{{9.8}{292}{The area under the curve tells you the probability that an observation falls within a particular range. The solid lines plot normal distributions with mean $\mu =0$ and standard deviation $\sigma =1$. The shaded areas illustrate ``areas under the curve'' for two important cases. In panel a, we can see that there is a 68.3\% chance that an observation will fall within one standard deviation of the mean. In panel b, we see that there is a 95.4\% chance that an observation will fall within two standard deviations of the mean}{figure.9.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.9}{\ignorespaces Two more examples of the ``area under the curve idea''. There is a 15.9\% chance that an observation is one standard deviation below the mean or smaller (panel a), and a 34.1\% chance that the observation is greater than one standard deviation below the mean but still below the mean (panel b). Notice that if you add these two numbers together you get $15.9\% + 34.1\% = 50\%$. For normally distributed data, there is a 50\% chance that an observation falls below the mean. And of course that also implies that there is a 50\% chance that it falls above the mean.}}{292}{figure.9.9}}
\newlabel{fig:sdnorm2}{{9.9}{292}{Two more examples of the ``area under the curve idea''. There is a 15.9\% chance that an observation is one standard deviation below the mean or smaller (panel a), and a 34.1\% chance that the observation is greater than one standard deviation below the mean but still below the mean (panel b). Notice that if you add these two numbers together you get $15.9\% + 34.1\% = 50\%$. For normally distributed data, there is a 50\% chance that an observation falls below the mean. And of course that also implies that there is a 50\% chance that it falls above the mean}{figure.9.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.10}{\ignorespaces A $t$ distribution with 3 degrees of freedom (solid line). It looks similar to a normal distribution, but it's not quite the same. For comparison purposes, I've plotted a standard normal distribution as the dashed line. Note that the ``tails'' of the $t$ distribution are ``heavier'' (i.e., extend further outwards) than the tails of the normal distribution? That's the important difference between the two. }}{294}{figure.9.10}}
\newlabel{fig:tdist}{{9.10}{294}{A $t$ distribution with 3 degrees of freedom (solid line). It looks similar to a normal distribution, but it's not quite the same. For comparison purposes, I've plotted a standard normal distribution as the dashed line. Note that the ``tails'' of the $t$ distribution are ``heavier'' (i.e., extend further outwards) than the tails of the normal distribution? That's the important difference between the two}{figure.9.10}{}}
\newlabel{sec:otherdists}{{9.6}{294}{Other useful distributions~\label {sec:otherdists}}{section.9.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.6}Other useful distributions\nobreakspace  {}}{294}{section.9.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.11}{\ignorespaces A $\chi ^2$ distribution with 3 degrees of freedom. Notice that the observed values must always be greater than zero, and that the distribution is pretty skewed. These are the key features of a chi-square distribution.}}{295}{figure.9.11}}
\newlabel{fig:chisqdist}{{9.11}{295}{A $\chi ^2$ distribution with 3 degrees of freedom. Notice that the observed values must always be greater than zero, and that the distribution is pretty skewed. These are the key features of a chi-square distribution}{figure.9.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.12}{\ignorespaces An $F$ distribution with 3 and 5 degrees of freedom. Qualitatively speaking, it looks pretty similar to a chi-square distribution, but they're not quite the same in general.}}{295}{figure.9.12}}
\newlabel{fig:Fdist}{{9.12}{295}{An $F$ distribution with 3 and 5 degrees of freedom. Qualitatively speaking, it looks pretty similar to a chi-square distribution, but they're not quite the same in general}{figure.9.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.13}{\ignorespaces Data sampled from different distributions. See the main text for details.}}{297}{figure.9.13}}
\newlabel{fig:variaterelations}{{9.13}{297}{Data sampled from different distributions. See the main text for details}{figure.9.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.7}Summary}{298}{section.9.7}}
\citation{Evans2000}
\@writefile{brf}{\backcite{Evans2000}{{299}{9.7}{section.9.7}}}
\@writefile{brf}{\backcite{Evans2000}{{299}{9.7}{section.9.7}}}
\@writefile{brf}{\backcite{Evans2000}{{299}{9.7}{section.9.7}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Estimating unknown quantities from a sample}{301}{chapter.10}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:estimation}{{10}{301}{Estimating unknown quantities from a sample}{chapter.10}{}}
\newlabel{sec:srs}{{10.1}{301}{Samples, populations and sampling~\label {sec:srs}}{section.10.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.1}Samples, populations and sampling\nobreakspace  {}}{301}{section.10.1}}
\newlabel{sec:pop}{{10.1.1}{302}{Defining a population~\label {sec:pop}}{subsection.10.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.1}Defining a population\nobreakspace  {}}{302}{subsection.10.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.2}Simple random samples}{302}{subsection.10.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.1}{\ignorespaces Simple random sampling without replacement from a finite population}}{303}{figure.10.1}}
\newlabel{fig:srs1}{{10.1}{303}{Simple random sampling without replacement from a finite population}{figure.10.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.2}{\ignorespaces Biased sampling without replacement from a finite population}}{304}{figure.10.2}}
\newlabel{fig:brs}{{10.2}{304}{Biased sampling without replacement from a finite population}{figure.10.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.3}{\ignorespaces Simple random sampling {\it  with} replacement from a finite population}}{304}{figure.10.3}}
\newlabel{fig:srs2}{{10.3}{304}{Simple random sampling {\it with} replacement from a finite population}{figure.10.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.3}Most samples are not simple random samples}{304}{subsection.10.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.4}How much does it matter if you don't have a simple random sample?}{305}{subsection.10.1.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.5}Population parameters and sample statistics}{306}{subsection.10.1.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.4}{\ignorespaces The population distribution of IQ scores (panel a) and two samples drawn randomly from it. In panel b we have a sample of 100 observations, and panel c we have a sample of 10,000 observations.}}{307}{figure.10.4}}
\newlabel{fig:IQdist}{{10.4}{307}{The population distribution of IQ scores (panel a) and two samples drawn randomly from it. In panel b we have a sample of 100 observations, and panel c we have a sample of 10,000 observations}{figure.10.4}{}}
\citation{Stigler1986}
\newlabel{sec:lawlargenumbers}{{10.2}{308}{The law of large numbers~\label {sec:lawlargenumbers}}{section.10.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.2}The law of large numbers\nobreakspace  {}}{308}{section.10.2}}
\@writefile{brf}{\backcite{Stigler1986}{{308}{10.2}{section.10.2}}}
\@writefile{brf}{\backcite{Stigler1986}{{308}{10.2}{section.10.2}}}
\@writefile{brf}{\backcite{Stigler1986}{{308}{10.2}{section.10.2}}}
\citation{Keynes1923}
\newlabel{sec:samplesandclt}{{10.3}{309}{Sampling distributions and the central limit theorem~\label {sec:samplesandclt}}{section.10.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.3}Sampling distributions and the central limit theorem\nobreakspace  {}}{309}{section.10.3}}
\@writefile{brf}{\backcite{Keynes1923}{{309}{10.3}{section.10.3}}}
\@writefile{brf}{\backcite{Keynes1923}{{309}{10.3}{section.10.3}}}
\@writefile{brf}{\backcite{Keynes1923}{{309}{10.3}{section.10.3}}}
\newlabel{sec:samplingdists}{{10.3.1}{309}{Sampling distribution of the mean~\label {sec:samplingdists}}{subsection.10.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.1}Sampling distribution of the mean\nobreakspace  {}}{309}{subsection.10.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {10.1}{\ignorespaces Ten replications of the IQ experiment, each with a sample size of $N=5$.}}{310}{table.10.1}}
\newlabel{tab:replications}{{10.1}{310}{Ten replications of the IQ experiment, each with a sample size of $N=5$}{table.10.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.2}Sampling distributions exist for any sample statistic!}{310}{subsection.10.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.5}{\ignorespaces The sampling distribution of the mean for the ``five IQ scores experiment''. If you sample 5 people at random and calculate their {\it  average} IQ, you'll almost certainly get a number between 80 and 120, even though there are quite a lot of individuals who have IQs above 120 or below 80. For comparison, the black line plots the population distribution of IQ scores.}}{311}{figure.10.5}}
\newlabel{fig:sampdistmean}{{10.5}{311}{The sampling distribution of the mean for the ``five IQ scores experiment''. If you sample 5 people at random and calculate their {\it average} IQ, you'll almost certainly get a number between 80 and 120, even though there are quite a lot of individuals who have IQs above 120 or below 80. For comparison, the black line plots the population distribution of IQ scores}{figure.10.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.6}{\ignorespaces The sampling distribution of the {\it  maximum} for the ``five IQ scores experiment''. If you sample 5 people at random and select the one with the highest IQ score, you'll probably see someone with an IQ between 100 and 140.}}{311}{figure.10.6}}
\newlabel{fig:sampdistmax}{{10.6}{311}{The sampling distribution of the {\it maximum} for the ``five IQ scores experiment''. If you sample 5 people at random and select the one with the highest IQ score, you'll probably see someone with an IQ between 100 and 140}{figure.10.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.7}{\ignorespaces An illustration of the how sampling distribution of the mean depends on sample size. In each panel, I generated 10,000 samples of IQ data, and calculated the mean IQ observed within each of these data sets. The histograms in these plots show the distribution of these means (i.e., the sampling distribution of the mean). Each individual IQ score was drawn from a normal distribution with mean 100 and standard deviation 15, which is shown as the solid black line). In panel a, each data set contained only a single observation, so the mean of each sample is just one person's IQ score. As a consequence, the sampling distribution of the mean is of course identical to the population distribution of IQ scores. However, when we raise the sample size to 2, the mean of any one sample tends to be closer to the population mean than a one person's IQ score, and so the histogram (i.e., the sampling distribution) is a bit narrower than the population distribution. By the time we raise the sample size to 10 (panel c), we can see that the distribution of sample means tend to be fairly tightly clustered around the true population mean.}}{312}{figure.10.7}}
\newlabel{fig:IQsamp}{{10.7}{312}{An illustration of the how sampling distribution of the mean depends on sample size. In each panel, I generated 10,000 samples of IQ data, and calculated the mean IQ observed within each of these data sets. The histograms in these plots show the distribution of these means (i.e., the sampling distribution of the mean). Each individual IQ score was drawn from a normal distribution with mean 100 and standard deviation 15, which is shown as the solid black line). In panel a, each data set contained only a single observation, so the mean of each sample is just one person's IQ score. As a consequence, the sampling distribution of the mean is of course identical to the population distribution of IQ scores. However, when we raise the sample size to 2, the mean of any one sample tends to be closer to the population mean than a one person's IQ score, and so the histogram (i.e., the sampling distribution) is a bit narrower than the population distribution. By the time we raise the sample size to 10 (panel c), we can see that the distribution of sample means tend to be fairly tightly clustered around the true population mean}{figure.10.7}{}}
\newlabel{sec:clt}{{10.3.3}{312}{The central limit theorem~\label {sec:clt}}{subsection.10.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.3}The central limit theorem\nobreakspace  {}}{312}{subsection.10.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.8}{\ignorespaces A demonstration of the central limit theorem. In panel a, we have a non-normal population distribution; and panels b-d show the sampling distribution of the mean for samples of size 2,4 and 8, for data drawn from the distribution in panel a. As you can see, even though the original population distribution is non-normal, the sampling distribution of the mean becomes pretty close to normal by the time you have a sample of even 4 observations. }}{314}{figure.10.8}}
\newlabel{fig:cltdemo}{{10.8}{314}{A demonstration of the central limit theorem. In panel a, we have a non-normal population distribution; and panels b-d show the sampling distribution of the mean for samples of size 2,4 and 8, for data drawn from the distribution in panel a. As you can see, even though the original population distribution is non-normal, the sampling distribution of the mean becomes pretty close to normal by the time you have a sample of even 4 observations}{figure.10.8}{}}
\newlabel{sec:pointestimates}{{10.4}{315}{Estimating population parameters~\label {sec:pointestimates}}{section.10.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.4}Estimating population parameters\nobreakspace  {}}{315}{section.10.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.1}Estimating the population mean}{315}{subsection.10.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.2}Estimating the population standard deviation}{316}{subsection.10.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.9}{\ignorespaces The sampling distribution of the sample standard deviation for a ``two IQ scores'' experiment. The true population standard deviation is 15 (dashed line), but as you can see from the histogram, the vast majority of experiments will produce a much smaller sample standard deviation than this. On average, this experiment would produce a sample standard deviation of only 8.5, well below the true value! In other words, the sample standard deviation is a {\it  biased} estimate of the population standard deviation. }}{317}{figure.10.9}}
\newlabel{fig:sampdistsd}{{10.9}{317}{The sampling distribution of the sample standard deviation for a ``two IQ scores'' experiment. The true population standard deviation is 15 (dashed line), but as you can see from the histogram, the vast majority of experiments will produce a much smaller sample standard deviation than this. On average, this experiment would produce a sample standard deviation of only 8.5, well below the true value! In other words, the sample standard deviation is a {\it biased} estimate of the population standard deviation}{figure.10.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.10}{\ignorespaces An illustration of the fact that the sample mean is an unbiased estimator of the population mean (panel a), but the sample standard deviation is a biased estimator of the population standard deviation (panel b). To generate the figure, I generated 10,000 simulated data sets with 1 observation each, 10,000 more with 2 observations, and so on up to a sample size of 10. Each data set consisted of fake IQ data: that is, the data were normally distributed with a true population mean of 100 and standard deviation 15. {\it  On average}, the sample means turn out to be 100, regardless of sample size (panel a). However, the sample standard deviations turn out to be systematically too small (panel b), especially for small sample sizes.}}{318}{figure.10.10}}
\newlabel{fig:estimatorbias}{{10.10}{318}{An illustration of the fact that the sample mean is an unbiased estimator of the population mean (panel a), but the sample standard deviation is a biased estimator of the population standard deviation (panel b). To generate the figure, I generated 10,000 simulated data sets with 1 observation each, 10,000 more with 2 observations, and so on up to a sample size of 10. Each data set consisted of fake IQ data: that is, the data were normally distributed with a true population mean of 100 and standard deviation 15. {\it On average}, the sample means turn out to be 100, regardless of sample size (panel a). However, the sample standard deviations turn out to be systematically too small (panel b), especially for small sample sizes}{figure.10.10}{}}
\newlabel{sec:ci}{{10.5}{320}{Estimating a confidence interval\label {sec:ci}}{section.10.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.5}Estimating a confidence interval}{320}{section.10.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.1}A slight mistake in the formula}{321}{subsection.10.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.2}Interpreting a confidence interval}{321}{subsection.10.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.3}Calculating confidence intervals in \textsf  {R}}{322}{subsection.10.5.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.11}{\ignorespaces 95\% confidence intervals. The top (panel a) shows 50 simulated replications of an experiment in which we measure the IQs of 10 people. The dot marks the location of the sample mean, and the line shows the 95\% confidence interval. In total 47 of the 50 confidence intervals do contain the true mean (i.e., 100), but the three intervals marked with asterisks do not. The lower graph (panel b) shows a similar simulation, but this time we simulate replications of an experiment that measures the IQs of 25 people.}}{323}{figure.10.11}}
\newlabel{fig:cirep}{{10.11}{323}{95\% confidence intervals. The top (panel a) shows 50 simulated replications of an experiment in which we measure the IQs of 10 people. The dot marks the location of the sample mean, and the line shows the 95\% confidence interval. In total 47 of the 50 confidence intervals do contain the true mean (i.e., 100), but the three intervals marked with asterisks do not. The lower graph (panel b) shows a similar simulation, but this time we simulate replications of an experiment that measures the IQs of 25 people}{figure.10.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.12}{\ignorespaces Means and 95\% confidence intervals for AFL \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}attendance}}, plotted separately for each \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}year}} from 1987 to 2010. This graph was drawn using the \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}bargraph.CI()}} function.}}{324}{figure.10.12}}
\newlabel{fig:bargraph.CI}{{10.12}{324}{Means and 95\% confidence intervals for AFL \rtext {attendance}, plotted separately for each \rtext {year} from 1987 to 2010. This graph was drawn using the \rtext {bargraph.CI()} function}{figure.10.12}{}}
\newlabel{sec:ciplots}{{10.5.4}{324}{Plotting confidence intervals in \R \label {sec:ciplots}}{subsection.10.5.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.4}Plotting confidence intervals in \textsf  {R}}{324}{subsection.10.5.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.13}{\ignorespaces Means and 95\% confidence intervals for AFL \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}attendance}}, plotted separately for each \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}year}} from 1987 to 2010. This graph was drawn using the \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}lineplot.CI()}} function.}}{325}{figure.10.13}}
\newlabel{fig:lineplot.CI}{{10.13}{325}{Means and 95\% confidence intervals for AFL \rtext {attendance}, plotted separately for each \rtext {year} from 1987 to 2010. This graph was drawn using the \rtext {lineplot.CI()} function}{figure.10.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.14}{\ignorespaces Means and 95\% confidence intervals for AFL \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}attendance}}, plotted separately for each \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}year}} from 1987 to 2010. This graph was drawn using the \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}plotmeans()}} function.}}{325}{figure.10.14}}
\newlabel{fig:plotmeans}{{10.14}{325}{Means and 95\% confidence intervals for AFL \rtext {attendance}, plotted separately for each \rtext {year} from 1987 to 2010. This graph was drawn using the \rtext {plotmeans()} function}{figure.10.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.6}Summary}{326}{section.10.6}}
\@writefile{toc}{\contentsline {chapter}{\numberline {11}Hypothesis testing}{327}{chapter.11}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:hypothesistesting}{{11}{327}{Hypothesis testing}{chapter.11}{}}
\newlabel{sec:hypotheses}{{11.1}{327}{A menagerie of hypotheses~\label {sec:hypotheses}}{section.11.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.1}A menagerie of hypotheses\nobreakspace  {}}{327}{section.11.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1.1}Research hypotheses versus statistical hypotheses}{328}{subsection.11.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1.2}Null hypotheses and alternative hypotheses}{330}{subsection.11.1.2}}
\newlabel{sec:errortypes}{{11.2}{330}{Two types of errors~\label {sec:errortypes}}{section.11.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.2}Two types of errors\nobreakspace  {}}{330}{section.11.2}}
\newlabel{sec:teststatistics}{{11.3}{332}{Test statistics and sampling distributions~\label {sec:teststatistics}}{section.11.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.3}Test statistics and sampling distributions\nobreakspace  {}}{332}{section.11.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.1}{\ignorespaces The sampling distribution for our test statistic $X$ when the null hypothesis is true. For our ESP scenario, this is a binomial distribution. Not surprisingly, since the null hypothesis says that the probability of a correct response is $\theta = .5$, the sampling distribution says that the most likely value is 50 (our of 100) correct responses. Most of the probability mass lies between 40 and 60.}}{333}{figure.11.1}}
\newlabel{fig:samplingdist}{{11.1}{333}{The sampling distribution for our test statistic $X$ when the null hypothesis is true. For our ESP scenario, this is a binomial distribution. Not surprisingly, since the null hypothesis says that the probability of a correct response is $\theta = .5$, the sampling distribution says that the most likely value is 50 (our of 100) correct responses. Most of the probability mass lies between 40 and 60}{figure.11.1}{}}
\newlabel{sec:decisionmaking}{{11.4}{333}{Making decisions~\label {sec:decisionmaking}}{section.11.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.4}Making decisions\nobreakspace  {}}{333}{section.11.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.1}Critical regions and critical values}{333}{subsection.11.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.2}{\ignorespaces The critical region associated with the hypothesis test for the ESP study, for a hypothesis test with a significance level of $\alpha = .05$. The plot itself shows the sampling distribution of $X$ under the null hypothesis (i.e., same as Figure\nobreakspace  {}\ref  {fig:samplingdist}): the grey bars correspond to those values of $X$ for which we would retain the null hypothesis. The black bars show the critical region: those values of $X$ for which we would reject the null. Because the alternative hypothesis is two sided (i.e., allows both $\theta <.5$ and $\theta >.5$), the critical region covers both tails of the distribution. To ensure an $\alpha $ level of $.05$, we need to ensure that each of the two regions encompasses 2.5\% of the sampling distribution. }}{334}{figure.11.2}}
\newlabel{fig:crit2}{{11.2}{334}{The critical region associated with the hypothesis test for the ESP study, for a hypothesis test with a significance level of $\alpha = .05$. The plot itself shows the sampling distribution of $X$ under the null hypothesis (i.e., same as Figure~\ref {fig:samplingdist}): the grey bars correspond to those values of $X$ for which we would retain the null hypothesis. The black bars show the critical region: those values of $X$ for which we would reject the null. Because the alternative hypothesis is two sided (i.e., allows both $\theta <.5$ and $\theta >.5$), the critical region covers both tails of the distribution. To ensure an $\alpha $ level of $.05$, we need to ensure that each of the two regions encompasses 2.5\% of the sampling distribution}{figure.11.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.2}A note on statistical ``significance''}{335}{subsection.11.4.2}}
\newlabel{sec:onesidedtests}{{11.4.3}{335}{The difference between one sided and two sided tests\label {sec:onesidedtests}}{subsection.11.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.3}The difference between one sided and two sided tests}{335}{subsection.11.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.3}{\ignorespaces The critical region for a one sided test. In this case, the alternative hypothesis is that $\theta > .05$, so we would only reject the null hypothesis for large values of $X$. As a consequence, the critical region only covers the upper tail of the sampling distribution; specifically the upper 5\% of the distribution. Contrast this to the two-sided version in Figure\nobreakspace  {}\ref  {fig:crit2}. }}{336}{figure.11.3}}
\newlabel{fig:crit1}{{11.3}{336}{The critical region for a one sided test. In this case, the alternative hypothesis is that $\theta > .05$, so we would only reject the null hypothesis for large values of $X$. As a consequence, the critical region only covers the upper tail of the sampling distribution; specifically the upper 5\% of the distribution. Contrast this to the two-sided version in Figure~\ref {fig:crit2}}{figure.11.3}{}}
\newlabel{sec:pvalue}{{11.5}{336}{The $p$ value of a test\label {sec:pvalue}}{section.11.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.5}The $p$ value of a test}{336}{section.11.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.5.1}A softer view of decision making}{336}{subsection.11.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.5.2}The probability of extreme data}{337}{subsection.11.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.5.3}A common mistake}{337}{subsection.11.5.3}}
\newlabel{sec:writeup}{{11.6}{338}{Reporting the results of a hypothesis test~\label {sec:writeup}}{section.11.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.6}Reporting the results of a hypothesis test\nobreakspace  {}}{338}{section.11.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.1}The issue}{338}{subsection.11.6.1}}
\@writefile{lot}{\contentsline {table}{\numberline {11.1}{\ignorespaces A commonly adopted convention for reporting $p$ values: in many places it is conventional to report one of four different things (e.g., $p<.05$) as shown below. I've included the ``significance stars'' notation (i.e., a * indicates $p<.05$) because you sometimes see this notation produced by statistical software. It's also worth noting that some people will write {\it  n.s.} (not significant) rather than $p>.05$.}}{339}{table.11.1}}
\newlabel{tab:pvaltable}{{11.1}{339}{A commonly adopted convention for reporting $p$ values: in many places it is conventional to report one of four different things (e.g., $p<.05$) as shown below. I've included the ``significance stars'' notation (i.e., a * indicates $p<.05$) because you sometimes see this notation produced by statistical software. It's also worth noting that some people will write {\it n.s.} (not significant) rather than $p>.05$}{table.11.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.2}Two proposed solutions}{339}{subsection.11.6.2}}
\@writefile{toc}{\contentsline {section}{\numberline {11.7}Running the hypothesis test in practice}{340}{section.11.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.4}{\ignorespaces Sampling distribution under the {\it  alternative} hypothesis, for a population parameter value of $\theta = 0.55$. A reasonable proportion of the distribution lies in the rejection region.}}{341}{figure.11.4}}
\newlabel{fig:crit3}{{11.4}{341}{Sampling distribution under the {\it alternative} hypothesis, for a population parameter value of $\theta = 0.55$. A reasonable proportion of the distribution lies in the rejection region}{figure.11.4}{}}
\newlabel{sec:effectsize}{{11.8}{341}{Effect size, sample size and power \label {sec:effectsize}}{section.11.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.8}Effect size, sample size and power }{341}{section.11.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.1}The power function}{341}{subsection.11.8.1}}
\citation{Box1976}
\citation{Cohen1988}
\citation{Ellis2010}
\@writefile{lof}{\contentsline {figure}{\numberline {11.5}{\ignorespaces Sampling distribution under the {\it  alternative} hypothesis, for a population parameter value of $\theta = 0.70$. Almost all of the distribution lies in the rejection region.}}{342}{figure.11.5}}
\newlabel{fig:crit4}{{11.5}{342}{Sampling distribution under the {\it alternative} hypothesis, for a population parameter value of $\theta = 0.70$. Almost all of the distribution lies in the rejection region}{figure.11.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.2}Effect size}{342}{subsection.11.8.2}}
\@writefile{brf}{\backcite{Box1976}{{342}{11.8}{subsection.11.8.2}}}
\@writefile{brf}{\backcite{Box1976}{{342}{11.8}{subsection.11.8.2}}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.6}{\ignorespaces The probability that we will reject the null hypothesis, plotted as a function of the true value of $\theta $. Obviously, the test is more powerful (greater chance of correct rejection) if the true value of $\theta $ is very different from the value that the null hypothesis specifies (i.e., $\theta =.5$). Notice that when $\theta $ actually is equal to .5 (plotted as a black dot), the null hypothesis is in fact true: rejecting the null hypothesis in this instance would be a Type I error.}}{343}{figure.11.6}}
\newlabel{fig:powerfunction}{{11.6}{343}{The probability that we will reject the null hypothesis, plotted as a function of the true value of $\theta $. Obviously, the test is more powerful (greater chance of correct rejection) if the true value of $\theta $ is very different from the value that the null hypothesis specifies (i.e., $\theta =.5$). Notice that when $\theta $ actually is equal to .5 (plotted as a black dot), the null hypothesis is in fact true: rejecting the null hypothesis in this instance would be a Type I error}{figure.11.6}{}}
\@writefile{brf}{\backcite{Cohen1988}{{343}{11.8}{subsection.11.8.2}}}
\@writefile{brf}{\backcite{Cohen1988}{{343}{11.8}{subsection.11.8.2}}}
\@writefile{brf}{\backcite{Cohen1988}{{343}{11.8}{subsection.11.8.2}}}
\@writefile{brf}{\backcite{Ellis2010}{{343}{11.8}{subsection.11.8.2}}}
\@writefile{brf}{\backcite{Ellis2010}{{343}{11.8}{subsection.11.8.2}}}
\@writefile{brf}{\backcite{Ellis2010}{{343}{11.8}{subsection.11.8.2}}}
\@writefile{lot}{\contentsline {table}{\numberline {11.2}{\ignorespaces A crude guide to understanding the relationship between statistical significance and effect sizes. Basically, if you don't have a significant result, then the effect size is pretty meaningless; because you don't have any evidence that it's even real. On the other hand, if you do have a significant effect but your effect size is small, then there's a pretty good chance that your result (although real) isn't all that interesting. However, this guide is very crude: it depends a lot on what exactly you're studying. Small effects can be of massive practical importance in some situations. So don't take this table too seriously. It's a rough guide at best.}}{344}{table.11.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.3}Increasing the power of your study}{344}{subsection.11.8.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.7}{\ignorespaces The power of our test, plotted as a function of the sample size $N$. In this case, the true value of $\theta $ is 0.7, but the null hypothesis is that $\theta = 0.5$. Overall, larger $N$ means greater power. (The small zig-zags in this function occur because of some odd interactions between $\theta $, $\alpha $ and the fact that the binomial distribution is discrete; it doesn't matter for any serious purpose) }}{345}{figure.11.7}}
\newlabel{fig:powerfunctionsample}{{11.7}{345}{The power of our test, plotted as a function of the sample size $N$. In this case, the true value of $\theta $ is 0.7, but the null hypothesis is that $\theta = 0.5$. Overall, larger $N$ means greater power. (The small zig-zags in this function occur because of some odd interactions between $\theta $, $\alpha $ and the fact that the binomial distribution is discrete; it doesn't matter for any serious purpose)}{figure.11.7}{}}
\citation{Lehmann2011}
\newlabel{sec:nhstmess}{{11.9}{346}{Some issues to consider~\label {sec:nhstmess}}{section.11.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.9}Some issues to consider\nobreakspace  {}}{346}{section.11.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.1}Neyman versus Fisher}{346}{subsection.11.9.1}}
\@writefile{brf}{\backcite{Lehmann2011}{{346}{11.9}{subsection.11.9.1}}}
\@writefile{brf}{\backcite{Lehmann2011}{{346}{11.9}{subsection.11.9.1}}}
\@writefile{brf}{\backcite{Lehmann2011}{{346}{11.9}{subsection.11.9.1}}}
\citation{Gelman2006}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.2}Bayesians versus frequentists}{347}{subsection.11.9.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.3}Traps}{347}{subsection.11.9.3}}
\@writefile{brf}{\backcite{Gelman2006}{{347}{11.9}{subsection.11.9.3}}}
\@writefile{brf}{\backcite{Gelman2006}{{347}{11.9}{subsection.11.9.3}}}
\@writefile{brf}{\backcite{Gelman2006}{{347}{11.9}{subsection.11.9.3}}}
\@writefile{toc}{\contentsline {section}{\numberline {11.10}Summary}{348}{section.11.10}}
\@writefile{toc}{\contentsline {part}{V\hspace  {1em}Statistical tools}{349}{part.5}}
\citation{Pearson1900}
\citation{Fisher1922}
\@writefile{toc}{\contentsline {chapter}{\numberline {12}Categorical data analysis}{351}{chapter.12}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:chisquare}{{12}{351}{Categorical data analysis}{chapter.12}{}}
\newlabel{sec:goftest}{{12.1}{351}{The $\chi ^2$ goodness-of-fit test~\label {sec:goftest}}{section.12.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.1}The $\chi ^2$ goodness-of-fit test\nobreakspace  {}}{351}{section.12.1}}
\@writefile{brf}{\backcite{Pearson1900}{{351}{12.1}{section.12.1}}}
\@writefile{brf}{\backcite{Pearson1900}{{351}{12.1}{section.12.1}}}
\@writefile{brf}{\backcite{Pearson1900}{{351}{12.1}{section.12.1}}}
\@writefile{brf}{\backcite{Fisher1922}{{351}{12.1}{section.12.1}}}
\@writefile{brf}{\backcite{Fisher1922}{{351}{12.1}{section.12.1}}}
\@writefile{brf}{\backcite{Fisher1922}{{351}{12.1}{section.12.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.1}The cards data}{351}{subsection.12.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.2}The null hypothesis and the alternative hypothesis}{353}{subsection.12.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.3}The ``goodness of fit'' test statistic}{354}{subsection.12.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.4}The sampling distribution of the GOF statistic }{355}{subsection.12.1.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.5}Degrees of freedom}{356}{subsection.12.1.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.1}{\ignorespaces Chi-square distributions with different values for the ``degrees of freedom''.}}{357}{figure.12.1}}
\newlabel{fig:manychi}{{12.1}{357}{Chi-square distributions with different values for the ``degrees of freedom''}{figure.12.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.6}Testing the null hypothesis}{357}{subsection.12.1.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.2}{\ignorespaces Illustration of how the hypothesis testing works for the chi-square goodness of fit test.}}{358}{figure.12.2}}
\newlabel{fig:goftest}{{12.2}{358}{Illustration of how the hypothesis testing works for the chi-square goodness of fit test}{figure.12.2}{}}
\newlabel{sec:gofTestInR}{{12.1.7}{359}{Doing the test in \R ~\label {sec:gofTestInR}}{subsection.12.1.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.7}Doing the test in \textsf  {R}\nobreakspace  {}}{359}{subsection.12.1.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.8}Specifying a different null hypothesis}{360}{subsection.12.1.8}}
\newlabel{sec:chisqreport}{{12.1.9}{361}{How to report the results of the test~\label {sec:chisqreport}}{subsection.12.1.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.9}How to report the results of the test\nobreakspace  {}}{361}{subsection.12.1.9}}
\citation{Sokal1994}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.10}A comment on statistical notation }{362}{subsection.12.1.10}}
\@writefile{brf}{\backcite{Sokal1994}{{363}{12.1}{subsection.12.1.10}}}
\@writefile{brf}{\backcite{Sokal1994}{{363}{12.1}{subsection.12.1.10}}}
\@writefile{brf}{\backcite{Sokal1994}{{363}{12.1}{subsection.12.1.10}}}
\newlabel{sec:chisqindependence}{{12.2}{363}{The $\chi ^2$ test of independence (or association)\label {sec:chisqindependence}}{section.12.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.2}The $\chi ^2$ test of independence (or association)}{363}{section.12.2}}
\citation{Hogg2005}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.1}Constructing our hypothesis test}{365}{subsection.12.2.1}}
\@writefile{brf}{\backcite{Hogg2005}{{365}{7}{Hfootnote.175}}}
\@writefile{brf}{\backcite{Hogg2005}{{365}{7}{Hfootnote.175}}}
\@writefile{brf}{\backcite{Hogg2005}{{365}{7}{Hfootnote.175}}}
\newlabel{sec:AssocTestInR}{{12.2.2}{367}{Doing the test in \R ~\label {sec:AssocTestInR}}{subsection.12.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.2}Doing the test in \textsf  {R}\nobreakspace  {}}{367}{subsection.12.2.2}}
\citation{Yates1934}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.3}Postscript}{369}{subsection.12.2.3}}
\newlabel{sec:yates}{{12.3}{369}{The continuity correction~\label {sec:yates}}{section.12.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.3}The continuity correction\nobreakspace  {}}{369}{section.12.3}}
\@writefile{brf}{\backcite{Yates1934}{{369}{12.3}{section.12.3}}}
\@writefile{brf}{\backcite{Yates1934}{{369}{12.3}{section.12.3}}}
\@writefile{brf}{\backcite{Yates1934}{{369}{12.3}{section.12.3}}}
\citation{Cramer1946}
\newlabel{sec:chisqeffectsize}{{12.4}{370}{Effect size~\label {sec:chisqeffectsize}}{section.12.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.4}Effect size\nobreakspace  {}}{370}{section.12.4}}
\@writefile{brf}{\backcite{Cramer1946}{{370}{12.4}{section.12.4}}}
\@writefile{brf}{\backcite{Cramer1946}{{370}{12.4}{section.12.4}}}
\@writefile{brf}{\backcite{Cramer1946}{{370}{12.4}{section.12.4}}}
\citation{Cochran1954}
\citation{Larntz1978}
\newlabel{sec:chisqassumptions}{{12.5}{371}{Assumptions of the test(s)~\label {sec:chisqassumptions}}{section.12.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.5}Assumptions of the test(s)\nobreakspace  {}}{371}{section.12.5}}
\@writefile{brf}{\backcite{Cochran1954}{{371}{12.5}{section.12.5}}}
\@writefile{brf}{\backcite{Cochran1954}{{371}{12.5}{section.12.5}}}
\@writefile{brf}{\backcite{Cochran1954}{{371}{12.5}{section.12.5}}}
\@writefile{brf}{\backcite{Larntz1978}{{371}{12.5}{section.12.5}}}
\@writefile{brf}{\backcite{Larntz1978}{{371}{12.5}{section.12.5}}}
\@writefile{brf}{\backcite{Larntz1978}{{371}{12.5}{section.12.5}}}
\newlabel{sec:chisq.test}{{12.6}{371}{The most typical way to do chi-square tests in \R \label {sec:chisq.test}}{section.12.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.6}The most typical way to do chi-square tests in \textsf  {R}}{371}{section.12.6}}
\newlabel{sec:fisherexacttest}{{12.7}{373}{The Fisher exact test~\label {sec:fisherexacttest}}{section.12.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.7}The Fisher exact test\nobreakspace  {}}{373}{section.12.7}}
\citation{Fisher1922}
\@writefile{brf}{\backcite{Fisher1922}{{374}{12.7}{section.12.7}}}
\@writefile{brf}{\backcite{Fisher1922}{{374}{12.7}{section.12.7}}}
\@writefile{brf}{\backcite{Fisher1922}{{374}{12.7}{section.12.7}}}
\citation{McNemar1947}
\newlabel{sec:mcnemar}{{12.8}{375}{The McNemar test\label {sec:mcnemar}}{section.12.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.8}The McNemar test}{375}{section.12.8}}
\@writefile{brf}{\backcite{McNemar1947}{{375}{12.8}{section.12.8}}}
\@writefile{brf}{\backcite{McNemar1947}{{375}{12.8}{section.12.8}}}
\@writefile{brf}{\backcite{McNemar1947}{{375}{12.8}{section.12.8}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.8.1}Doing the McNemar test in \textsf  {R}}{376}{subsection.12.8.1}}
\@writefile{toc}{\contentsline {section}{\numberline {12.9}What's the difference between McNemar and independence?}{377}{section.12.9}}
\citation{Agresti1996}
\citation{Agresti2002}
\@writefile{toc}{\contentsline {section}{\numberline {12.10}Summary}{378}{section.12.10}}
\@writefile{brf}{\backcite{Agresti1996}{{378}{12.10}{section.12.10}}}
\@writefile{brf}{\backcite{Agresti1996}{{378}{12.10}{section.12.10}}}
\@writefile{brf}{\backcite{Agresti1996}{{378}{12.10}{section.12.10}}}
\@writefile{brf}{\backcite{Agresti2002}{{378}{12.10}{section.12.10}}}
\@writefile{brf}{\backcite{Agresti2002}{{378}{12.10}{section.12.10}}}
\@writefile{brf}{\backcite{Agresti2002}{{378}{12.10}{section.12.10}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {13}Comparing two means }{379}{chapter.13}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:ttest}{{13}{379}{Comparing two means}{chapter.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {13.1}The one-sample $z$-test}{379}{section.13.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.1}The inference problem that the test addresses}{380}{subsection.13.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.2}Constructing the hypothesis test}{380}{subsection.13.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.1}{\ignorespaces The theoretical distribution (solid line) from which the psychology student grades (grey bars) are supposed to have been generated.}}{381}{figure.13.1}}
\newlabel{fig:zeppo}{{13.1}{381}{The theoretical distribution (solid line) from which the psychology student grades (grey bars) are supposed to have been generated}{figure.13.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.2}{\ignorespaces Graphical illustration of the null and alternative hypotheses assumed by the one sample $z$-test (the two sided version, that is). The null and alternative hypotheses both assume that the population distribution is normal, and additionally assumes that the population standard deviation is known (fixed at some value $\sigma _0$). The null hypothesis (left) is that the population mean $\mu $ is equal to some specified value $\mu _0$. The alternative hypothesis is that the population mean differs from this value, $\mu \neq \mu _0$.}}{382}{figure.13.2}}
\newlabel{fig:ztesthyp}{{13.2}{382}{Graphical illustration of the null and alternative hypotheses assumed by the one sample $z$-test (the two sided version, that is). The null and alternative hypotheses both assume that the population distribution is normal, and additionally assumes that the population standard deviation is known (fixed at some value $\sigma _0$). The null hypothesis (left) is that the population mean $\mu $ is equal to some specified value $\mu _0$. The alternative hypothesis is that the population mean differs from this value, $\mu \neq \mu _0$}{figure.13.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.3}{\ignorespaces Rejection regions for the two-sided $z$-test (panel a) and the one-sided $z$-test (panel b).}}{383}{figure.13.3}}
\newlabel{fig:ztest}{{13.3}{383}{Rejection regions for the two-sided $z$-test (panel a) and the one-sided $z$-test (panel b)}{figure.13.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.3}A worked example using \textsf  {R}}{383}{subsection.13.1.3}}
\newlabel{sec:zassumptions}{{13.1.4}{385}{Assumptions of the $z$-test~\label {sec:zassumptions}}{subsection.13.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.4}Assumptions of the $z$-test\nobreakspace  {}}{385}{subsection.13.1.4}}
\newlabel{sec:onesamplettest}{{13.2}{385}{The one-sample $t$-test~\label {sec:onesamplettest}}{section.13.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {13.2}The one-sample $t$-test\nobreakspace  {}}{385}{section.13.2}}
\citation{Student1908}
\citation{Box1987}
\@writefile{lof}{\contentsline {figure}{\numberline {13.4}{\ignorespaces Graphical illustration of the null and alternative hypotheses assumed by the (two sided) one sample $t$-test. Note the similarity to the $z$-test (Figure\nobreakspace  {}\ref  {fig:ztesthyp}). The null hypothesis is that the population mean $\mu $ is equal to some specified value $\mu _0$, and the alternative hypothesis is that it is not. Like the $z$-test, we assume that the data are normally distributed; but we do not assume that the population standard deviation $\sigma $ is known in advance.}}{386}{figure.13.4}}
\newlabel{fig:ttesthyp_onesample}{{13.4}{386}{Graphical illustration of the null and alternative hypotheses assumed by the (two sided) one sample $t$-test. Note the similarity to the $z$-test (Figure~\ref {fig:ztesthyp}). The null hypothesis is that the population mean $\mu $ is equal to some specified value $\mu _0$, and the alternative hypothesis is that it is not. Like the $z$-test, we assume that the data are normally distributed; but we do not assume that the population standard deviation $\sigma $ is known in advance}{figure.13.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.1}Introducing the $t$-test}{386}{subsection.13.2.1}}
\@writefile{brf}{\backcite{Student1908}{{386}{13.2}{subsection.13.2.1}}}
\@writefile{brf}{\backcite{Student1908}{{386}{13.2}{subsection.13.2.1}}}
\@writefile{brf}{\backcite{Student1908}{{386}{13.2}{subsection.13.2.1}}}
\@writefile{brf}{\backcite{Box1987}{{386}{13.2}{subsection.13.2.1}}}
\@writefile{brf}{\backcite{Box1987}{{386}{13.2}{subsection.13.2.1}}}
\@writefile{brf}{\backcite{Box1987}{{386}{13.2}{subsection.13.2.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.5}{\ignorespaces The $t$ distribution with 2 degrees of freedom (left) and 10 degrees of freedom (right), with a standard normal distribution (i.e., mean 0 and std dev 1) plotted as dotted lines for comparison purposes. Notice that the $t$ distribution has heavier tails (higher kurtosis) than the normal distribution; this effect is quite exaggerated when the degrees of freedom are very small, but negligible for larger values. In other words, for large $df$ the $t$ distribution is essentially identical to a normal distribution.}}{387}{figure.13.5}}
\newlabel{fig:ttestdist}{{13.5}{387}{The $t$ distribution with 2 degrees of freedom (left) and 10 degrees of freedom (right), with a standard normal distribution (i.e., mean 0 and std dev 1) plotted as dotted lines for comparison purposes. Notice that the $t$ distribution has heavier tails (higher kurtosis) than the normal distribution; this effect is quite exaggerated when the degrees of freedom are very small, but negligible for larger values. In other words, for large $df$ the $t$ distribution is essentially identical to a normal distribution}{figure.13.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.2}Doing the test in \textsf  {R}}{387}{subsection.13.2.2}}
\newlabel{sec:ttestoneassumptions}{{13.2.3}{389}{Assumptions of the one sample $t$-test~\label {sec:ttestoneassumptions}}{subsection.13.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.3}Assumptions of the one sample $t$-test\nobreakspace  {}}{389}{subsection.13.2.3}}
\newlabel{sec:studentttest}{{13.3}{389}{The independent samples $t$-test (Student test)~\label {sec:studentttest}}{section.13.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {13.3}The independent samples $t$-test (Student test)\nobreakspace  {}}{389}{section.13.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3.1}The data}{390}{subsection.13.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3.2}Introducing the test}{390}{subsection.13.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.6}{\ignorespaces Histograms showing the overall distribution of grades for students in Anastasia's class (panel a) and in Bernadette's class (panel b). Inspection of these histograms suggests that the students in Anastasia's class may be getting slightly better grades on average, though they also seem a little more variable.}}{391}{figure.13.6}}
\newlabel{fig:harpohist}{{13.6}{391}{Histograms showing the overall distribution of grades for students in Anastasia's class (panel a) and in Bernadette's class (panel b). Inspection of these histograms suggests that the students in Anastasia's class may be getting slightly better grades on average, though they also seem a little more variable}{figure.13.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.7}{\ignorespaces Plots showing the mean grade for the students in Anastasia's and Bernadette's tutorials. Error bars depict 95\% confidence intervals around the mean. On the basis of visual inspection, it does look like there's a real difference between the groups, though it's hard to say for sure.}}{392}{figure.13.7}}
\newlabel{fig:ttestci}{{13.7}{392}{Plots showing the mean grade for the students in Anastasia's and Bernadette's tutorials. Error bars depict 95\% confidence intervals around the mean. On the basis of visual inspection, it does look like there's a real difference between the groups, though it's hard to say for sure}{figure.13.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3.3}A ``pooled estimate'' of the standard deviation}{392}{subsection.13.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.8}{\ignorespaces Graphical illustration of the null and alternative hypotheses assumed by the Student $t$-test. The null hypothesis assumes that both groups have the same mean $\mu $, whereas the alternative assumes that they have different means $\mu _1$ and $\mu _2$. Notice that it is assumed that the population distributions are normal, and that, although the alternative hypothesis allows the group to have different means, it assumes they have the same standard deviation.}}{393}{figure.13.8}}
\newlabel{fig:ttesthyp}{{13.8}{393}{Graphical illustration of the null and alternative hypotheses assumed by the Student $t$-test. The null hypothesis assumes that both groups have the same mean $\mu $, whereas the alternative assumes that they have different means $\mu _1$ and $\mu _2$. Notice that it is assumed that the population distributions are normal, and that, although the alternative hypothesis allows the group to have different means, it assumes they have the same standard deviation}{figure.13.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3.4}The same pooled estimate, described differently}{393}{subsection.13.3.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3.5}Completing the test}{394}{subsection.13.3.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3.6}Doing the test in \textsf  {R}}{394}{subsection.13.3.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3.7}Positive and negative $t$ values}{396}{subsection.13.3.7}}
\newlabel{sec:studentassumptions}{{13.3.8}{397}{Assumptions of the test~\label {sec:studentassumptions}}{subsection.13.3.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3.8}Assumptions of the test\nobreakspace  {}}{397}{subsection.13.3.8}}
\citation{Welch1947}
\newlabel{sec:welchttest}{{13.4}{398}{The independent samples $t$-test (Welch test)~\label {sec:welchttest}}{section.13.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {13.4}The independent samples $t$-test (Welch test)\nobreakspace  {}}{398}{section.13.4}}
\@writefile{brf}{\backcite{Welch1947}{{398}{13.4}{section.13.4}}}
\@writefile{brf}{\backcite{Welch1947}{{398}{13.4}{section.13.4}}}
\@writefile{brf}{\backcite{Welch1947}{{398}{13.4}{section.13.4}}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.9}{\ignorespaces Graphical illustration of the null and alternative hypotheses assumed by the Welch $t$-test. Like the Student test (Figure\nobreakspace  {}\ref  {fig:ttesthyp}) we assume that both samples are drawn from a normal population; but the alternative hypothesis no longer requires the two populations to have equal variance.}}{399}{figure.13.9}}
\newlabel{fig:ttesthyp2}{{13.9}{399}{Graphical illustration of the null and alternative hypotheses assumed by the Welch $t$-test. Like the Student test (Figure~\ref {fig:ttesthyp}) we assume that both samples are drawn from a normal population; but the alternative hypothesis no longer requires the two populations to have equal variance}{figure.13.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.4.1}Doing the test in \textsf  {R}}{399}{subsection.13.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.4.2}Assumptions of the test}{400}{subsection.13.4.2}}
\newlabel{sec:pairedsamplesttest}{{13.5}{400}{The paired-samples $t$-test~\label {sec:pairedsamplesttest}}{section.13.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {13.5}The paired-samples $t$-test\nobreakspace  {}}{400}{section.13.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.5.1}The data}{401}{subsection.13.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.10}{\ignorespaces Mean grade for test 1 and test 2, with associated 95\% confidence intervals (panel a). Scatterplot showing the individual grades for test 1 and test 2 (panel b). Histogram showing the improvement made by each student in Dr Chico's class (panel c). In panel c, notice that almost the entire distribution is above zero: the vast majority of students did improve their performance from the first test to the second one}}{402}{figure.13.10}}
\newlabel{fig:pairedt}{{13.10}{402}{Mean grade for test 1 and test 2, with associated 95\% confidence intervals (panel a). Scatterplot showing the individual grades for test 1 and test 2 (panel b). Histogram showing the improvement made by each student in Dr Chico's class (panel c). In panel c, notice that almost the entire distribution is above zero: the vast majority of students did improve their performance from the first test to the second one}{figure.13.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.5.2}What is the paired samples $t$-test?}{403}{subsection.13.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.5.3}Doing the test in \textsf  {R}, part 1}{404}{subsection.13.5.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.5.4}Doing the test in \textsf  {R}, part 2}{405}{subsection.13.5.4}}
\@writefile{toc}{\contentsline {section}{\numberline {13.6}One sided tests}{408}{section.13.6}}
\newlabel{sec:ttestfunction}{{13.7}{410}{Using the t.test() function\label {sec:ttestfunction}}{section.13.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {13.7}Using the t.test() function}{410}{section.13.7}}
\citation{Cohen1988}
\citation{McGrath2006}
\newlabel{sec:cohensd}{{13.8}{412}{Effect size~\label {sec:cohensd}}{section.13.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {13.8}Effect size\nobreakspace  {}}{412}{section.13.8}}
\@writefile{brf}{\backcite{Cohen1988}{{412}{13.8}{section.13.8}}}
\@writefile{brf}{\backcite{Cohen1988}{{412}{13.8}{section.13.8}}}
\@writefile{brf}{\backcite{Cohen1988}{{412}{13.8}{section.13.8}}}
\@writefile{brf}{\backcite{McGrath2006}{{412}{13.8}{section.13.8}}}
\@writefile{brf}{\backcite{McGrath2006}{{412}{13.8}{section.13.8}}}
\@writefile{brf}{\backcite{McGrath2006}{{412}{13.8}{section.13.8}}}
\@writefile{lot}{\contentsline {table}{\numberline {13.1}{\ignorespaces A (very) rough guide to interpreting Cohen's $d$. My personal recommendation is to not use these blindly. The $d$ statistic has a natural interpretation in and of itself: it redescribes the different in means as the number of standard deviations that separates those means. So it's generally a good idea to think about what that means in practical terms. In some contexts a ``small'' effect could be of big practical importance. In other situations a ``large'' effect may not be all that interesting.}}{413}{table.13.1}}
\newlabel{tab:cohensdinterpretation}{{13.1}{413}{A (very) rough guide to interpreting Cohen's $d$. My personal recommendation is to not use these blindly. The $d$ statistic has a natural interpretation in and of itself: it redescribes the different in means as the number of standard deviations that separates those means. So it's generally a good idea to think about what that means in practical terms. In some contexts a ``small'' effect could be of big practical importance. In other situations a ``large'' effect may not be all that interesting}{table.13.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.8.1}Cohen's $d$ from one sample}{413}{subsection.13.8.1}}
\citation{Hedges1981}
\citation{Hedges1985}
\citation{Cohen1988}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.8.2}Cohen's $d$ from a Student $t$ test}{414}{subsection.13.8.2}}
\@writefile{brf}{\backcite{Hedges1981}{{414}{13.8}{subsection.13.8.2}}}
\@writefile{brf}{\backcite{Hedges1981}{{414}{13.8}{subsection.13.8.2}}}
\@writefile{brf}{\backcite{Hedges1981}{{414}{13.8}{subsection.13.8.2}}}
\@writefile{brf}{\backcite{Hedges1985}{{414}{13.8}{subsection.13.8.2}}}
\@writefile{brf}{\backcite{Hedges1985}{{414}{13.8}{subsection.13.8.2}}}
\@writefile{brf}{\backcite{Hedges1985}{{414}{13.8}{subsection.13.8.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.8.3}Cohen's $d$ from a Welch test}{414}{subsection.13.8.3}}
\@writefile{brf}{\backcite{Cohen1988}{{415}{13.8}{subsection.13.8.3}}}
\@writefile{brf}{\backcite{Cohen1988}{{415}{13.8}{subsection.13.8.3}}}
\@writefile{brf}{\backcite{Cohen1988}{{415}{13.8}{subsection.13.8.3}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.8.4}Cohen's $d$ from a paired-samples test}{415}{subsection.13.8.4}}
\newlabel{sec:shapiro}{{13.9}{416}{Checking the normality of a sample\label {sec:shapiro}}{section.13.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {13.9}Checking the normality of a sample}{416}{section.13.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.9.1}QQ plots}{416}{subsection.13.9.1}}
\citation{Shapiro1965}
\@writefile{lof}{\contentsline {figure}{\numberline {13.11}{\ignorespaces Histogram (panel a) and normal QQ plot (panel b) of \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}normal.data}}, a normally distributed sample with 100 observations. The Shapiro-Wilk statistic associated with these data is $W = .99$, indicating that no significant departures from normality were detected ($p = .73$).}}{417}{figure.13.11}}
\newlabel{fig:qq1}{{13.11}{417}{Histogram (panel a) and normal QQ plot (panel b) of \rtext {normal.data}, a normally distributed sample with 100 observations. The Shapiro-Wilk statistic associated with these data is $W = .99$, indicating that no significant departures from normality were detected ($p = .73$)}{figure.13.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.9.2}Shapiro-Wilk tests}{417}{subsection.13.9.2}}
\@writefile{brf}{\backcite{Shapiro1965}{{417}{13.9}{subsection.13.9.2}}}
\@writefile{brf}{\backcite{Shapiro1965}{{417}{13.9}{subsection.13.9.2}}}
\@writefile{brf}{\backcite{Shapiro1965}{{417}{13.9}{subsection.13.9.2}}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.12}{\ignorespaces In the top row, a histogram (panel a) and normal QQ plot (panel b) of the 100 observations in a \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}skewed.data}} set. The skewness of the data here is 1.94, and is reflected in a QQ plot that curves upwards. As a consequence, the Shapiro-Wilk statistic is $W=.80$, reflecting a significant departure from normality ($p<.001$). The bottom row shows the same plots for a heavy tailed data set, again consisting of 100 observations. In this case, the heavy tails in the data produce a high kurtosis (2.80), and cause the QQ plot to flatten in the middle, and curve away sharply on either side. The resulting Shapiro-Wilk statistic is $W = .93$, again reflecting significant non-normality ($p < .001$).}}{418}{figure.13.12}}
\newlabel{fig:qq2}{{13.12}{418}{In the top row, a histogram (panel a) and normal QQ plot (panel b) of the 100 observations in a \rtext {skewed.data} set. The skewness of the data here is 1.94, and is reflected in a QQ plot that curves upwards. As a consequence, the Shapiro-Wilk statistic is $W=.80$, reflecting a significant departure from normality ($p<.001$). The bottom row shows the same plots for a heavy tailed data set, again consisting of 100 observations. In this case, the heavy tails in the data produce a high kurtosis (2.80), and cause the QQ plot to flatten in the middle, and curve away sharply on either side. The resulting Shapiro-Wilk statistic is $W = .93$, again reflecting significant non-normality ($p < .001$)}{figure.13.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.13}{\ignorespaces Sampling distribution of the Shapiro-Wilk $W$ statistic, under the null hypothesis that the data are normally distributed, for samples of size 10, 20 and 50. Note that {\it  small} values of $W$ indicate departure from normality.}}{419}{figure.13.13}}
\newlabel{fig:swdist}{{13.13}{419}{Sampling distribution of the Shapiro-Wilk $W$ statistic, under the null hypothesis that the data are normally distributed, for samples of size 10, 20 and 50. Note that {\it small} values of $W$ indicate departure from normality}{figure.13.13}{}}
\newlabel{sec:wilcox}{{13.10}{420}{Testing non-normal data with Wilcoxon tests\label {sec:wilcox}}{section.13.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {13.10}Testing non-normal data with Wilcoxon tests}{420}{section.13.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.10.1}Two sample Wilcoxon test}{420}{subsection.13.10.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.10.2}One sample Wilcoxon test}{421}{subsection.13.10.2}}
\@writefile{toc}{\contentsline {section}{\numberline {13.11}Summary}{422}{section.13.11}}
\@writefile{toc}{\contentsline {chapter}{\numberline {14}Comparing several means (one-way ANOVA)}{425}{chapter.14}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:anova}{{14}{425}{Comparing several means (one-way ANOVA)}{chapter.14}{}}
\newlabel{sec:anxifree}{{14.1}{425}{An illustrative data set~\label {sec:anxifree}}{section.14.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {14.1}An illustrative data set\nobreakspace  {}}{425}{section.14.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.1}{\ignorespaces Average mood gain as a function of drug administered. Error bars depict 95\% confidence intervals associated with each of the group means.}}{427}{figure.14.1}}
\newlabel{fig:moodgain}{{14.1}{427}{Average mood gain as a function of drug administered. Error bars depict 95\% confidence intervals associated with each of the group means}{figure.14.1}{}}
\newlabel{sec:anovaintro}{{14.2}{427}{How ANOVA works \label {sec:anovaintro}}{section.14.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {14.2}How ANOVA works }{427}{section.14.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2.1}Two formulas for the variance of $Y$}{428}{subsection.14.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2.2}From variances to sums of squares}{429}{subsection.14.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.2}{\ignorespaces Graphical illustration of ``between groups'' variation (panel a) and ``within groups'' variation (panel b). On the left, the arrows show the differences in the group means; on the right, the arrows highlight the variability within each group.}}{430}{figure.14.2}}
\newlabel{fig:anovavar}{{14.2}{430}{Graphical illustration of ``between groups'' variation (panel a) and ``within groups'' variation (panel b). On the left, the arrows show the differences in the group means; on the right, the arrows highlight the variability within each group}{figure.14.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2.3}From sums of squares to the $F$-test}{431}{subsection.14.2.3}}
\@writefile{lot}{\contentsline {table}{\numberline {14.1}{\ignorespaces All of the key quantities involved in an ANOVA, organised into a ``standard'' ANOVA table. The formulas for all quantities (except the $p$-value, which has a very ugly formula and would be nightmarishly hard to calculate without a computer) are shown.}}{432}{table.14.1}}
\newlabel{tab:anovatable}{{14.1}{432}{All of the key quantities involved in an ANOVA, organised into a ``standard'' ANOVA table. The formulas for all quantities (except the $p$-value, which has a very ugly formula and would be nightmarishly hard to calculate without a computer) are shown}{table.14.1}{}}
\newlabel{sec:anovamodel}{{14.2.4}{432}{The model for the data and the meaning of $F$ \advanced \label {sec:anovamodel}}{subsection.14.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2.4}The model for the data and the meaning of $F$ }{432}{subsection.14.2.4}}
\citation{Hays1994}
\@writefile{brf}{\backcite{Hays1994}{{433}{14.2}{subsection.14.2.4}}}
\@writefile{brf}{\backcite{Hays1994}{{433}{14.2}{subsection.14.2.4}}}
\@writefile{brf}{\backcite{Hays1994}{{433}{14.2}{subsection.14.2.4}}}
\newlabel{sec:anovacalc}{{14.2.5}{433}{A worked example \label {sec:anovacalc}}{subsection.14.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2.5}A worked example }{433}{subsection.14.2.5}}
\newlabel{sec:introduceaov}{{14.3}{437}{Running an ANOVA in \R \label {sec:introduceaov}}{section.14.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {14.3}Running an ANOVA in \textsf  {R}}{437}{section.14.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.1}Using the \texttt  {\relax \fontsize  {10.95}{14}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}aov()}} function to specify your ANOVA}{438}{subsection.14.3.1}}
\newlabel{sec:aovobjects}{{14.3.2}{438}{Understanding what the \rtext {aov()} function produces \label {sec:aovobjects}}{subsection.14.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.2}Understanding what the \texttt  {\relax \fontsize  {10.95}{14}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}aov()}} function produces }{438}{subsection.14.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.3}Running the hypothesis tests for the ANOVA}{439}{subsection.14.3.3}}
\newlabel{sec:etasquared}{{14.4}{440}{Effect size\label {sec:etasquared}}{section.14.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {14.4}Effect size}{440}{section.14.4}}
\newlabel{sec:posthoc}{{14.5}{441}{Multiple comparisons and post hoc tests~\label {sec:posthoc}}{section.14.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {14.5}Multiple comparisons and post hoc tests\nobreakspace  {}}{441}{section.14.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5.1}Running ``pairwise'' $t$-tests}{442}{subsection.14.5.1}}
\citation{Shaffer1995}
\citation{Hsu1996}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5.2}Corrections for multiple testing}{443}{subsection.14.5.2}}
\@writefile{brf}{\backcite{Shaffer1995}{{443}{14.5}{subsection.14.5.2}}}
\@writefile{brf}{\backcite{Shaffer1995}{{443}{14.5}{subsection.14.5.2}}}
\@writefile{brf}{\backcite{Shaffer1995}{{443}{14.5}{subsection.14.5.2}}}
\citation{Dunn1961}
\citation{Dunn1961}
\citation{Holm1979}
\@writefile{brf}{\backcite{Hsu1996}{{444}{14.5}{subsection.14.5.2}}}
\@writefile{brf}{\backcite{Hsu1996}{{444}{14.5}{subsection.14.5.2}}}
\@writefile{brf}{\backcite{Hsu1996}{{444}{14.5}{subsection.14.5.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5.3}Bonferroni corrections}{444}{subsection.14.5.3}}
\@writefile{brf}{\backcite{Dunn1961}{{444}{14.5}{subsection.14.5.3}}}
\@writefile{brf}{\backcite{Dunn1961}{{444}{14.5}{subsection.14.5.3}}}
\@writefile{brf}{\backcite{Dunn1961}{{444}{14.5}{subsection.14.5.3}}}
\@writefile{brf}{\backcite{Dunn1961}{{444}{14.5}{subsection.14.5.3}}}
\@writefile{brf}{\backcite{Dunn1961}{{444}{14.5}{subsection.14.5.3}}}
\@writefile{brf}{\backcite{Dunn1961}{{444}{14.5}{subsection.14.5.3}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5.4}Holm corrections}{445}{subsection.14.5.4}}
\@writefile{brf}{\backcite{Holm1979}{{445}{14.5}{subsection.14.5.4}}}
\@writefile{brf}{\backcite{Holm1979}{{445}{14.5}{subsection.14.5.4}}}
\@writefile{brf}{\backcite{Holm1979}{{445}{14.5}{subsection.14.5.4}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5.5}Writing up the post hoc test}{446}{subsection.14.5.5}}
\newlabel{sec:anovaassumptions}{{14.6}{446}{Assumptions of one-way ANOVA \label {sec:anovaassumptions}}{section.14.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {14.6}Assumptions of one-way ANOVA }{446}{section.14.6}}
\citation{Levene1960}
\citation{BrownForsythe1974}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.6.1}How robust is ANOVA?}{447}{subsection.14.6.1}}
\newlabel{sec:levene}{{14.7}{447}{Checking the homogeneity of variance assumption~\label {sec:levene}}{section.14.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {14.7}Checking the homogeneity of variance assumption\nobreakspace  {}}{447}{section.14.7}}
\@writefile{brf}{\backcite{Levene1960}{{447}{14.7}{section.14.7}}}
\@writefile{brf}{\backcite{Levene1960}{{447}{14.7}{section.14.7}}}
\@writefile{brf}{\backcite{Levene1960}{{447}{14.7}{section.14.7}}}
\@writefile{brf}{\backcite{BrownForsythe1974}{{447}{14.7}{section.14.7}}}
\@writefile{brf}{\backcite{BrownForsythe1974}{{447}{14.7}{section.14.7}}}
\@writefile{brf}{\backcite{BrownForsythe1974}{{447}{14.7}{section.14.7}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.7.1}Running the Levene's test in \textsf  {R}}{448}{subsection.14.7.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.7.2}Additional comments}{448}{subsection.14.7.2}}
\citation{Welch1951}
\newlabel{sec:welchoneway}{{14.8}{449}{Removing the homogeneity of variance assumption~\label {sec:welchoneway}}{section.14.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {14.8}Removing the homogeneity of variance assumption\nobreakspace  {}}{449}{section.14.8}}
\@writefile{brf}{\backcite{Welch1951}{{449}{14.8}{section.14.8}}}
\@writefile{brf}{\backcite{Welch1951}{{449}{14.8}{section.14.8}}}
\@writefile{brf}{\backcite{Welch1951}{{449}{14.8}{section.14.8}}}
\citation{KruskalWallis1952}
\newlabel{sec:anovanormality}{{14.9}{450}{Checking the normality assumption~\label {sec:anovanormality}}{section.14.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {14.9}Checking the normality assumption\nobreakspace  {}}{450}{section.14.9}}
\newlabel{sec:kruskalwallis}{{14.10}{450}{Removing the normality assumption~\label {sec:kruskalwallis}}{section.14.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {14.10}Removing the normality assumption\nobreakspace  {}}{450}{section.14.10}}
\@writefile{brf}{\backcite{KruskalWallis1952}{{450}{14.10}{section.14.10}}}
\@writefile{brf}{\backcite{KruskalWallis1952}{{450}{14.10}{section.14.10}}}
\@writefile{brf}{\backcite{KruskalWallis1952}{{450}{14.10}{section.14.10}}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.3}{\ignorespaces Histogram (panel a) and QQ plot (panel b) for the residuals of our ANOVA: both of these are in agreement with the Shapiro-Wilk test, and suggest that the residuals are normally distributed}}{451}{figure.14.3}}
\newlabel{fig:normalityanova}{{14.3}{451}{Histogram (panel a) and QQ plot (panel b) for the residuals of our ANOVA: both of these are in agreement with the Shapiro-Wilk test, and suggest that the residuals are normally distributed}{figure.14.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.10.1}The logic behind the Kruskal-Wallis test}{451}{subsection.14.10.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.10.2}Additional details}{452}{subsection.14.10.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.10.3}How to run the Kruskal-Wallis test in \textsf  {R}}{453}{subsection.14.10.3}}
\newlabel{sec:anovaandt}{{14.11}{453}{On the relationship between ANOVA and the Student $t$ test~\label {sec:anovaandt}}{section.14.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {14.11}On the relationship between ANOVA and the Student $t$ test\nobreakspace  {}}{453}{section.14.11}}
\citation{Sahai2000}
\@writefile{toc}{\contentsline {section}{\numberline {14.12}Summary}{454}{section.14.12}}
\@writefile{brf}{\backcite{Sahai2000}{{455}{14.12}{section.14.12}}}
\@writefile{brf}{\backcite{Sahai2000}{{455}{14.12}{section.14.12}}}
\@writefile{brf}{\backcite{Sahai2000}{{455}{14.12}{section.14.12}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {15}Linear regression}{457}{chapter.15}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:regression}{{15}{457}{Linear regression}{chapter.15}{}}
\newlabel{sec:introregression}{{15.1}{457}{What is a linear regression model?~\label {sec:introregression}}{section.15.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15.1}What is a linear regression model?\nobreakspace  {}}{457}{section.15.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.1}{\ignorespaces Scatterplot showing grumpiness as a function of hours slept.}}{458}{figure.15.1}}
\newlabel{fig:regression0}{{15.1}{458}{Scatterplot showing grumpiness as a function of hours slept}{figure.15.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.2}{\ignorespaces Panel a shows the sleep-grumpiness scatterplot from Figure\nobreakspace  {}\ref  {fig:regression0} with the best fitting regression line drawn over the top. Not surprisingly, the line goes through the middle of the data. In contrast, panel b shows the same data, but with a very poor choice of regression line drawn over the top.}}{459}{figure.15.2}}
\newlabel{fig:regression1}{{15.2}{459}{Panel a shows the sleep-grumpiness scatterplot from Figure~\ref {fig:regression0} with the best fitting regression line drawn over the top. Not surprisingly, the line goes through the middle of the data. In contrast, panel b shows the same data, but with a very poor choice of regression line drawn over the top}{figure.15.2}{}}
\newlabel{sec:regressionestimation}{{15.2}{459}{Estimating a linear regression model~\label {sec:regressionestimation}}{section.15.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15.2}Estimating a linear regression model\nobreakspace  {}}{459}{section.15.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.3}{\ignorespaces A depiction of the residuals associated with the best fitting regression line (panel a), and the residuals associated with a poor regression line (panel b). The residuals are much smaller for the good regression line. Again, this is no surprise given that the good line is the one that goes right through the middle of the data.}}{460}{figure.15.3}}
\newlabel{fig:regression3}{{15.3}{460}{A depiction of the residuals associated with the best fitting regression line (panel a), and the residuals associated with a poor regression line (panel b). The residuals are much smaller for the good regression line. Again, this is no surprise given that the good line is the one that goes right through the middle of the data}{figure.15.3}{}}
\newlabel{sec:lm}{{15.2.1}{460}{Using the \rtext {lm()} function~\label {sec:lm}}{subsection.15.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.2.1}Using the \texttt  {\relax \fontsize  {10.95}{14}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}lm()}} function\nobreakspace  {}}{460}{subsection.15.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.2.2}Interpreting the estimated model}{461}{subsection.15.2.2}}
\newlabel{sec:multipleregression}{{15.3}{461}{Multiple linear regression~\label {sec:multipleregression}}{section.15.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15.3}Multiple linear regression\nobreakspace  {}}{461}{section.15.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3.1}Doing it in \textsf  {R}}{462}{subsection.15.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.4}{\ignorespaces A 3D visualisation of a multiple regression model. There are two predictors in the model, \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}dan.sleep}} and \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}baby.sleep}}; the outcome variable is \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}dan.grump}}. Together, these three variables form a 3D space: each observation (blue dots) is a point in this space. In much the same way that a simple linear regression model forms a line in 2D space, this multiple regression model forms a plane in 3D space. When we estimate the regression coefficients, what we're trying to do is find a plane that is as close to all the blue dots as possible. (This plot was drawn using the \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}scatter3d()}} function in the \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}car}} package, and it looked much nicer before it got butchered by the image conversion process that I used to get it into the book pdf)}}{463}{figure.15.4}}
\newlabel{fig:multipleregression}{{15.4}{463}{A 3D visualisation of a multiple regression model. There are two predictors in the model, \rtext {dan.sleep} and \rtext {baby.sleep}; the outcome variable is \rtext {dan.grump}. Together, these three variables form a 3D space: each observation (blue dots) is a point in this space. In much the same way that a simple linear regression model forms a line in 2D space, this multiple regression model forms a plane in 3D space. When we estimate the regression coefficients, what we're trying to do is find a plane that is as close to all the blue dots as possible. (This plot was drawn using the \rtext {scatter3d()} function in the \rtext {car} package, and it looked much nicer before it got butchered by the image conversion process that I used to get it into the book pdf)}{figure.15.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3.2}Formula for the general case}{464}{subsection.15.3.2}}
\newlabel{sec:r2}{{15.4}{464}{Quantifying the fit of the regression model~\label {sec:r2}}{section.15.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15.4}Quantifying the fit of the regression model\nobreakspace  {}}{464}{section.15.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4.1}The $R^2$ value}{464}{subsection.15.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4.2}The relationship between regression and correlation}{465}{subsection.15.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4.3}The adjusted $R^2$ value}{466}{subsection.15.4.3}}
\newlabel{sec:regressiontests}{{15.5}{466}{Hypothesis tests for regression models~\label {sec:regressiontests}}{section.15.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15.5}Hypothesis tests for regression models\nobreakspace  {}}{466}{section.15.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.5.1}Testing the model as a whole}{466}{subsection.15.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.5.2}Tests for individual coefficients}{467}{subsection.15.5.2}}
\newlabel{sec:regressionsummary}{{15.5.3}{468}{Running the hypothesis tests in \R ~\label {sec:regressionsummary}}{subsection.15.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.5.3}Running the hypothesis tests in \textsf  {R}\nobreakspace  {}}{468}{subsection.15.5.3}}
\newlabel{sec:corrhyp}{{15.6}{470}{Testing the significance of a correlation~\label {sec:corrhyp}}{section.15.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15.6}Testing the significance of a correlation\nobreakspace  {}}{470}{section.15.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.6.1}Hypothesis tests for a single correlation}{470}{subsection.15.6.1}}
\newlabel{sec:corrhyp2}{{15.6.2}{471}{Hypothesis tests for all pairwise correlations~\label {sec:corrhyp2}}{subsection.15.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.6.2}Hypothesis tests for all pairwise correlations\nobreakspace  {}}{471}{subsection.15.6.2}}
\newlabel{sec:regressioncoefs}{{15.7}{472}{Regarding regression coefficients~\label {sec:regressioncoefs}}{section.15.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15.7}Regarding regression coefficients\nobreakspace  {}}{472}{section.15.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.7.1}Confidence intervals for the coefficients}{472}{subsection.15.7.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.7.2}Calculating standardised regression coefficients}{473}{subsection.15.7.2}}
\newlabel{sec:regressionassumptions}{{15.8}{474}{Assumptions of regression~\label {sec:regressionassumptions}}{section.15.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15.8}Assumptions of regression\nobreakspace  {}}{474}{section.15.8}}
\citation{Fox2011}
\citation{Fox2011}
\newlabel{sec:regressiondiagnostics}{{15.9}{475}{Model checking~\label {sec:regressiondiagnostics}}{section.15.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15.9}Model checking\nobreakspace  {}}{475}{section.15.9}}
\@writefile{brf}{\backcite{Fox2011}{{475}{15.9}{section.15.9}}}
\@writefile{brf}{\backcite{Fox2011}{{475}{15.9}{section.15.9}}}
\@writefile{brf}{\backcite{Fox2011}{{475}{15.9}{section.15.9}}}
\@writefile{brf}{\backcite{Fox2011}{{475}{15.9}{section.15.9}}}
\@writefile{brf}{\backcite{Fox2011}{{475}{15.9}{section.15.9}}}
\@writefile{brf}{\backcite{Fox2011}{{475}{15.9}{section.15.9}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.9.1}Three kinds of residuals}{475}{subsection.15.9.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.5}{\ignorespaces An illustration of outliers. The dotted lines plot the regression line that would have been estimated without the anomalous observation included, and the corresponding residual (i.e., the Studentised residual). The solid line shows the regression line with the anomalous observation included. The outlier has an unusual value on the outcome (y axis location) but not the predictor (x axis location), and lies a long way from the regression line.}}{477}{figure.15.5}}
\newlabel{fig:outlier}{{15.5}{477}{An illustration of outliers. The dotted lines plot the regression line that would have been estimated without the anomalous observation included, and the corresponding residual (i.e., the Studentised residual). The solid line shows the regression line with the anomalous observation included. The outlier has an unusual value on the outcome (y axis location) but not the predictor (x axis location), and lies a long way from the regression line}{figure.15.5}{}}
\newlabel{sec:regressionoutliers}{{15.9.2}{477}{Three kinds of anomalous data~\label {sec:regressionoutliers}}{subsection.15.9.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.9.2}Three kinds of anomalous data\nobreakspace  {}}{477}{subsection.15.9.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.6}{\ignorespaces An illustration of high leverage points. The anomalous observation in this case is unusual both in terms of the predictor (x axis) and the outcome (y axis), but this unusualness is highly consistent with the pattern of correlations that exists among the other observations; as a consequence, the observation falls very close to the regression line and does not distort it.}}{478}{figure.15.6}}
\newlabel{fig:leverage}{{15.6}{478}{An illustration of high leverage points. The anomalous observation in this case is unusual both in terms of the predictor (x axis) and the outcome (y axis), but this unusualness is highly consistent with the pattern of correlations that exists among the other observations; as a consequence, the observation falls very close to the regression line and does not distort it}{figure.15.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.7}{\ignorespaces An illustration of high influence points. In this case, the anomalous observation is highly unusual on the predictor variable (x axis), and falls a long way from the regression line. As a consequence, the regression line is highly distorted, even though (in this case) the anomalous observation is entirely typical in terms of the outcome variable (y axis).}}{479}{figure.15.7}}
\newlabel{fig:influence}{{15.7}{479}{An illustration of high influence points. In this case, the anomalous observation is highly unusual on the predictor variable (x axis), and falls a long way from the regression line. As a consequence, the regression line is highly distorted, even though (in this case) the anomalous observation is entirely typical in terms of the outcome variable (y axis)}{figure.15.7}{}}
\newlabel{sec:regressionnormality}{{15.9.3}{480}{Checking the normality of the residuals~\label {sec:regressionnormality}}{subsection.15.9.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.9.3}Checking the normality of the residuals\nobreakspace  {}}{480}{subsection.15.9.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.8}{\ignorespaces Cook's distance for every observation. This is one of the standard regression plots produced by the \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}plot()}} function when the input is a linear regression object. It is obtained by setting \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}which=4}}.}}{481}{figure.15.8}}
\newlabel{fig:regressionplot4}{{15.8}{481}{Cook's distance for every observation. This is one of the standard regression plots produced by the \rtext {plot()} function when the input is a linear regression object. It is obtained by setting \rtext {which=4}}{figure.15.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.9}{\ignorespaces Residuals versus leverage. This is one of the standard regression plots produced by the \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}plot()}} function when the input is a linear regression object. It is obtained by setting \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}which=5}}.}}{481}{figure.15.9}}
\newlabel{fig:regressionplot5}{{15.9}{481}{Residuals versus leverage. This is one of the standard regression plots produced by the \rtext {plot()} function when the input is a linear regression object. It is obtained by setting \rtext {which=5}}{figure.15.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.10}{\ignorespaces A histogram of the (ordinary) residuals in the \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}regression.2}} model. These residuals look very close to being normally distributed, much moreso than is typically seen with real data. This shouldn't surprise you... they aren't real data, and they aren't real residuals!}}{482}{figure.15.10}}
\newlabel{fig:residhist}{{15.10}{482}{A histogram of the (ordinary) residuals in the \rtext {regression.2} model. These residuals look very close to being normally distributed, much moreso than is typically seen with real data. This shouldn't surprise you... they aren't real data, and they aren't real residuals!}{figure.15.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.11}{\ignorespaces Plot of the theoretical quantiles according to the model, against the quantiles of the standardised residuals. This is one of the standard regression plots produced by the \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}plot()}} function when the input is a linear regression object. It is obtained by setting \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}which=2}}. }}{483}{figure.15.11}}
\newlabel{fig:regressionplot2}{{15.11}{483}{Plot of the theoretical quantiles according to the model, against the quantiles of the standardised residuals. This is one of the standard regression plots produced by the \rtext {plot()} function when the input is a linear regression object. It is obtained by setting \rtext {which=2}}{figure.15.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.12}{\ignorespaces Plot of the fitted values against the observed values of the outcome variable. A straight line is what we're hoping to see here. This looks pretty good, suggesting that there's nothing grossly wrong, but there could be hidden subtle issues. }}{484}{figure.15.12}}
\newlabel{fig:regressionlinearity}{{15.12}{484}{Plot of the fitted values against the observed values of the outcome variable. A straight line is what we're hoping to see here. This looks pretty good, suggesting that there's nothing grossly wrong, but there could be hidden subtle issues}{figure.15.12}{}}
\newlabel{sec:regressionlinearity}{{15.9.4}{484}{Checking the linearity of the relationship~\label {sec:regressionlinearity}}{subsection.15.9.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.9.4}Checking the linearity of the relationship\nobreakspace  {}}{484}{subsection.15.9.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.13}{\ignorespaces Plot of the fitted values against the residuals for \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}regression.2}}, with a line showing the relationship between the two. If this is horizontal and straight, then we can feel reasonably confident that the ``average residual'' for all ``fitted values'' is more or less the same. This is one of the standard regression plots produced by the \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}plot()}} function when the input is a linear regression object. It is obtained by setting \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}which=1}}.}}{485}{figure.15.13}}
\newlabel{fig:regressionplot1}{{15.13}{485}{Plot of the fitted values against the residuals for \rtext {regression.2}, with a line showing the relationship between the two. If this is horizontal and straight, then we can feel reasonably confident that the ``average residual'' for all ``fitted values'' is more or less the same. This is one of the standard regression plots produced by the \rtext {plot()} function when the input is a linear regression object. It is obtained by setting \rtext {which=1}}{figure.15.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.14}{\ignorespaces Plot of the fitted values against the residuals for \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}regression.2}}, along with similar plots for the two predictors individually. This plot is produced by the \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}residualPlots()}} function in the \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}car}} package. Note that it refers to the residuals as ``Pearson residuals'', but in this context these are the same as ordinary residuals. }}{486}{figure.15.14}}
\newlabel{fig:residualPlots}{{15.14}{486}{Plot of the fitted values against the residuals for \rtext {regression.2}, along with similar plots for the two predictors individually. This plot is produced by the \rtext {residualPlots()} function in the \rtext {car} package. Note that it refers to the residuals as ``Pearson residuals'', but in this context these are the same as ordinary residuals}{figure.15.14}{}}
\citation{Cook1983}
\citation{White1980}
\citation{Fox2011}
\citation{Long2000}
\newlabel{sec:regressionhomogeneity}{{15.9.5}{487}{Checking the homogeneity of variance~\label {sec:regressionhomogeneity}}{subsection.15.9.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.9.5}Checking the homogeneity of variance\nobreakspace  {}}{487}{subsection.15.9.5}}
\@writefile{brf}{\backcite{Cook1983}{{487}{15.9}{figure.15.15}}}
\@writefile{brf}{\backcite{Cook1983}{{487}{15.9}{figure.15.15}}}
\@writefile{brf}{\backcite{Cook1983}{{487}{15.9}{figure.15.15}}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.15}{\ignorespaces Plot of the fitted values (model predictions) against the square root of the abs standardised residuals. This plot is used to diagnose violations of homogeneity of variance. If the variance is really constant, then the line through the middle should be horizontal and flat. This is one of the standard regression plots produced by the \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}plot()}} function when the input is a linear regression object. It is obtained by setting \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}which=3}}.}}{488}{figure.15.15}}
\newlabel{fig:regressionplot3}{{15.15}{488}{Plot of the fitted values (model predictions) against the square root of the abs standardised residuals. This plot is used to diagnose violations of homogeneity of variance. If the variance is really constant, then the line through the middle should be horizontal and flat. This is one of the standard regression plots produced by the \rtext {plot()} function when the input is a linear regression object. It is obtained by setting \rtext {which=3}}{figure.15.15}{}}
\@writefile{brf}{\backcite{White1980}{{488}{14}{Hfootnote.226}}}
\@writefile{brf}{\backcite{White1980}{{488}{14}{Hfootnote.226}}}
\@writefile{brf}{\backcite{White1980}{{488}{14}{Hfootnote.226}}}
\@writefile{brf}{\backcite{Fox2011}{{488}{14}{Hfootnote.226}}}
\@writefile{brf}{\backcite{Fox2011}{{488}{14}{Hfootnote.226}}}
\@writefile{brf}{\backcite{Fox2011}{{488}{14}{Hfootnote.226}}}
\@writefile{brf}{\backcite{Long2000}{{488}{14}{Hfootnote.226}}}
\@writefile{brf}{\backcite{Long2000}{{488}{14}{Hfootnote.226}}}
\@writefile{brf}{\backcite{Long2000}{{488}{14}{Hfootnote.226}}}
\newlabel{sec:regressioncollinearity}{{15.9.6}{489}{Checking for collinearity~\label {sec:regressioncollinearity}}{subsection.15.9.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.9.6}Checking for collinearity\nobreakspace  {}}{489}{subsection.15.9.6}}
\citation{Akaike1974}
\newlabel{sec:modelselreg}{{15.10}{490}{Model selection\label {sec:modelselreg}}{section.15.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15.10}Model selection}{490}{section.15.10}}
\@writefile{brf}{\backcite{Akaike1974}{{490}{15.10}{section.15.10}}}
\@writefile{brf}{\backcite{Akaike1974}{{490}{15.10}{section.15.10}}}
\@writefile{brf}{\backcite{Akaike1974}{{490}{15.10}{section.15.10}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.10.1}Backward elimination}{491}{subsection.15.10.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.10.2}Forward selection}{492}{subsection.15.10.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.10.3}A caveat}{493}{subsection.15.10.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.10.4}Comparing two regression models}{493}{subsection.15.10.4}}
\@writefile{toc}{\contentsline {section}{\numberline {15.11}Summary}{495}{section.15.11}}
\@writefile{toc}{\contentsline {chapter}{\numberline {16}Factorial ANOVA }{497}{chapter.16}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:anova2}{{16}{497}{Factorial ANOVA}{chapter.16}{}}
\newlabel{sec:factorialanovasimple}{{16.1}{497}{Factorial ANOVA 1: balanced designs, no interactions\label {sec:factorialanovasimple}}{section.16.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.1}Factorial ANOVA 1: balanced designs, no interactions}{497}{section.16.1}}
\newlabel{sec:factanovahyp}{{16.1.1}{498}{What hypotheses are we testing?\label {sec:factanovahyp}}{subsection.16.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.1}What hypotheses are we testing?}{498}{subsection.16.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.2}Running the analysis in \textsf  {R}}{500}{subsection.16.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.3}How are the sum of squares calculated?}{502}{subsection.16.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.4}What are our degrees of freedom?}{505}{subsection.16.1.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.5}Factorial ANOVA versus one-way ANOVAs}{505}{subsection.16.1.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.6}What kinds of outcomes does this analysis capture?}{506}{subsection.16.1.6}}
\newlabel{sec:interactions}{{16.2}{506}{Factorial ANOVA 2: balanced designs, interactions allowed\label {sec:interactions}}{section.16.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.2}Factorial ANOVA 2: balanced designs, interactions allowed}{506}{section.16.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.1}{\ignorespaces The four different outcomes for a $2 \times 2$ ANOVA when no interactions are present. In panel (a) we see a main effect of Factor A, and no effect of Factor B. Panel (b) shows a main effect of Factor B but no effect of Factor A. Panel (c) shows main effects of both Factor A and Factor B. Finally, panel (d) shows no effect of either factor.}}{507}{figure.16.1}}
\newlabel{fig:maineffects}{{16.1}{507}{The four different outcomes for a $2 \times 2$ ANOVA when no interactions are present. In panel (a) we see a main effect of Factor A, and no effect of Factor B. Panel (b) shows a main effect of Factor B but no effect of Factor A. Panel (c) shows main effects of both Factor A and Factor B. Finally, panel (d) shows no effect of either factor}{figure.16.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.2}{\ignorespaces Qualitatively different interactions for a $2 \times 2$ ANOVA}}{508}{figure.16.2}}
\newlabel{fig:interaction}{{16.2}{508}{Qualitatively different interactions for a $2 \times 2$ ANOVA}{figure.16.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.2.1}What exactly \relax $\@@underline {\hbox {is}}\mathsurround \z@ $\relax  an interaction effect?}{509}{subsection.16.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.3}{\ignorespaces An interaction plot for the group means in the clinical trial data. The command to produce it is included in the main text. You'll notice that the legend doesn't quite fit properly. You can fix this by playing around with the \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}x.leg}} and \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}y.leg}} arguments: type \texttt  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \leavevmode {\color  {Rtextcol}?lineplot.CI}} for details.}}{510}{figure.16.3}}
\newlabel{fig:interactionplot}{{16.3}{510}{An interaction plot for the group means in the clinical trial data. The command to produce it is included in the main text. You'll notice that the legend doesn't quite fit properly. You can fix this by playing around with the \rtext {x.leg} and \rtext {y.leg} arguments: type \rtext {?lineplot.CI} for details}{figure.16.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.2.2}Calculating sums of squares for the interaction}{510}{subsection.16.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.2.3}Degrees of freedom for the interaction}{511}{subsection.16.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.2.4}Running the ANOVA in \textsf  {R}}{512}{subsection.16.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.2.5}Interpreting the results}{512}{subsection.16.2.5}}
\newlabel{sec:effectsizefactorialanova}{{16.3}{513}{Effect size, estimated means, and confidence intervals\label {sec:effectsizefactorialanova}}{section.16.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.3}Effect size, estimated means, and confidence intervals}{513}{section.16.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.1}Effect sizes}{513}{subsection.16.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.2}Estimated group means}{515}{subsection.16.3.2}}
\newlabel{sec:factorialanovaassumptions}{{16.4}{517}{Assumption checking~\label {sec:factorialanovaassumptions}}{section.16.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.4}Assumption checking\nobreakspace  {}}{517}{section.16.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.4.1}Levene test for homogeneity of variance}{517}{subsection.16.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.4.2}Normality of residuals}{518}{subsection.16.4.2}}
\newlabel{sec:omnibusF}{{16.5}{518}{The $F$ test as a model comparison\label {sec:omnibusF}}{section.16.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.5}The $F$ test as a model comparison}{518}{section.16.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.5.1}The $F$ test comparing two models}{519}{subsection.16.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.5.2}Running the test in \textsf  {R}}{520}{subsection.16.5.2}}
\newlabel{sec:anovalm}{{16.6}{521}{ANOVA as a linear model\label {sec:anovalm}}{section.16.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.6}ANOVA as a linear model}{521}{section.16.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.6.1}Some data}{522}{subsection.16.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.6.2}ANOVA with binary factors as a regression model}{523}{subsection.16.6.2}}
\newlabel{sec:changingbaseline}{{16.6.3}{526}{Changing the baseline category\label {sec:changingbaseline}}{subsection.16.6.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.6.3}Changing the baseline category}{526}{subsection.16.6.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.6.4}How to encode non binary factors as contrasts}{528}{subsection.16.6.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.6.5}The equivalence between ANOVA and regression for non-binary factors}{529}{subsection.16.6.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.6.6}Degrees of freedom as parameter counting!}{530}{subsection.16.6.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.6.7}A postscript}{531}{subsection.16.6.7}}
\newlabel{sec:contrasts}{{16.7}{532}{Different ways to specify contrasts\label {sec:contrasts}}{section.16.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.7}Different ways to specify contrasts}{532}{section.16.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.7.1}Treatment contrasts}{532}{subsection.16.7.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.7.2}Helmert contrasts}{533}{subsection.16.7.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.7.3}Sum to zero contrasts}{534}{subsection.16.7.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.7.4}Viewing and setting the default contrasts in \textsf  {R}}{534}{subsection.16.7.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.7.5}Setting the contrasts for a single factor}{535}{subsection.16.7.5}}
\citation{Hsu1996}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.7.6}Setting the contrasts for a single analysis}{536}{subsection.16.7.6}}
\newlabel{sec:posthoc2}{{16.8}{537}{Post hoc tests\label {sec:posthoc2}}{section.16.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.8}Post hoc tests}{537}{section.16.8}}
\@writefile{brf}{\backcite{Hsu1996}{{537}{16.8}{section.16.8}}}
\@writefile{brf}{\backcite{Hsu1996}{{537}{16.8}{section.16.8}}}
\@writefile{brf}{\backcite{Hsu1996}{{537}{16.8}{section.16.8}}}
\newlabel{sec:plannedcomparisons}{{16.9}{539}{The method of planned comparisons\label {sec:plannedcomparisons}}{section.16.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.9}The method of planned comparisons}{539}{section.16.9}}
\newlabel{sec:unbalancedanova}{{16.10}{539}{Factorial ANOVA 3: unbalanced designs\label {sec:unbalancedanova}}{section.16.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.10}Factorial ANOVA 3: unbalanced designs}{539}{section.16.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.10.1}The coffee data}{540}{subsection.16.10.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.10.2}``Standard ANOVA'' does not exist for unbalanced designs}{541}{subsection.16.10.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.10.3}Type I sum of squares}{542}{subsection.16.10.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.10.4}Type III sum of squares}{544}{subsection.16.10.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.10.5}Type II sum of squares}{547}{subsection.16.10.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.10.6}Effect sizes (and non-additive sums of squares)}{549}{subsection.16.10.6}}
\@writefile{toc}{\contentsline {section}{\numberline {16.11}Summary}{551}{section.16.11}}
\@writefile{toc}{\contentsline {part}{VI\hspace  {1em}Endings, alternatives and prospects}{553}{part.6}}
\@writefile{toc}{\contentsline {chapter}{\numberline {17}Bayesian statistics }{555}{chapter.17}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:bayes}{{17}{555}{Bayesian statistics}{chapter.17}{}}
\newlabel{sec:basicbayes}{{17.1}{555}{Probabilistic reasoning by rational agents\label {sec:basicbayes}}{section.17.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {17.1}Probabilistic reasoning by rational agents}{555}{section.17.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.1.1}Priors: what you believed before}{556}{subsection.17.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.1.2}Likelihoods: theories about the data}{556}{subsection.17.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.1.3}The joint probability of data and hypothesis}{557}{subsection.17.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.1.4}Updating beliefs using Bayes' rule}{558}{subsection.17.1.4}}
\newlabel{sec:bayesianhypothesistests}{{17.2}{560}{Bayesian hypothesis tests~\label {sec:bayesianhypothesistests}}{section.17.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {17.2}Bayesian hypothesis tests\nobreakspace  {}}{560}{section.17.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.2.1}The Bayes factor}{560}{subsection.17.2.1}}
\citation{Jeffreys1961}
\citation{Kass1995}
\citation{Kass1995}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.2.2}Interpreting Bayes factors}{561}{subsection.17.2.2}}
\@writefile{brf}{\backcite{Jeffreys1961}{{561}{17.2.2}{subsection.17.2.2}}}
\@writefile{brf}{\backcite{Jeffreys1961}{{561}{17.2.2}{subsection.17.2.2}}}
\@writefile{brf}{\backcite{Jeffreys1961}{{561}{17.2.2}{subsection.17.2.2}}}
\@writefile{brf}{\backcite{Kass1995}{{561}{17.2.2}{subsection.17.2.2}}}
\@writefile{brf}{\backcite{Kass1995}{{561}{17.2.2}{subsection.17.2.2}}}
\@writefile{brf}{\backcite{Kass1995}{{561}{17.2.2}{subsection.17.2.2}}}
\@writefile{brf}{\backcite{Kass1995}{{561}{17.2.2}{subsection.17.2.2}}}
\@writefile{brf}{\backcite{Kass1995}{{561}{17.2.2}{subsection.17.2.2}}}
\@writefile{brf}{\backcite{Kass1995}{{561}{17.2.2}{subsection.17.2.2}}}
\newlabel{sec:whybayes}{{17.3}{562}{Why be a Bayesian?\label {sec:whybayes}}{section.17.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {17.3}Why be a Bayesian?}{562}{section.17.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.1}Statistics that mean what you think they mean}{562}{subsection.17.3.1}}
\citation{Fisher1925}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.2}Evidentiary standards you can believe}{563}{subsection.17.3.2}}
\@writefile{brf}{\backcite{Fisher1925}{{563}{17.3.2}{subsection.17.3.2}}}
\@writefile{brf}{\backcite{Fisher1925}{{563}{17.3.2}{subsection.17.3.2}}}
\@writefile{brf}{\backcite{Fisher1925}{{563}{17.3.2}{subsection.17.3.2}}}
\citation{Johnson2013}
\citation{Johnson2013}
\citation{Johnson2013}
\@writefile{brf}{\backcite{Johnson2013}{{564}{17.3.2}{subsection.17.3.2}}}
\@writefile{brf}{\backcite{Johnson2013}{{564}{17.3.2}{subsection.17.3.2}}}
\@writefile{brf}{\backcite{Johnson2013}{{564}{17.3.2}{subsection.17.3.2}}}
\@writefile{brf}{\backcite{Johnson2013}{{564}{17.3.2}{subsection.17.3.2}}}
\@writefile{brf}{\backcite{Johnson2013}{{564}{17.3.2}{subsection.17.3.2}}}
\@writefile{brf}{\backcite{Johnson2013}{{564}{17.3.2}{subsection.17.3.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.3}The $p$-value is a lie.}{564}{subsection.17.3.3}}
\@writefile{brf}{\backcite{Johnson2013}{{564}{17.3.3}{subsection.17.3.3}}}
\@writefile{brf}{\backcite{Johnson2013}{{564}{17.3.3}{subsection.17.3.3}}}
\@writefile{brf}{\backcite{Johnson2013}{{564}{17.3.3}{subsection.17.3.3}}}
\citation{Johnson2013}
\citation{Johnson2013}
\@writefile{lof}{\contentsline {figure}{\numberline {17.1}{\ignorespaces How badly can things go wrong if you re-run your tests every time new data arrive? If you are a frequentist, the answer is ``very wrong''.}}{566}{figure.17.1}}
\newlabel{fig:type1}{{17.1}{566}{How badly can things go wrong if you re-run your tests every time new data arrive? If you are a frequentist, the answer is ``very wrong''}{figure.17.1}{}}
\@writefile{brf}{\backcite{Johnson2013}{{567}{17.3.3}{figure.17.1}}}
\@writefile{brf}{\backcite{Johnson2013}{{567}{17.3.3}{figure.17.1}}}
\@writefile{brf}{\backcite{Johnson2013}{{567}{17.3.3}{figure.17.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.4}Is it really this bad?}{567}{subsection.17.3.4}}
\@writefile{brf}{\backcite{Johnson2013}{{567}{13}{Hfootnote.264}}}
\@writefile{brf}{\backcite{Johnson2013}{{567}{13}{Hfootnote.264}}}
\@writefile{brf}{\backcite{Johnson2013}{{567}{13}{Hfootnote.264}}}
\newlabel{sec:bayescontingency}{{17.4}{568}{Bayesian analysis of contingency tables~\label {sec:bayescontingency}}{section.17.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {17.4}Bayesian analysis of contingency tables\nobreakspace  {}}{568}{section.17.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.4.1}The orthodox text}{569}{subsection.17.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.4.2}The Bayesian test}{569}{subsection.17.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.4.3}Writing up the results}{571}{subsection.17.4.3}}
\citation{Gunel1974}
\citation{Morey2015}
\citation{Morey2015}
\citation{Gunel1974}
\citation{Morey2015}
\@writefile{brf}{\backcite{Gunel1974}{{572}{17.4}{subsection.17.4.3}}}
\@writefile{brf}{\backcite{Gunel1974}{{572}{17.4}{subsection.17.4.3}}}
\@writefile{brf}{\backcite{Gunel1974}{{572}{17.4}{subsection.17.4.3}}}
\@writefile{brf}{\backcite{Morey2015}{{572}{17.4}{subsection.17.4.3}}}
\@writefile{brf}{\backcite{Morey2015}{{572}{17.4}{subsection.17.4.3}}}
\@writefile{brf}{\backcite{Morey2015}{{572}{17.4}{subsection.17.4.3}}}
\@writefile{brf}{\backcite{Morey2015}{{572}{17.4}{subsection.17.4.3}}}
\@writefile{brf}{\backcite{Morey2015}{{572}{17.4}{subsection.17.4.3}}}
\@writefile{brf}{\backcite{Morey2015}{{572}{17.4}{subsection.17.4.3}}}
\@writefile{brf}{\backcite{Gunel1974}{{572}{17.4}{subsection.17.4.3}}}
\@writefile{brf}{\backcite{Gunel1974}{{572}{17.4}{subsection.17.4.3}}}
\@writefile{brf}{\backcite{Gunel1974}{{572}{17.4}{subsection.17.4.3}}}
\@writefile{brf}{\backcite{Morey2015}{{572}{17.4}{subsection.17.4.3}}}
\@writefile{brf}{\backcite{Morey2015}{{572}{17.4}{subsection.17.4.3}}}
\@writefile{brf}{\backcite{Morey2015}{{572}{17.4}{subsection.17.4.3}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.4.4}Other sampling plans}{572}{subsection.17.4.4}}
\newlabel{sec:ttestbf}{{17.5}{574}{Bayesian $t$-tests\label {sec:ttestbf}}{section.17.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {17.5}Bayesian $t$-tests}{574}{section.17.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.1}Independent samples $t$-test}{574}{subsection.17.5.1}}
\citation{Rouder2009}
\@writefile{brf}{\backcite{Rouder2009}{{575}{19}{Hfootnote.270}}}
\@writefile{brf}{\backcite{Rouder2009}{{575}{19}{Hfootnote.270}}}
\@writefile{brf}{\backcite{Rouder2009}{{575}{19}{Hfootnote.270}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.2}Paired samples $t$-test}{576}{subsection.17.5.2}}
\newlabel{sec:bayesregression}{{17.6}{577}{Bayesian regression~\label {sec:bayesregression}}{section.17.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {17.6}Bayesian regression\nobreakspace  {}}{577}{section.17.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.6.1}A quick refresher}{577}{subsection.17.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.6.2}The Bayesian version}{578}{subsection.17.6.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.6.3}Finding the best model}{578}{subsection.17.6.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.6.4}Extracting Bayes factors for all included terms}{580}{subsection.17.6.4}}
\newlabel{sec:bayesanova}{{17.7}{581}{Bayesian ANOVA~\label {sec:bayesanova}}{section.17.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {17.7}Bayesian ANOVA\nobreakspace  {}}{581}{section.17.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.7.1}A quick refresher}{581}{subsection.17.7.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.7.2}The Bayesian version}{582}{subsection.17.7.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.7.3}Constructing Bayesian Type II tests}{583}{subsection.17.7.3}}
\citation{Kruschke2011}
\citation{Lee2014}
\@writefile{toc}{\contentsline {section}{\numberline {17.8}Summary}{584}{section.17.8}}
\@writefile{brf}{\backcite{Kruschke2011}{{584}{17.8}{section.17.8}}}
\@writefile{brf}{\backcite{Kruschke2011}{{584}{17.8}{section.17.8}}}
\@writefile{brf}{\backcite{Kruschke2011}{{584}{17.8}{section.17.8}}}
\@writefile{brf}{\backcite{Lee2014}{{585}{17.8}{section.17.8}}}
\@writefile{brf}{\backcite{Lee2014}{{585}{17.8}{section.17.8}}}
\@writefile{brf}{\backcite{Lee2014}{{585}{17.8}{section.17.8}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {18}Epilogue}{587}{chapter.18}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {18.1}The undiscovered statistics}{587}{section.18.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.1.1}Omissions within the topics covered}{587}{subsection.18.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.1.2}Statistical models missing from the book}{588}{subsection.18.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.1.3}Other ways of doing inference}{592}{subsection.18.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.1.4}Miscellaneous topics}{593}{subsection.18.1.4}}
\@writefile{toc}{\contentsline {section}{\numberline {18.2}Learning the basics, and learning them in \textsf  {R}}{595}{section.18.2}}
\bibdata{../tex/refs}
\bibcite{Adair1984}{\citeauthoryear {Adair}{Adair}{{\APACyear {1984}}}}
\APACbibcite{Adair1984}{\citeauthoryear {Adair}{Adair}{{\APACyear {1984}}}}
\bibcite{Agresti1996}{\citeauthoryear {Agresti}{Agresti}{{\APACyear {1996}}}}
\APACbibcite{Agresti1996}{\citeauthoryear {Agresti}{Agresti}{{\APACyear {1996}}}}
\bibcite{Agresti2002}{\citeauthoryear {Agresti}{Agresti}{{\APACyear {2002}}}}
\APACbibcite{Agresti2002}{\citeauthoryear {Agresti}{Agresti}{{\APACyear {2002}}}}
\bibcite{Akaike1974}{\citeauthoryear {Akaike}{Akaike}{{\APACyear {1974}}}}
\APACbibcite{Akaike1974}{\citeauthoryear {Akaike}{Akaike}{{\APACyear {1974}}}}
\bibcite{Anscombe1973}{\citeauthoryear {Anscombe}{Anscombe}{{\APACyear {1973}}}}
\APACbibcite{Anscombe1973}{\citeauthoryear {Anscombe}{Anscombe}{{\APACyear {1973}}}}
\bibcite{Bickel1975}{\citeauthoryear {Bickel, Hammel,{}\ \BBA{} O'Connell}{Bickel\ \BOthers {.}}{{\APACyear {1975}}}}
\APACbibcite{Bickel1975}{\citeauthoryear {Bickel, Hammel,{}\ \BBA{} O'Connell}{Bickel\ \BOthers {.}}{{\APACyear {1975}}}}
\bibcite{Box1976}{\citeauthoryear {G\BPBI  E\BPBI  P.\nobreakspace  {}Box}{G\BPBI  E\BPBI  P.\nobreakspace  {}Box}{{\APACyear {1976}}}}
\APACbibcite{Box1976}{\citeauthoryear {G\BPBI  E\BPBI  P.\nobreakspace  {}Box}{G\BPBI  E\BPBI  P.\nobreakspace  {}Box}{{\APACyear {1976}}}}
\bibcite{Box1987}{\citeauthoryear {J\BPBI  F.\nobreakspace  {}Box}{J\BPBI  F.\nobreakspace  {}Box}{{\APACyear {1987}}}}
\APACbibcite{Box1987}{\citeauthoryear {J\BPBI  F.\nobreakspace  {}Box}{J\BPBI  F.\nobreakspace  {}Box}{{\APACyear {1987}}}}
\bibcite{Braun2007}{\citeauthoryear {Braun\ \BBA{} Murdoch}{Braun\ \BBA{} Murdoch}{{\APACyear {2007}}}}
\APACbibcite{Braun2007}{\citeauthoryear {Braun\ \BBA{} Murdoch}{Braun\ \BBA{} Murdoch}{{\APACyear {2007}}}}
\bibcite{BrownForsythe1974}{\citeauthoryear {Brown\ \BBA{} Forsythe}{Brown\ \BBA{} Forsythe}{{\APACyear {1974}}}}
\APACbibcite{BrownForsythe1974}{\citeauthoryear {Brown\ \BBA{} Forsythe}{Brown\ \BBA{} Forsythe}{{\APACyear {1974}}}}
\bibcite{Campbell1963}{\citeauthoryear {Campbell\ \BBA{} Stanley}{Campbell\ \BBA{} Stanley}{{\APACyear {1963}}}}
\APACbibcite{Campbell1963}{\citeauthoryear {Campbell\ \BBA{} Stanley}{Campbell\ \BBA{} Stanley}{{\APACyear {1963}}}}
\bibcite{Cochran1954}{\citeauthoryear {Cochran}{Cochran}{{\APACyear {1954}}}}
\APACbibcite{Cochran1954}{\citeauthoryear {Cochran}{Cochran}{{\APACyear {1954}}}}
\bibcite{Cohen1988}{\citeauthoryear {Cohen}{Cohen}{{\APACyear {1988}}}}
\APACbibcite{Cohen1988}{\citeauthoryear {Cohen}{Cohen}{{\APACyear {1988}}}}
\bibcite{Cook1983}{\citeauthoryear {Cook\ \BBA{} Weisberg}{Cook\ \BBA{} Weisberg}{{\APACyear {1983}}}}
\APACbibcite{Cook1983}{\citeauthoryear {Cook\ \BBA{} Weisberg}{Cook\ \BBA{} Weisberg}{{\APACyear {1983}}}}
\bibcite{Cramer1946}{\citeauthoryear {Cram\'er}{Cram\'er}{{\APACyear {1946}}}}
\APACbibcite{Cramer1946}{\citeauthoryear {Cram\'er}{Cram\'er}{{\APACyear {1946}}}}
\bibcite{Dunn1961}{\citeauthoryear {Dunn}{Dunn}{{\APACyear {1961}}}}
\APACbibcite{Dunn1961}{\citeauthoryear {Dunn}{Dunn}{{\APACyear {1961}}}}
\bibcite{Ellis2010}{\citeauthoryear {Ellis}{Ellis}{{\APACyear {2010}}}}
\APACbibcite{Ellis2010}{\citeauthoryear {Ellis}{Ellis}{{\APACyear {2010}}}}
\bibcite{Ellman2002}{\citeauthoryear {Ellman}{Ellman}{{\APACyear {2002}}}}
\APACbibcite{Ellman2002}{\citeauthoryear {Ellman}{Ellman}{{\APACyear {2002}}}}
\bibcite{Evans1983}{\citeauthoryear {J\BPBI  S\BPBI  B\BPBI  T.\nobreakspace  {}Evans, Barston,{}\ \BBA{} Pollard}{J\BPBI  S\BPBI  B\BPBI  T.\nobreakspace  {}Evans\ \BOthers {.}}{{\APACyear {1983}}}}
\APACbibcite{Evans1983}{\citeauthoryear {J\BPBI  S\BPBI  B\BPBI  T.\nobreakspace  {}Evans, Barston,{}\ \BBA{} Pollard}{J\BPBI  S\BPBI  B\BPBI  T.\nobreakspace  {}Evans\ \BOthers {.}}{{\APACyear {1983}}}}
\bibcite{Evans2000}{\citeauthoryear {M.\nobreakspace  {}Evans, Hastings,{}\ \BBA{} Peacock}{M.\nobreakspace  {}Evans\ \BOthers {.}}{{\APACyear {2011}}}}
\APACbibcite{Evans2000}{\citeauthoryear {M.\nobreakspace  {}Evans, Hastings,{}\ \BBA{} Peacock}{M.\nobreakspace  {}Evans\ \BOthers {.}}{{\APACyear {2011}}}}
\bibcite{Fisher1922}{\citeauthoryear {Fisher}{Fisher}{{\APACyear {1922}}{\APACexlab {{\BCnt {1}}}}}}
\APACbibcite{Fisher1922}{\citeauthoryear {Fisher}{Fisher}{{\APACyear {1922}}{\APACexlab {{\BCnt {1}}}}}}
\@writefile{toc}{\contentsline {section}{References}{597}{section.18.2}}
\bibcite{Fisher1922b}{\citeauthoryear {Fisher}{Fisher}{{\APACyear {1922}}{\APACexlab {{\BCnt {2}}}}}}
\APACbibcite{Fisher1922b}{\citeauthoryear {Fisher}{Fisher}{{\APACyear {1922}}{\APACexlab {{\BCnt {2}}}}}}
\bibcite{Fisher1925}{\citeauthoryear {Fisher}{Fisher}{{\APACyear {1925}}}}
\APACbibcite{Fisher1925}{\citeauthoryear {Fisher}{Fisher}{{\APACyear {1925}}}}
\bibcite{Fox2011}{\citeauthoryear {Fox\ \BBA{} Weisberg}{Fox\ \BBA{} Weisberg}{{\APACyear {2011}}}}
\APACbibcite{Fox2011}{\citeauthoryear {Fox\ \BBA{} Weisberg}{Fox\ \BBA{} Weisberg}{{\APACyear {2011}}}}
\bibcite{Friendly2011}{\citeauthoryear {Friendly}{Friendly}{{\APACyear {2011}}}}
\APACbibcite{Friendly2011}{\citeauthoryear {Friendly}{Friendly}{{\APACyear {2011}}}}
\bibcite{Gelman2006}{\citeauthoryear {Gelman\ \BBA{} Stern}{Gelman\ \BBA{} Stern}{{\APACyear {2006}}}}
\APACbibcite{Gelman2006}{\citeauthoryear {Gelman\ \BBA{} Stern}{Gelman\ \BBA{} Stern}{{\APACyear {2006}}}}
\bibcite{Gunel1974}{\citeauthoryear {Gunel\ \BBA{} Dickey}{Gunel\ \BBA{} Dickey}{{\APACyear {1974}}}}
\APACbibcite{Gunel1974}{\citeauthoryear {Gunel\ \BBA{} Dickey}{Gunel\ \BBA{} Dickey}{{\APACyear {1974}}}}
\bibcite{Hays1994}{\citeauthoryear {Hays}{Hays}{{\APACyear {1994}}}}
\APACbibcite{Hays1994}{\citeauthoryear {Hays}{Hays}{{\APACyear {1994}}}}
\bibcite{Hedges1981}{\citeauthoryear {Hedges}{Hedges}{{\APACyear {1981}}}}
\APACbibcite{Hedges1981}{\citeauthoryear {Hedges}{Hedges}{{\APACyear {1981}}}}
\bibcite{Hedges1985}{\citeauthoryear {Hedges\ \BBA{} Olkin}{Hedges\ \BBA{} Olkin}{{\APACyear {1985}}}}
\APACbibcite{Hedges1985}{\citeauthoryear {Hedges\ \BBA{} Olkin}{Hedges\ \BBA{} Olkin}{{\APACyear {1985}}}}
\bibcite{Hogg2005}{\citeauthoryear {Hogg, McKean,{}\ \BBA{} Craig}{Hogg\ \BOthers {.}}{{\APACyear {2005}}}}
\APACbibcite{Hogg2005}{\citeauthoryear {Hogg, McKean,{}\ \BBA{} Craig}{Hogg\ \BOthers {.}}{{\APACyear {2005}}}}
\bibcite{Holm1979}{\citeauthoryear {Holm}{Holm}{{\APACyear {1979}}}}
\APACbibcite{Holm1979}{\citeauthoryear {Holm}{Holm}{{\APACyear {1979}}}}
\bibcite{Hothersall2004}{\citeauthoryear {Hothersall}{Hothersall}{{\APACyear {2004}}}}
\APACbibcite{Hothersall2004}{\citeauthoryear {Hothersall}{Hothersall}{{\APACyear {2004}}}}
\bibcite{Hsu1996}{\citeauthoryear {Hsu}{Hsu}{{\APACyear {1996}}}}
\APACbibcite{Hsu1996}{\citeauthoryear {Hsu}{Hsu}{{\APACyear {1996}}}}
\bibcite{Ioannidis2005}{\citeauthoryear {Ioannidis}{Ioannidis}{{\APACyear {2005}}}}
\APACbibcite{Ioannidis2005}{\citeauthoryear {Ioannidis}{Ioannidis}{{\APACyear {2005}}}}
\bibcite{Jeffreys1961}{\citeauthoryear {Jeffreys}{Jeffreys}{{\APACyear {1961}}}}
\APACbibcite{Jeffreys1961}{\citeauthoryear {Jeffreys}{Jeffreys}{{\APACyear {1961}}}}
\bibcite{Johnson2013}{\citeauthoryear {Johnson}{Johnson}{{\APACyear {2013}}}}
\APACbibcite{Johnson2013}{\citeauthoryear {Johnson}{Johnson}{{\APACyear {2013}}}}
\bibcite{Kahneman1973}{\citeauthoryear {Kahneman\ \BBA{} Tversky}{Kahneman\ \BBA{} Tversky}{{\APACyear {1973}}}}
\APACbibcite{Kahneman1973}{\citeauthoryear {Kahneman\ \BBA{} Tversky}{Kahneman\ \BBA{} Tversky}{{\APACyear {1973}}}}
\bibcite{Kass1995}{\citeauthoryear {Kass\ \BBA{} Raftery}{Kass\ \BBA{} Raftery}{{\APACyear {1995}}}}
\APACbibcite{Kass1995}{\citeauthoryear {Kass\ \BBA{} Raftery}{Kass\ \BBA{} Raftery}{{\APACyear {1995}}}}
\bibcite{Keynes1923}{\citeauthoryear {Keynes}{Keynes}{{\APACyear {1923}}}}
\APACbibcite{Keynes1923}{\citeauthoryear {Keynes}{Keynes}{{\APACyear {1923}}}}
\bibcite{Kruschke2011}{\citeauthoryear {Kruschke}{Kruschke}{{\APACyear {2011}}}}
\APACbibcite{Kruschke2011}{\citeauthoryear {Kruschke}{Kruschke}{{\APACyear {2011}}}}
\bibcite{KruskalWallis1952}{\citeauthoryear {Kruskal\ \BBA{} Wallis}{Kruskal\ \BBA{} Wallis}{{\APACyear {1952}}}}
\APACbibcite{KruskalWallis1952}{\citeauthoryear {Kruskal\ \BBA{} Wallis}{Kruskal\ \BBA{} Wallis}{{\APACyear {1952}}}}
\bibcite{Kuhberger2014}{\citeauthoryear {K\"uhberger, Fritz,{}\ \BBA{} Scherndl}{K\"uhberger\ \BOthers {.}}{{\APACyear {2014}}}}
\APACbibcite{Kuhberger2014}{\citeauthoryear {K\"uhberger, Fritz,{}\ \BBA{} Scherndl}{K\"uhberger\ \BOthers {.}}{{\APACyear {2014}}}}
\bibcite{Larntz1978}{\citeauthoryear {Larntz}{Larntz}{{\APACyear {1978}}}}
\APACbibcite{Larntz1978}{\citeauthoryear {Larntz}{Larntz}{{\APACyear {1978}}}}
\bibcite{Lee2014}{\citeauthoryear {Lee\ \BBA{} Wagenmakers}{Lee\ \BBA{} Wagenmakers}{{\APACyear {2014}}}}
\APACbibcite{Lee2014}{\citeauthoryear {Lee\ \BBA{} Wagenmakers}{Lee\ \BBA{} Wagenmakers}{{\APACyear {2014}}}}
\bibcite{Lehmann2011}{\citeauthoryear {Lehmann}{Lehmann}{{\APACyear {2011}}}}
\APACbibcite{Lehmann2011}{\citeauthoryear {Lehmann}{Lehmann}{{\APACyear {2011}}}}
\bibcite{Levene1960}{\citeauthoryear {Levene}{Levene}{{\APACyear {1960}}}}
\APACbibcite{Levene1960}{\citeauthoryear {Levene}{Levene}{{\APACyear {1960}}}}
\bibcite{Long2000}{\citeauthoryear {Long\ \BBA{} Ervin}{Long\ \BBA{} Ervin}{{\APACyear {2000}}}}
\APACbibcite{Long2000}{\citeauthoryear {Long\ \BBA{} Ervin}{Long\ \BBA{} Ervin}{{\APACyear {2000}}}}
\bibcite{Matloff2011}{\citeauthoryear {Matloff\ \BBA{} Matloff}{Matloff\ \BBA{} Matloff}{{\APACyear {2011}}}}
\APACbibcite{Matloff2011}{\citeauthoryear {Matloff\ \BBA{} Matloff}{Matloff\ \BBA{} Matloff}{{\APACyear {2011}}}}
\bibcite{McGrath2006}{\citeauthoryear {McGrath\ \BBA{} Meyer}{McGrath\ \BBA{} Meyer}{{\APACyear {2006}}}}
\APACbibcite{McGrath2006}{\citeauthoryear {McGrath\ \BBA{} Meyer}{McGrath\ \BBA{} Meyer}{{\APACyear {2006}}}}
\bibcite{McNemar1947}{\citeauthoryear {McNemar}{McNemar}{{\APACyear {1947}}}}
\APACbibcite{McNemar1947}{\citeauthoryear {McNemar}{McNemar}{{\APACyear {1947}}}}
\bibcite{Meehl1967}{\citeauthoryear {Meehl}{Meehl}{{\APACyear {1967}}}}
\APACbibcite{Meehl1967}{\citeauthoryear {Meehl}{Meehl}{{\APACyear {1967}}}}
\bibcite{Morey2015}{\citeauthoryear {Morey\ \BBA{} Rouder}{Morey\ \BBA{} Rouder}{{\APACyear {2015}}}}
\APACbibcite{Morey2015}{\citeauthoryear {Morey\ \BBA{} Rouder}{Morey\ \BBA{} Rouder}{{\APACyear {2015}}}}
\bibcite{Pearson1900}{\citeauthoryear {Pearson}{Pearson}{{\APACyear {1900}}}}
\APACbibcite{Pearson1900}{\citeauthoryear {Pearson}{Pearson}{{\APACyear {1900}}}}
\bibcite{Pfungst1911}{\citeauthoryear {Pfungst}{Pfungst}{{\APACyear {1911}}}}
\APACbibcite{Pfungst1911}{\citeauthoryear {Pfungst}{Pfungst}{{\APACyear {1911}}}}
\bibcite{R2013}{\citeauthoryear {{R Core Team}}{{R Core Team}}{{\APACyear {2013}}}}
\APACbibcite{R2013}{\citeauthoryear {{R Core Team}}{{R Core Team}}{{\APACyear {2013}}}}
\bibcite{Rosenthal1966}{\citeauthoryear {Rosenthal}{Rosenthal}{{\APACyear {1966}}}}
\APACbibcite{Rosenthal1966}{\citeauthoryear {Rosenthal}{Rosenthal}{{\APACyear {1966}}}}
\bibcite{Rouder2009}{\citeauthoryear {Rouder, Speckman, Sun, Morey,{}\ \BBA{} Iverson}{Rouder\ \BOthers {.}}{{\APACyear {2009}}}}
\APACbibcite{Rouder2009}{\citeauthoryear {Rouder, Speckman, Sun, Morey,{}\ \BBA{} Iverson}{Rouder\ \BOthers {.}}{{\APACyear {2009}}}}
\bibcite{Sahai2000}{\citeauthoryear {Sahai\ \BBA{} Ageel}{Sahai\ \BBA{} Ageel}{{\APACyear {2000}}}}
\APACbibcite{Sahai2000}{\citeauthoryear {Sahai\ \BBA{} Ageel}{Sahai\ \BBA{} Ageel}{{\APACyear {2000}}}}
\bibcite{Shaffer1995}{\citeauthoryear {Shaffer}{Shaffer}{{\APACyear {1995}}}}
\APACbibcite{Shaffer1995}{\citeauthoryear {Shaffer}{Shaffer}{{\APACyear {1995}}}}
\bibcite{Shapiro1965}{\citeauthoryear {Shapiro\ \BBA{} Wilk}{Shapiro\ \BBA{} Wilk}{{\APACyear {1965}}}}
\APACbibcite{Shapiro1965}{\citeauthoryear {Shapiro\ \BBA{} Wilk}{Shapiro\ \BBA{} Wilk}{{\APACyear {1965}}}}
\bibcite{Sokal1994}{\citeauthoryear {Sokal\ \BBA{} Rohlf}{Sokal\ \BBA{} Rohlf}{{\APACyear {1994}}}}
\APACbibcite{Sokal1994}{\citeauthoryear {Sokal\ \BBA{} Rohlf}{Sokal\ \BBA{} Rohlf}{{\APACyear {1994}}}}
\bibcite{Spector2008}{\citeauthoryear {Spector}{Spector}{{\APACyear {2008}}}}
\APACbibcite{Spector2008}{\citeauthoryear {Spector}{Spector}{{\APACyear {2008}}}}
\bibcite{Stevens1946}{\citeauthoryear {Stevens}{Stevens}{{\APACyear {1946}}}}
\APACbibcite{Stevens1946}{\citeauthoryear {Stevens}{Stevens}{{\APACyear {1946}}}}
\bibcite{Stigler1986}{\citeauthoryear {Stigler}{Stigler}{{\APACyear {1986}}}}
\APACbibcite{Stigler1986}{\citeauthoryear {Stigler}{Stigler}{{\APACyear {1986}}}}
\bibcite{Student1908}{\citeauthoryear {Student}{Student}{{\APACyear {1908}}}}
\APACbibcite{Student1908}{\citeauthoryear {Student}{Student}{{\APACyear {1908}}}}
\bibcite{Teetor2011}{\citeauthoryear {Teetor}{Teetor}{{\APACyear {2011}}}}
\APACbibcite{Teetor2011}{\citeauthoryear {Teetor}{Teetor}{{\APACyear {2011}}}}
\bibcite{Welch1947}{\citeauthoryear {Welch}{Welch}{{\APACyear {1947}}}}
\APACbibcite{Welch1947}{\citeauthoryear {Welch}{Welch}{{\APACyear {1947}}}}
\bibcite{Welch1951}{\citeauthoryear {Welch}{Welch}{{\APACyear {1951}}}}
\APACbibcite{Welch1951}{\citeauthoryear {Welch}{Welch}{{\APACyear {1951}}}}
\bibcite{White1980}{\citeauthoryear {White}{White}{{\APACyear {1980}}}}
\APACbibcite{White1980}{\citeauthoryear {White}{White}{{\APACyear {1980}}}}
\bibcite{Wickham2007}{\citeauthoryear {Wickham}{Wickham}{{\APACyear {2007}}}}
\APACbibcite{Wickham2007}{\citeauthoryear {Wickham}{Wickham}{{\APACyear {2007}}}}
\bibcite{Wilkinson2006}{\citeauthoryear {Wilkinson, Wills, Rope, Norton,{}\ \BBA{} Dubbs}{Wilkinson\ \BOthers {.}}{{\APACyear {2006}}}}
\APACbibcite{Wilkinson2006}{\citeauthoryear {Wilkinson, Wills, Rope, Norton,{}\ \BBA{} Dubbs}{Wilkinson\ \BOthers {.}}{{\APACyear {2006}}}}
\bibcite{Yates1934}{\citeauthoryear {Yates}{Yates}{{\APACyear {1934}}}}
\APACbibcite{Yates1934}{\citeauthoryear {Yates}{Yates}{{\APACyear {1934}}}}
\bibstyle{apacite}
\ttl@finishall
