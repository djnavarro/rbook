<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 17 Bayesian statistics | Learning statistics with R: A tutorial for psychology students and other beginners. (Version 0.6.1)</title>
  <meta name="description" content="Learning Statistics with R covers the contents of an introductory statistics class, as typically taught to undergraduate psychology students, focusing on the use of the R statistical software.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 17 Bayesian statistics | Learning statistics with R: A tutorial for psychology students and other beginners. (Version 0.6.1)" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Learning Statistics with R covers the contents of an introductory statistics class, as typically taught to undergraduate psychology students, focusing on the use of the R statistical software." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 17 Bayesian statistics | Learning statistics with R: A tutorial for psychology students and other beginners. (Version 0.6.1)" />
  
  <meta name="twitter:description" content="Learning Statistics with R covers the contents of an introductory statistics class, as typically taught to undergraduate psychology students, focusing on the use of the R statistical software." />
  

<meta name="author" content="Danielle Navarro (bookdown translation: Emily Kothe)">


<meta name="date" content="2019-01-11">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="part-vi-endings-alternatives-and-prospects.html">
<link rel="next" href="epilogue.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />








<!-- ###### start inserted header ##### -->

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-115940772-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-115940772-1');
</script>

<!-- add the twitter card and open graph tags -->
<meta name="twitter:card" content="summary">
<meta name="twitter:creator" content="@djnavarro">
<meta property="og:image" content="http://learningstatisticswithr.com/images/jasmine-faint.jpg">
<meta name="twitter:image" content="http://learningstatisticswithr.com/images/jasmine-faint.jpg">

<!-- ###### end inserted header ##### -->


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Learning Statistics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Overview</a></li>
<li class="chapter" data-level="" data-path="licensing.html"><a href="licensing.html"><i class="fa fa-check"></i>Licensing</a></li>
<li class="chapter" data-level="" data-path="dedication.html"><a href="dedication.html"><i class="fa fa-check"></i>Dedication</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="preface.html"><a href="preface.html#preface-to-version-0.6.1"><i class="fa fa-check"></i><b>0.1</b> Preface to Version 0.6.1</a></li>
<li class="chapter" data-level="0.2" data-path="preface.html"><a href="preface.html#preface-to-version-0.6"><i class="fa fa-check"></i><b>0.2</b> Preface to Version 0.6</a></li>
<li class="chapter" data-level="0.3" data-path="preface.html"><a href="preface.html#preface-to-version-0.5"><i class="fa fa-check"></i><b>0.3</b> Preface to Version 0.5</a></li>
<li class="chapter" data-level="0.4" data-path="preface.html"><a href="preface.html#preface-to-version-0.4"><i class="fa fa-check"></i><b>0.4</b> Preface to Version 0.4</a></li>
<li class="chapter" data-level="0.5" data-path="preface.html"><a href="preface.html#preface-to-version-0.3"><i class="fa fa-check"></i><b>0.5</b> Preface to Version 0.3</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-i-background.html"><a href="part-i-background.html"><i class="fa fa-check"></i>Part I. Background</a></li>
<li class="chapter" data-level="1" data-path="why-do-we-learn-statistics.html"><a href="why-do-we-learn-statistics.html"><i class="fa fa-check"></i><b>1</b> Why do we learn statistics?</a><ul>
<li class="chapter" data-level="1.1" data-path="why-do-we-learn-statistics.html"><a href="why-do-we-learn-statistics.html#whywhywhy"><i class="fa fa-check"></i><b>1.1</b> On the psychology of statistics</a><ul>
<li class="chapter" data-level="1.1.1" data-path="why-do-we-learn-statistics.html"><a href="why-do-we-learn-statistics.html#the-curse-of-belief-bias"><i class="fa fa-check"></i><b>1.1.1</b> The curse of belief bias</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="why-do-we-learn-statistics.html"><a href="why-do-we-learn-statistics.html#the-cautionary-tale-of-simpsons-paradox"><i class="fa fa-check"></i><b>1.2</b> The cautionary tale of Simpson’s paradox</a></li>
<li class="chapter" data-level="1.3" data-path="why-do-we-learn-statistics.html"><a href="why-do-we-learn-statistics.html#statistics-in-psychology"><i class="fa fa-check"></i><b>1.3</b> Statistics in psychology</a></li>
<li class="chapter" data-level="1.4" data-path="why-do-we-learn-statistics.html"><a href="why-do-we-learn-statistics.html#statistics-in-everyday-life"><i class="fa fa-check"></i><b>1.4</b> Statistics in everyday life</a></li>
<li class="chapter" data-level="1.5" data-path="why-do-we-learn-statistics.html"><a href="why-do-we-learn-statistics.html#theres-more-to-research-methods-than-statistics"><i class="fa fa-check"></i><b>1.5</b> There’s more to research methods than statistics</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="studydesign.html"><a href="studydesign.html"><i class="fa fa-check"></i><b>2</b> A brief introduction to research design</a><ul>
<li class="chapter" data-level="2.1" data-path="studydesign.html"><a href="studydesign.html#measurement"><i class="fa fa-check"></i><b>2.1</b> Introduction to psychological measurement</a><ul>
<li class="chapter" data-level="2.1.1" data-path="studydesign.html"><a href="studydesign.html#some-thoughts-about-psychological-measurement"><i class="fa fa-check"></i><b>2.1.1</b> Some thoughts about psychological measurement</a></li>
<li class="chapter" data-level="2.1.2" data-path="studydesign.html"><a href="studydesign.html#operationalisation-defining-your-measurement"><i class="fa fa-check"></i><b>2.1.2</b> Operationalisation: defining your measurement</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="studydesign.html"><a href="studydesign.html#scales"><i class="fa fa-check"></i><b>2.2</b> Scales of measurement</a><ul>
<li class="chapter" data-level="2.2.1" data-path="studydesign.html"><a href="studydesign.html#nominal-scale"><i class="fa fa-check"></i><b>2.2.1</b> Nominal scale</a></li>
<li class="chapter" data-level="2.2.2" data-path="studydesign.html"><a href="studydesign.html#ordinal-scale"><i class="fa fa-check"></i><b>2.2.2</b> Ordinal scale</a></li>
<li class="chapter" data-level="2.2.3" data-path="studydesign.html"><a href="studydesign.html#interval-scale"><i class="fa fa-check"></i><b>2.2.3</b> Interval scale</a></li>
<li class="chapter" data-level="2.2.4" data-path="studydesign.html"><a href="studydesign.html#ratio-scale"><i class="fa fa-check"></i><b>2.2.4</b> Ratio scale</a></li>
<li class="chapter" data-level="2.2.5" data-path="studydesign.html"><a href="studydesign.html#continuousdiscrete"><i class="fa fa-check"></i><b>2.2.5</b> Continuous versus discrete variables</a></li>
<li class="chapter" data-level="2.2.6" data-path="studydesign.html"><a href="studydesign.html#some-complexities"><i class="fa fa-check"></i><b>2.2.6</b> Some complexities</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="studydesign.html"><a href="studydesign.html#reliability"><i class="fa fa-check"></i><b>2.3</b> Assessing the reliability of a measurement</a></li>
<li class="chapter" data-level="2.4" data-path="studydesign.html"><a href="studydesign.html#ivdv"><i class="fa fa-check"></i><b>2.4</b> The “role” of variables: predictors and outcomes</a></li>
<li class="chapter" data-level="2.5" data-path="studydesign.html"><a href="studydesign.html#researchdesigns"><i class="fa fa-check"></i><b>2.5</b> Experimental and non-experimental research</a><ul>
<li class="chapter" data-level="2.5.1" data-path="studydesign.html"><a href="studydesign.html#experimental-research"><i class="fa fa-check"></i><b>2.5.1</b> Experimental research</a></li>
<li class="chapter" data-level="2.5.2" data-path="studydesign.html"><a href="studydesign.html#non-experimental-research"><i class="fa fa-check"></i><b>2.5.2</b> Non-experimental research</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="studydesign.html"><a href="studydesign.html#validity"><i class="fa fa-check"></i><b>2.6</b> Assessing the validity of a study</a><ul>
<li class="chapter" data-level="2.6.1" data-path="studydesign.html"><a href="studydesign.html#internal-validity"><i class="fa fa-check"></i><b>2.6.1</b> Internal validity</a></li>
<li class="chapter" data-level="2.6.2" data-path="studydesign.html"><a href="studydesign.html#external-validity"><i class="fa fa-check"></i><b>2.6.2</b> External validity</a></li>
<li class="chapter" data-level="2.6.3" data-path="studydesign.html"><a href="studydesign.html#construct-validity"><i class="fa fa-check"></i><b>2.6.3</b> Construct validity</a></li>
<li class="chapter" data-level="2.6.4" data-path="studydesign.html"><a href="studydesign.html#face-validity"><i class="fa fa-check"></i><b>2.6.4</b> Face validity</a></li>
<li class="chapter" data-level="2.6.5" data-path="studydesign.html"><a href="studydesign.html#ecological-validity"><i class="fa fa-check"></i><b>2.6.5</b> Ecological validity</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="studydesign.html"><a href="studydesign.html#confounds-artifacts-and-other-threats-to-validity"><i class="fa fa-check"></i><b>2.7</b> Confounds, artifacts and other threats to validity</a><ul>
<li class="chapter" data-level="2.7.1" data-path="studydesign.html"><a href="studydesign.html#history-effects"><i class="fa fa-check"></i><b>2.7.1</b> History effects</a></li>
<li class="chapter" data-level="2.7.2" data-path="studydesign.html"><a href="studydesign.html#maturation-effects"><i class="fa fa-check"></i><b>2.7.2</b> Maturation effects</a></li>
<li class="chapter" data-level="2.7.3" data-path="studydesign.html"><a href="studydesign.html#repeated-testing-effects"><i class="fa fa-check"></i><b>2.7.3</b> Repeated testing effects</a></li>
<li class="chapter" data-level="2.7.4" data-path="studydesign.html"><a href="studydesign.html#selection-bias"><i class="fa fa-check"></i><b>2.7.4</b> Selection bias</a></li>
<li class="chapter" data-level="2.7.5" data-path="studydesign.html"><a href="studydesign.html#differentialattrition"><i class="fa fa-check"></i><b>2.7.5</b> Differential attrition</a></li>
<li class="chapter" data-level="2.7.6" data-path="studydesign.html"><a href="studydesign.html#non-response-bias"><i class="fa fa-check"></i><b>2.7.6</b> Non-response bias</a></li>
<li class="chapter" data-level="2.7.7" data-path="studydesign.html"><a href="studydesign.html#regression-to-the-mean"><i class="fa fa-check"></i><b>2.7.7</b> Regression to the mean</a></li>
<li class="chapter" data-level="2.7.8" data-path="studydesign.html"><a href="studydesign.html#experimenter-bias"><i class="fa fa-check"></i><b>2.7.8</b> Experimenter bias</a></li>
<li class="chapter" data-level="2.7.9" data-path="studydesign.html"><a href="studydesign.html#demand-effects-and-reactivity"><i class="fa fa-check"></i><b>2.7.9</b> Demand effects and reactivity</a></li>
<li class="chapter" data-level="2.7.10" data-path="studydesign.html"><a href="studydesign.html#placebo-effects"><i class="fa fa-check"></i><b>2.7.10</b> Placebo effects</a></li>
<li class="chapter" data-level="2.7.11" data-path="studydesign.html"><a href="studydesign.html#situation-measurement-and-subpopulation-effects"><i class="fa fa-check"></i><b>2.7.11</b> Situation, measurement and subpopulation effects</a></li>
<li class="chapter" data-level="2.7.12" data-path="studydesign.html"><a href="studydesign.html#fraud-deception-and-self-deception"><i class="fa fa-check"></i><b>2.7.12</b> Fraud, deception and self-deception</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="studydesign.html"><a href="studydesign.html#summary"><i class="fa fa-check"></i><b>2.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-ii-an-introduction-to-r.html"><a href="part-ii-an-introduction-to-r.html"><i class="fa fa-check"></i>Part II. An introduction to R</a></li>
<li class="chapter" data-level="3" data-path="introR.html"><a href="introR.html"><i class="fa fa-check"></i><b>3</b> Getting started with R</a><ul>
<li class="chapter" data-level="3.1" data-path="introR.html"><a href="introR.html#gettingR"><i class="fa fa-check"></i><b>3.1</b> Installing R</a><ul>
<li class="chapter" data-level="3.1.1" data-path="introR.html"><a href="introR.html#installing-r-on-a-windows-computer"><i class="fa fa-check"></i><b>3.1.1</b> Installing R on a Windows computer</a></li>
<li class="chapter" data-level="3.1.2" data-path="introR.html"><a href="introR.html#installing-r-on-a-mac"><i class="fa fa-check"></i><b>3.1.2</b> Installing R on a Mac</a></li>
<li class="chapter" data-level="3.1.3" data-path="introR.html"><a href="introR.html#installing-r-on-a-linux-computer"><i class="fa fa-check"></i><b>3.1.3</b> Installing R on a Linux computer</a></li>
<li class="chapter" data-level="3.1.4" data-path="introR.html"><a href="introR.html#installingrstudio"><i class="fa fa-check"></i><b>3.1.4</b> Downloading and installing RStudio</a></li>
<li class="chapter" data-level="3.1.5" data-path="introR.html"><a href="introR.html#startingR"><i class="fa fa-check"></i><b>3.1.5</b> Starting up R</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="introR.html"><a href="introR.html#firstcommand"><i class="fa fa-check"></i><b>3.2</b> Typing commands at the R console</a><ul>
<li class="chapter" data-level="3.2.1" data-path="introR.html"><a href="introR.html#an-important-digression-about-formatting"><i class="fa fa-check"></i><b>3.2.1</b> An important digression about formatting</a></li>
<li class="chapter" data-level="3.2.2" data-path="introR.html"><a href="introR.html#be-very-careful-to-avoid-typos"><i class="fa fa-check"></i><b>3.2.2</b> Be very careful to avoid typos</a></li>
<li class="chapter" data-level="3.2.3" data-path="introR.html"><a href="introR.html#r-is-a-bit-flexible-with-spacing"><i class="fa fa-check"></i><b>3.2.3</b> R is (a bit) flexible with spacing</a></li>
<li class="chapter" data-level="3.2.4" data-path="introR.html"><a href="introR.html#r-can-sometimes-tell-that-youre-not-finished-yet-but-not-often"><i class="fa fa-check"></i><b>3.2.4</b> R can sometimes tell that you’re not finished yet (but not often)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="introR.html"><a href="introR.html#arithmetic"><i class="fa fa-check"></i><b>3.3</b> Doing simple calculations with R</a><ul>
<li class="chapter" data-level="3.3.1" data-path="introR.html"><a href="introR.html#adding-subtracting-multiplying-and-dividing"><i class="fa fa-check"></i><b>3.3.1</b> Adding, subtracting, multiplying and dividing</a></li>
<li class="chapter" data-level="3.3.2" data-path="introR.html"><a href="introR.html#taking-powers"><i class="fa fa-check"></i><b>3.3.2</b> Taking powers</a></li>
<li class="chapter" data-level="3.3.3" data-path="introR.html"><a href="introR.html#bedmas"><i class="fa fa-check"></i><b>3.3.3</b> Doing calculations in the right order</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="introR.html"><a href="introR.html#assign"><i class="fa fa-check"></i><b>3.4</b> Storing a number as a variable</a><ul>
<li class="chapter" data-level="3.4.1" data-path="introR.html"><a href="introR.html#variable-assignment-using---and--"><i class="fa fa-check"></i><b>3.4.1</b> Variable assignment using <code>&lt;-</code> and <code>-&gt;</code></a></li>
<li class="chapter" data-level="3.4.2" data-path="introR.html"><a href="introR.html#doing-calculations-using-variables"><i class="fa fa-check"></i><b>3.4.2</b> Doing calculations using variables</a></li>
<li class="chapter" data-level="3.4.3" data-path="introR.html"><a href="introR.html#rules-and-conventions-for-naming-variables"><i class="fa fa-check"></i><b>3.4.3</b> Rules and conventions for naming variables</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="introR.html"><a href="introR.html#usingfunctions"><i class="fa fa-check"></i><b>3.5</b> Using functions to do calculations</a><ul>
<li class="chapter" data-level="3.5.1" data-path="introR.html"><a href="introR.html#functionarguments"><i class="fa fa-check"></i><b>3.5.1</b> Function arguments, their names and their defaults</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="introR.html"><a href="introR.html#RStudio1"><i class="fa fa-check"></i><b>3.6</b> Letting RStudio help you with your commands</a><ul>
<li class="chapter" data-level="3.6.1" data-path="introR.html"><a href="introR.html#autocomplete-using-tab"><i class="fa fa-check"></i><b>3.6.1</b> Autocomplete using “tab”</a></li>
<li class="chapter" data-level="3.6.2" data-path="introR.html"><a href="introR.html#browsing-your-command-history"><i class="fa fa-check"></i><b>3.6.2</b> Browsing your command history</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="introR.html"><a href="introR.html#vectors"><i class="fa fa-check"></i><b>3.7</b> Storing many numbers as a vector</a><ul>
<li class="chapter" data-level="3.7.1" data-path="introR.html"><a href="introR.html#creating-a-vector"><i class="fa fa-check"></i><b>3.7.1</b> Creating a vector</a></li>
<li class="chapter" data-level="3.7.2" data-path="introR.html"><a href="introR.html#a-handy-digression"><i class="fa fa-check"></i><b>3.7.2</b> A handy digression</a></li>
<li class="chapter" data-level="3.7.3" data-path="introR.html"><a href="introR.html#vectorsubset"><i class="fa fa-check"></i><b>3.7.3</b> Getting information out of vectors</a></li>
<li class="chapter" data-level="3.7.4" data-path="introR.html"><a href="introR.html#altering-the-elements-of-a-vector"><i class="fa fa-check"></i><b>3.7.4</b> Altering the elements of a vector</a></li>
<li class="chapter" data-level="3.7.5" data-path="introR.html"><a href="introR.html#veclength"><i class="fa fa-check"></i><b>3.7.5</b> Useful things to know about vectors</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="introR.html"><a href="introR.html#text"><i class="fa fa-check"></i><b>3.8</b> Storing text data</a><ul>
<li class="chapter" data-level="3.8.1" data-path="introR.html"><a href="introR.html#simpletext"><i class="fa fa-check"></i><b>3.8.1</b> Working with text</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="introR.html"><a href="introR.html#logicals"><i class="fa fa-check"></i><b>3.9</b> Storing “true or false” data</a><ul>
<li class="chapter" data-level="3.9.1" data-path="introR.html"><a href="introR.html#assessing-mathematical-truths"><i class="fa fa-check"></i><b>3.9.1</b> Assessing mathematical truths</a></li>
<li class="chapter" data-level="3.9.2" data-path="introR.html"><a href="introR.html#logical-operations"><i class="fa fa-check"></i><b>3.9.2</b> Logical operations</a></li>
<li class="chapter" data-level="3.9.3" data-path="introR.html"><a href="introR.html#storing-and-using-logical-data"><i class="fa fa-check"></i><b>3.9.3</b> Storing and using logical data</a></li>
<li class="chapter" data-level="3.9.4" data-path="introR.html"><a href="introR.html#vectors-of-logicals"><i class="fa fa-check"></i><b>3.9.4</b> Vectors of logicals</a></li>
<li class="chapter" data-level="3.9.5" data-path="introR.html"><a href="introR.html#logictext"><i class="fa fa-check"></i><b>3.9.5</b> Applying logical operation to text</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="introR.html"><a href="introR.html#indexing"><i class="fa fa-check"></i><b>3.10</b> Indexing vectors</a><ul>
<li class="chapter" data-level="3.10.1" data-path="introR.html"><a href="introR.html#extracting-multiple-elements"><i class="fa fa-check"></i><b>3.10.1</b> Extracting multiple elements</a></li>
<li class="chapter" data-level="3.10.2" data-path="introR.html"><a href="introR.html#logical-indexing"><i class="fa fa-check"></i><b>3.10.2</b> Logical indexing</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="introR.html"><a href="introR.html#quitting-r"><i class="fa fa-check"></i><b>3.11</b> Quitting R</a></li>
<li class="chapter" data-level="3.12" data-path="introR.html"><a href="introR.html#summary-1"><i class="fa fa-check"></i><b>3.12</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mechanics.html"><a href="mechanics.html"><i class="fa fa-check"></i><b>4</b> Additional R concepts</a><ul>
<li class="chapter" data-level="4.1" data-path="mechanics.html"><a href="mechanics.html#comments"><i class="fa fa-check"></i><b>4.1</b> Using comments</a></li>
<li class="chapter" data-level="4.2" data-path="mechanics.html"><a href="mechanics.html#packageinstall"><i class="fa fa-check"></i><b>4.2</b> Installing and loading packages</a><ul>
<li class="chapter" data-level="4.2.1" data-path="mechanics.html"><a href="mechanics.html#the-package-panel-in-rstudio"><i class="fa fa-check"></i><b>4.2.1</b> The package panel in RStudio</a></li>
<li class="chapter" data-level="4.2.2" data-path="mechanics.html"><a href="mechanics.html#packageload"><i class="fa fa-check"></i><b>4.2.2</b> Loading a package</a></li>
<li class="chapter" data-level="4.2.3" data-path="mechanics.html"><a href="mechanics.html#packageunload"><i class="fa fa-check"></i><b>4.2.3</b> Unloading a package</a></li>
<li class="chapter" data-level="4.2.4" data-path="mechanics.html"><a href="mechanics.html#a-few-extra-comments"><i class="fa fa-check"></i><b>4.2.4</b> A few extra comments</a></li>
<li class="chapter" data-level="4.2.5" data-path="mechanics.html"><a href="mechanics.html#downloading-new-packages"><i class="fa fa-check"></i><b>4.2.5</b> Downloading new packages</a></li>
<li class="chapter" data-level="4.2.6" data-path="mechanics.html"><a href="mechanics.html#updating-r-and-r-packages"><i class="fa fa-check"></i><b>4.2.6</b> Updating R and R packages</a></li>
<li class="chapter" data-level="4.2.7" data-path="mechanics.html"><a href="mechanics.html#what-packages-does-this-book-use"><i class="fa fa-check"></i><b>4.2.7</b> What packages does this book use?</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="mechanics.html"><a href="mechanics.html#workspace"><i class="fa fa-check"></i><b>4.3</b> Managing the workspace</a><ul>
<li class="chapter" data-level="4.3.1" data-path="mechanics.html"><a href="mechanics.html#listing-the-contents-of-the-workspace"><i class="fa fa-check"></i><b>4.3.1</b> Listing the contents of the workspace</a></li>
<li class="chapter" data-level="4.3.2" data-path="mechanics.html"><a href="mechanics.html#removing-variables-from-the-workspace"><i class="fa fa-check"></i><b>4.3.2</b> Removing variables from the workspace</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="mechanics.html"><a href="mechanics.html#navigation"><i class="fa fa-check"></i><b>4.4</b> Navigating the file system</a><ul>
<li class="chapter" data-level="4.4.1" data-path="mechanics.html"><a href="mechanics.html#filesystem"><i class="fa fa-check"></i><b>4.4.1</b> The file system itself</a></li>
<li class="chapter" data-level="4.4.2" data-path="mechanics.html"><a href="mechanics.html#navigationR"><i class="fa fa-check"></i><b>4.4.2</b> Navigating the file system using the R console</a></li>
<li class="chapter" data-level="4.4.3" data-path="mechanics.html"><a href="mechanics.html#why-do-the-windows-paths-use-the-wrong-slash"><i class="fa fa-check"></i><b>4.4.3</b> Why do the Windows paths use the wrong slash?</a></li>
<li class="chapter" data-level="4.4.4" data-path="mechanics.html"><a href="mechanics.html#nav3"><i class="fa fa-check"></i><b>4.4.4</b> Navigating the file system using the RStudio file panel</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="mechanics.html"><a href="mechanics.html#load"><i class="fa fa-check"></i><b>4.5</b> Loading and saving data</a><ul>
<li class="chapter" data-level="4.5.1" data-path="mechanics.html"><a href="mechanics.html#loading-workspace-files-using-r"><i class="fa fa-check"></i><b>4.5.1</b> Loading workspace files using R</a></li>
<li class="chapter" data-level="4.5.2" data-path="mechanics.html"><a href="mechanics.html#loading-workspace-files-using-rstudio"><i class="fa fa-check"></i><b>4.5.2</b> Loading workspace files using RStudio</a></li>
<li class="chapter" data-level="4.5.3" data-path="mechanics.html"><a href="mechanics.html#loadingcsv"><i class="fa fa-check"></i><b>4.5.3</b> Importing data from CSV files using loadingcsv</a></li>
<li class="chapter" data-level="4.5.4" data-path="mechanics.html"><a href="mechanics.html#importing-data-from-csv-files-using-rstudio"><i class="fa fa-check"></i><b>4.5.4</b> Importing data from CSV files using RStudio</a></li>
<li class="chapter" data-level="4.5.5" data-path="mechanics.html"><a href="mechanics.html#saving-a-workspace-file-using-save"><i class="fa fa-check"></i><b>4.5.5</b> Saving a workspace file using <code>save</code></a></li>
<li class="chapter" data-level="4.5.6" data-path="mechanics.html"><a href="mechanics.html#save1"><i class="fa fa-check"></i><b>4.5.6</b> Saving a workspace file using RStudio</a></li>
<li class="chapter" data-level="4.5.7" data-path="mechanics.html"><a href="mechanics.html#other-things-you-might-want-to-save"><i class="fa fa-check"></i><b>4.5.7</b> Other things you might want to save</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="mechanics.html"><a href="mechanics.html#useful"><i class="fa fa-check"></i><b>4.6</b> Useful things to know about variables</a><ul>
<li class="chapter" data-level="4.6.1" data-path="mechanics.html"><a href="mechanics.html#specials"><i class="fa fa-check"></i><b>4.6.1</b> Special values</a></li>
<li class="chapter" data-level="4.6.2" data-path="mechanics.html"><a href="mechanics.html#names"><i class="fa fa-check"></i><b>4.6.2</b> Assigning names to vector elements</a></li>
<li class="chapter" data-level="4.6.3" data-path="mechanics.html"><a href="mechanics.html#variable-classes"><i class="fa fa-check"></i><b>4.6.3</b> Variable classes</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="mechanics.html"><a href="mechanics.html#factors"><i class="fa fa-check"></i><b>4.7</b> Factors</a><ul>
<li class="chapter" data-level="4.7.1" data-path="mechanics.html"><a href="mechanics.html#introducing-factors"><i class="fa fa-check"></i><b>4.7.1</b> Introducing factors</a></li>
<li class="chapter" data-level="4.7.2" data-path="mechanics.html"><a href="mechanics.html#labelling-the-factor-levels"><i class="fa fa-check"></i><b>4.7.2</b> Labelling the factor levels</a></li>
<li class="chapter" data-level="4.7.3" data-path="mechanics.html"><a href="mechanics.html#moving-on"><i class="fa fa-check"></i><b>4.7.3</b> Moving on…</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="mechanics.html"><a href="mechanics.html#dataframes"><i class="fa fa-check"></i><b>4.8</b> Data frames</a><ul>
<li class="chapter" data-level="4.8.1" data-path="mechanics.html"><a href="mechanics.html#introducing-data-frames"><i class="fa fa-check"></i><b>4.8.1</b> Introducing data frames</a></li>
<li class="chapter" data-level="4.8.2" data-path="mechanics.html"><a href="mechanics.html#pulling-out-the-contents-of-the-data-frame-using"><i class="fa fa-check"></i><b>4.8.2</b> Pulling out the contents of the data frame using <code>$</code></a></li>
<li class="chapter" data-level="4.8.3" data-path="mechanics.html"><a href="mechanics.html#getting-information-about-a-data-frame"><i class="fa fa-check"></i><b>4.8.3</b> Getting information about a data frame</a></li>
<li class="chapter" data-level="4.8.4" data-path="mechanics.html"><a href="mechanics.html#looking-for-more-on-data-frames"><i class="fa fa-check"></i><b>4.8.4</b> Looking for more on data frames?</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="mechanics.html"><a href="mechanics.html#lists"><i class="fa fa-check"></i><b>4.9</b> Lists</a></li>
<li class="chapter" data-level="4.10" data-path="mechanics.html"><a href="mechanics.html#formulas"><i class="fa fa-check"></i><b>4.10</b> Formulas</a></li>
<li class="chapter" data-level="4.11" data-path="mechanics.html"><a href="mechanics.html#generics"><i class="fa fa-check"></i><b>4.11</b> Generic functions</a></li>
<li class="chapter" data-level="4.12" data-path="mechanics.html"><a href="mechanics.html#help"><i class="fa fa-check"></i><b>4.12</b> Getting help</a><ul>
<li class="chapter" data-level="4.12.1" data-path="mechanics.html"><a href="mechanics.html#how-to-read-the-help-documentation"><i class="fa fa-check"></i><b>4.12.1</b> How to read the help documentation</a></li>
<li class="chapter" data-level="4.12.2" data-path="mechanics.html"><a href="mechanics.html#other-resources"><i class="fa fa-check"></i><b>4.12.2</b> Other resources</a></li>
</ul></li>
<li class="chapter" data-level="4.13" data-path="mechanics.html"><a href="mechanics.html#summary-2"><i class="fa fa-check"></i><b>4.13</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-iii-working-with-data.html"><a href="part-iii-working-with-data.html"><i class="fa fa-check"></i>Part III. Working with data</a></li>
<li class="chapter" data-level="5" data-path="descriptives.html"><a href="descriptives.html"><i class="fa fa-check"></i><b>5</b> Descriptive statistics</a><ul>
<li class="chapter" data-level="5.1" data-path="descriptives.html"><a href="descriptives.html#centraltendency"><i class="fa fa-check"></i><b>5.1</b> Measures of central tendency</a><ul>
<li class="chapter" data-level="5.1.1" data-path="descriptives.html"><a href="descriptives.html#mean"><i class="fa fa-check"></i><b>5.1.1</b> The mean</a></li>
<li class="chapter" data-level="5.1.2" data-path="descriptives.html"><a href="descriptives.html#calculating-the-mean-in-r"><i class="fa fa-check"></i><b>5.1.2</b> Calculating the mean in R</a></li>
<li class="chapter" data-level="5.1.3" data-path="descriptives.html"><a href="descriptives.html#median"><i class="fa fa-check"></i><b>5.1.3</b> The median</a></li>
<li class="chapter" data-level="5.1.4" data-path="descriptives.html"><a href="descriptives.html#mean-or-median-whats-the-difference"><i class="fa fa-check"></i><b>5.1.4</b> Mean or median? What’s the difference?</a></li>
<li class="chapter" data-level="5.1.5" data-path="descriptives.html"><a href="descriptives.html#housingpriceexample"><i class="fa fa-check"></i><b>5.1.5</b> A real life example</a></li>
<li class="chapter" data-level="5.1.6" data-path="descriptives.html"><a href="descriptives.html#trimmedmean"><i class="fa fa-check"></i><b>5.1.6</b> Trimmed mean</a></li>
<li class="chapter" data-level="5.1.7" data-path="descriptives.html"><a href="descriptives.html#mode"><i class="fa fa-check"></i><b>5.1.7</b> Mode</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="descriptives.html"><a href="descriptives.html#var"><i class="fa fa-check"></i><b>5.2</b> Measures of variability</a><ul>
<li class="chapter" data-level="5.2.1" data-path="descriptives.html"><a href="descriptives.html#range"><i class="fa fa-check"></i><b>5.2.1</b> Range</a></li>
<li class="chapter" data-level="5.2.2" data-path="descriptives.html"><a href="descriptives.html#interquartile-range"><i class="fa fa-check"></i><b>5.2.2</b> Interquartile range</a></li>
<li class="chapter" data-level="5.2.3" data-path="descriptives.html"><a href="descriptives.html#aad"><i class="fa fa-check"></i><b>5.2.3</b> Mean absolute deviation</a></li>
<li class="chapter" data-level="5.2.4" data-path="descriptives.html"><a href="descriptives.html#variance"><i class="fa fa-check"></i><b>5.2.4</b> Variance</a></li>
<li class="chapter" data-level="5.2.5" data-path="descriptives.html"><a href="descriptives.html#sd"><i class="fa fa-check"></i><b>5.2.5</b> Standard deviation</a></li>
<li class="chapter" data-level="5.2.6" data-path="descriptives.html"><a href="descriptives.html#mad"><i class="fa fa-check"></i><b>5.2.6</b> Median absolute deviation</a></li>
<li class="chapter" data-level="5.2.7" data-path="descriptives.html"><a href="descriptives.html#which-measure-to-use"><i class="fa fa-check"></i><b>5.2.7</b> Which measure to use?</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="descriptives.html"><a href="descriptives.html#skewandkurtosis"><i class="fa fa-check"></i><b>5.3</b> Skew and kurtosis</a></li>
<li class="chapter" data-level="5.4" data-path="studydesign.html"><a href="studydesign.html#summary"><i class="fa fa-check"></i><b>5.4</b> Getting an overall summary of a variable</a><ul>
<li class="chapter" data-level="5.4.1" data-path="descriptives.html"><a href="descriptives.html#summarising-a-variable"><i class="fa fa-check"></i><b>5.4.1</b> “Summarising” a variable</a></li>
<li class="chapter" data-level="5.4.2" data-path="descriptives.html"><a href="descriptives.html#summarising-a-data-frame"><i class="fa fa-check"></i><b>5.4.2</b> “Summarising” a data frame</a></li>
<li class="chapter" data-level="5.4.3" data-path="descriptives.html"><a href="descriptives.html#describing-a-data-frame"><i class="fa fa-check"></i><b>5.4.3</b> “Describing” a data frame</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="descriptives.html"><a href="descriptives.html#groupdescriptives"><i class="fa fa-check"></i><b>5.5</b> Descriptive statistics separately for each group</a></li>
<li class="chapter" data-level="5.6" data-path="descriptives.html"><a href="descriptives.html#zscore"><i class="fa fa-check"></i><b>5.6</b> Standard scores</a></li>
<li class="chapter" data-level="5.7" data-path="descriptives.html"><a href="descriptives.html#correl"><i class="fa fa-check"></i><b>5.7</b> Correlations</a><ul>
<li class="chapter" data-level="5.7.1" data-path="descriptives.html"><a href="descriptives.html#the-data"><i class="fa fa-check"></i><b>5.7.1</b> The data</a></li>
<li class="chapter" data-level="5.7.2" data-path="descriptives.html"><a href="descriptives.html#the-strength-and-direction-of-a-relationship"><i class="fa fa-check"></i><b>5.7.2</b> The strength and direction of a relationship</a></li>
<li class="chapter" data-level="5.7.3" data-path="descriptives.html"><a href="descriptives.html#the-correlation-coefficient"><i class="fa fa-check"></i><b>5.7.3</b> The correlation coefficient</a></li>
<li class="chapter" data-level="5.7.4" data-path="descriptives.html"><a href="descriptives.html#calculating-correlations-in-r"><i class="fa fa-check"></i><b>5.7.4</b> Calculating correlations in R</a></li>
<li class="chapter" data-level="5.7.5" data-path="descriptives.html"><a href="descriptives.html#interpretingcorrelations"><i class="fa fa-check"></i><b>5.7.5</b> Interpreting a correlation</a></li>
<li class="chapter" data-level="5.7.6" data-path="descriptives.html"><a href="descriptives.html#spearmans-rank-correlations"><i class="fa fa-check"></i><b>5.7.6</b> Spearman’s rank correlations</a></li>
<li class="chapter" data-level="5.7.7" data-path="descriptives.html"><a href="descriptives.html#the-correlate-function"><i class="fa fa-check"></i><b>5.7.7</b> The <code>correlate()</code> function</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="descriptives.html"><a href="descriptives.html#missing"><i class="fa fa-check"></i><b>5.8</b> Handling missing values</a><ul>
<li class="chapter" data-level="5.8.1" data-path="descriptives.html"><a href="descriptives.html#the-single-variable-case"><i class="fa fa-check"></i><b>5.8.1</b> The single variable case</a></li>
<li class="chapter" data-level="5.8.2" data-path="descriptives.html"><a href="descriptives.html#missing-values-in-pairwise-calculations"><i class="fa fa-check"></i><b>5.8.2</b> Missing values in pairwise calculations</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="descriptives.html"><a href="descriptives.html#summary-3"><i class="fa fa-check"></i><b>5.9</b> Summary</a></li>
<li class="chapter" data-level="5.10" data-path="descriptives.html"><a href="descriptives.html#epilogue-good-descriptive-statistics-are-descriptive"><i class="fa fa-check"></i><b>5.10</b> Epilogue: Good descriptive statistics are descriptive!</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="graphics.html"><a href="graphics.html"><i class="fa fa-check"></i><b>6</b> Drawing graphs</a><ul>
<li class="chapter" data-level="6.1" data-path="graphics.html"><a href="graphics.html#rgraphics"><i class="fa fa-check"></i><b>6.1</b> An overview of R graphics</a></li>
<li class="chapter" data-level="6.2" data-path="graphics.html"><a href="graphics.html#introplotting"><i class="fa fa-check"></i><b>6.2</b> An introduction to plotting</a><ul>
<li class="chapter" data-level="6.2.1" data-path="graphics.html"><a href="graphics.html#a-tedious-digression"><i class="fa fa-check"></i><b>6.2.1</b> A tedious digression</a></li>
<li class="chapter" data-level="6.2.2" data-path="graphics.html"><a href="graphics.html#figtitles"><i class="fa fa-check"></i><b>6.2.2</b> Customising the title and the axis labels</a></li>
<li class="chapter" data-level="6.2.3" data-path="graphics.html"><a href="graphics.html#changing-the-plot-type"><i class="fa fa-check"></i><b>6.2.3</b> Changing the plot type</a></li>
<li class="chapter" data-level="6.2.4" data-path="graphics.html"><a href="graphics.html#changing-other-features-of-the-plot"><i class="fa fa-check"></i><b>6.2.4</b> Changing other features of the plot</a></li>
<li class="chapter" data-level="6.2.5" data-path="graphics.html"><a href="graphics.html#changing-the-appearance-of-the-axes"><i class="fa fa-check"></i><b>6.2.5</b> Changing the appearance of the axes</a></li>
<li class="chapter" data-level="6.2.6" data-path="graphics.html"><a href="graphics.html#dont-panic"><i class="fa fa-check"></i><b>6.2.6</b> Don’t panic</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="graphics.html"><a href="graphics.html#hist"><i class="fa fa-check"></i><b>6.3</b> Histograms</a><ul>
<li class="chapter" data-level="6.3.1" data-path="graphics.html"><a href="graphics.html#visual-style-of-your-histogram"><i class="fa fa-check"></i><b>6.3.1</b> Visual style of your histogram</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="graphics.html"><a href="graphics.html#stem"><i class="fa fa-check"></i><b>6.4</b> Stem and leaf plots</a></li>
<li class="chapter" data-level="6.5" data-path="graphics.html"><a href="graphics.html#boxplots"><i class="fa fa-check"></i><b>6.5</b> Boxplots</a><ul>
<li class="chapter" data-level="6.5.1" data-path="graphics.html"><a href="graphics.html#visual-style-of-your-boxplot"><i class="fa fa-check"></i><b>6.5.1</b> Visual style of your boxplot</a></li>
<li class="chapter" data-level="6.5.2" data-path="graphics.html"><a href="graphics.html#boxplotoutliers"><i class="fa fa-check"></i><b>6.5.2</b> Using box plots to detect outliers</a></li>
<li class="chapter" data-level="6.5.3" data-path="graphics.html"><a href="graphics.html#multipleboxplots"><i class="fa fa-check"></i><b>6.5.3</b> Drawing multiple boxplots</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="graphics.html"><a href="graphics.html#scatterplots"><i class="fa fa-check"></i><b>6.6</b> Scatterplots</a><ul>
<li class="chapter" data-level="6.6.1" data-path="graphics.html"><a href="graphics.html#more-elaborate-options"><i class="fa fa-check"></i><b>6.6.1</b> More elaborate options</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="graphics.html"><a href="graphics.html#bargraph"><i class="fa fa-check"></i><b>6.7</b> Bar graphs</a><ul>
<li class="chapter" data-level="6.7.1" data-path="graphics.html"><a href="graphics.html#par"><i class="fa fa-check"></i><b>6.7.1</b> Changing global settings using par()</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="graphics.html"><a href="graphics.html#saveimage"><i class="fa fa-check"></i><b>6.8</b> Saving image files using R and Rstudio</a><ul>
<li class="chapter" data-level="6.8.1" data-path="graphics.html"><a href="graphics.html#the-ugly-details-advanced"><i class="fa fa-check"></i><b>6.8.1</b> The ugly details (advanced)</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="graphics.html"><a href="graphics.html#summary-4"><i class="fa fa-check"></i><b>6.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="datahandling.html"><a href="datahandling.html"><i class="fa fa-check"></i><b>7</b> Pragmatic matters</a><ul>
<li class="chapter" data-level="7.1" data-path="datahandling.html"><a href="datahandling.html#freqtables"><i class="fa fa-check"></i><b>7.1</b> Tabulating and cross-tabulating data</a><ul>
<li class="chapter" data-level="7.1.1" data-path="datahandling.html"><a href="datahandling.html#creating-tables-from-vectors"><i class="fa fa-check"></i><b>7.1.1</b> Creating tables from vectors</a></li>
<li class="chapter" data-level="7.1.2" data-path="datahandling.html"><a href="datahandling.html#creating-tables-from-data-frames"><i class="fa fa-check"></i><b>7.1.2</b> Creating tables from data frames</a></li>
<li class="chapter" data-level="7.1.3" data-path="datahandling.html"><a href="datahandling.html#converting-a-table-of-counts-to-a-table-of-proportions"><i class="fa fa-check"></i><b>7.1.3</b> Converting a table of counts to a table of proportions</a></li>
<li class="chapter" data-level="7.1.4" data-path="datahandling.html"><a href="datahandling.html#low-level-tabulation"><i class="fa fa-check"></i><b>7.1.4</b> Low level tabulation</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="datahandling.html"><a href="datahandling.html#transform"><i class="fa fa-check"></i><b>7.2</b> Transforming and recoding a variable</a><ul>
<li class="chapter" data-level="7.2.1" data-path="datahandling.html"><a href="datahandling.html#creating-a-transformed-variable"><i class="fa fa-check"></i><b>7.2.1</b> Creating a transformed variable</a></li>
<li class="chapter" data-level="7.2.2" data-path="datahandling.html"><a href="datahandling.html#cutting-a-numeric-variable-into-categories"><i class="fa fa-check"></i><b>7.2.2</b> Cutting a numeric variable into categories</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="datahandling.html"><a href="datahandling.html#mathfunc"><i class="fa fa-check"></i><b>7.3</b> A few more mathematical functions and operations</a><ul>
<li class="chapter" data-level="7.3.1" data-path="datahandling.html"><a href="datahandling.html#rounding-a-number"><i class="fa fa-check"></i><b>7.3.1</b> Rounding a number</a></li>
<li class="chapter" data-level="7.3.2" data-path="datahandling.html"><a href="datahandling.html#modulus-and-integer-division"><i class="fa fa-check"></i><b>7.3.2</b> Modulus and integer division</a></li>
<li class="chapter" data-level="7.3.3" data-path="datahandling.html"><a href="datahandling.html#logarithms-and-exponentials"><i class="fa fa-check"></i><b>7.3.3</b> Logarithms and exponentials</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="datahandling.html"><a href="datahandling.html#subset"><i class="fa fa-check"></i><b>7.4</b> Extracting a subset of a vector</a><ul>
<li class="chapter" data-level="7.4.1" data-path="datahandling.html"><a href="datahandling.html#refresher"><i class="fa fa-check"></i><b>7.4.1</b> Refresher</a></li>
<li class="chapter" data-level="7.4.2" data-path="datahandling.html"><a href="datahandling.html#using-in-to-match-multiple-cases"><i class="fa fa-check"></i><b>7.4.2</b> Using <code>%in%</code> to match multiple cases</a></li>
<li class="chapter" data-level="7.4.3" data-path="datahandling.html"><a href="datahandling.html#using-negative-indices-to-drop-elements"><i class="fa fa-check"></i><b>7.4.3</b> Using negative indices to drop elements</a></li>
<li class="chapter" data-level="7.4.4" data-path="datahandling.html"><a href="datahandling.html#splitting-a-vector-by-group"><i class="fa fa-check"></i><b>7.4.4</b> Splitting a vector by group</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="datahandling.html"><a href="datahandling.html#subsetdataframe"><i class="fa fa-check"></i><b>7.5</b> Extracting a subset of a data frame</a><ul>
<li class="chapter" data-level="7.5.1" data-path="datahandling.html"><a href="datahandling.html#using-the-subset-function"><i class="fa fa-check"></i><b>7.5.1</b> Using the <code>subset()</code> function</a></li>
<li class="chapter" data-level="7.5.2" data-path="datahandling.html"><a href="datahandling.html#using-square-brackets-i.-rows-and-columns"><i class="fa fa-check"></i><b>7.5.2</b> Using square brackets: I. Rows and columns</a></li>
<li class="chapter" data-level="7.5.3" data-path="datahandling.html"><a href="datahandling.html#using-square-brackets-ii.-some-elaborations"><i class="fa fa-check"></i><b>7.5.3</b> Using square brackets: II. Some elaborations</a></li>
<li class="chapter" data-level="7.5.4" data-path="datahandling.html"><a href="datahandling.html#dropping"><i class="fa fa-check"></i><b>7.5.4</b> Using square brackets: III. Understanding “dropping”</a></li>
<li class="chapter" data-level="7.5.5" data-path="datahandling.html"><a href="datahandling.html#using-square-brackets-iv.-columns-only"><i class="fa fa-check"></i><b>7.5.5</b> Using square brackets: IV. Columns only</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="datahandling.html"><a href="datahandling.html#sort"><i class="fa fa-check"></i><b>7.6</b> Sorting, flipping and merging data</a><ul>
<li class="chapter" data-level="7.6.1" data-path="datahandling.html"><a href="datahandling.html#sorting-a-numeric-or-character-vector"><i class="fa fa-check"></i><b>7.6.1</b> Sorting a numeric or character vector</a></li>
<li class="chapter" data-level="7.6.2" data-path="datahandling.html"><a href="datahandling.html#sorting-a-factor"><i class="fa fa-check"></i><b>7.6.2</b> Sorting a factor</a></li>
<li class="chapter" data-level="7.6.3" data-path="datahandling.html"><a href="datahandling.html#sortframe"><i class="fa fa-check"></i><b>7.6.3</b> Sorting a data frame</a></li>
<li class="chapter" data-level="7.6.4" data-path="datahandling.html"><a href="datahandling.html#binding-vectors-together"><i class="fa fa-check"></i><b>7.6.4</b> Binding vectors together</a></li>
<li class="chapter" data-level="7.6.5" data-path="datahandling.html"><a href="datahandling.html#binding-multiple-copies-of-the-same-vector-together"><i class="fa fa-check"></i><b>7.6.5</b> Binding multiple copies of the same vector together</a></li>
<li class="chapter" data-level="7.6.6" data-path="datahandling.html"><a href="datahandling.html#transposing-a-matrix-or-data-frame"><i class="fa fa-check"></i><b>7.6.6</b> Transposing a matrix or data frame</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="datahandling.html"><a href="datahandling.html#reshape"><i class="fa fa-check"></i><b>7.7</b> Reshaping a data frame</a><ul>
<li class="chapter" data-level="7.7.1" data-path="datahandling.html"><a href="datahandling.html#long-form-and-wide-form-data"><i class="fa fa-check"></i><b>7.7.1</b> Long form and wide form data</a></li>
<li class="chapter" data-level="7.7.2" data-path="datahandling.html"><a href="datahandling.html#reshaping-data-using-widetolong"><i class="fa fa-check"></i><b>7.7.2</b> Reshaping data using <code>wideToLong()</code></a></li>
<li class="chapter" data-level="7.7.3" data-path="datahandling.html"><a href="datahandling.html#reshaping-data-using-longtowide"><i class="fa fa-check"></i><b>7.7.3</b> Reshaping data using <code>longToWide()</code></a></li>
<li class="chapter" data-level="7.7.4" data-path="datahandling.html"><a href="datahandling.html#reshaping-with-multiple-within-subject-factors"><i class="fa fa-check"></i><b>7.7.4</b> Reshaping with multiple within-subject factors</a></li>
<li class="chapter" data-level="7.7.5" data-path="datahandling.html"><a href="datahandling.html#what-other-options-are-there"><i class="fa fa-check"></i><b>7.7.5</b> What other options are there?</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="datahandling.html"><a href="datahandling.html#textprocessing"><i class="fa fa-check"></i><b>7.8</b> Working with text</a><ul>
<li class="chapter" data-level="7.8.1" data-path="datahandling.html"><a href="datahandling.html#shortening-a-string"><i class="fa fa-check"></i><b>7.8.1</b> Shortening a string</a></li>
<li class="chapter" data-level="7.8.2" data-path="datahandling.html"><a href="datahandling.html#pasting-strings-together"><i class="fa fa-check"></i><b>7.8.2</b> Pasting strings together</a></li>
<li class="chapter" data-level="7.8.3" data-path="datahandling.html"><a href="datahandling.html#splitting-strings"><i class="fa fa-check"></i><b>7.8.3</b> Splitting strings</a></li>
<li class="chapter" data-level="7.8.4" data-path="datahandling.html"><a href="datahandling.html#making-simple-conversions"><i class="fa fa-check"></i><b>7.8.4</b> Making simple conversions</a></li>
<li class="chapter" data-level="7.8.5" data-path="datahandling.html"><a href="datahandling.html#logictext2"><i class="fa fa-check"></i><b>7.8.5</b> Applying logical operations to text</a></li>
<li class="chapter" data-level="7.8.6" data-path="datahandling.html"><a href="datahandling.html#concatenating-and-printing-with-cat"><i class="fa fa-check"></i><b>7.8.6</b> Concatenating and printing with <code>cat()</code></a></li>
<li class="chapter" data-level="7.8.7" data-path="datahandling.html"><a href="datahandling.html#escapechars"><i class="fa fa-check"></i><b>7.8.7</b> Using escape characters in text</a></li>
<li class="chapter" data-level="7.8.8" data-path="datahandling.html"><a href="datahandling.html#matching-and-substituting-text"><i class="fa fa-check"></i><b>7.8.8</b> Matching and substituting text</a></li>
<li class="chapter" data-level="7.8.9" data-path="datahandling.html"><a href="datahandling.html#regex"><i class="fa fa-check"></i><b>7.8.9</b> Regular expressions (not really)</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="datahandling.html"><a href="datahandling.html#importing"><i class="fa fa-check"></i><b>7.9</b> Reading unusual data files</a><ul>
<li class="chapter" data-level="7.9.1" data-path="datahandling.html"><a href="datahandling.html#loading-data-from-text-files"><i class="fa fa-check"></i><b>7.9.1</b> Loading data from text files</a></li>
<li class="chapter" data-level="7.9.2" data-path="datahandling.html"><a href="datahandling.html#loading-data-from-spss-and-other-statistics-packages"><i class="fa fa-check"></i><b>7.9.2</b> Loading data from SPSS (and other statistics packages)</a></li>
<li class="chapter" data-level="7.9.3" data-path="datahandling.html"><a href="datahandling.html#loading-excel-files"><i class="fa fa-check"></i><b>7.9.3</b> Loading Excel files</a></li>
<li class="chapter" data-level="7.9.4" data-path="datahandling.html"><a href="datahandling.html#loading-matlab-octave-files"><i class="fa fa-check"></i><b>7.9.4</b> Loading Matlab (&amp; Octave) files</a></li>
<li class="chapter" data-level="7.9.5" data-path="datahandling.html"><a href="datahandling.html#saving-other-kinds-of-data"><i class="fa fa-check"></i><b>7.9.5</b> Saving other kinds of data</a></li>
<li class="chapter" data-level="7.9.6" data-path="datahandling.html"><a href="datahandling.html#are-we-done-yet"><i class="fa fa-check"></i><b>7.9.6</b> Are we done yet?</a></li>
</ul></li>
<li class="chapter" data-level="7.10" data-path="datahandling.html"><a href="datahandling.html#coercion"><i class="fa fa-check"></i><b>7.10</b> Coercing data from one class to another</a></li>
<li class="chapter" data-level="7.11" data-path="datahandling.html"><a href="datahandling.html#datastructures"><i class="fa fa-check"></i><b>7.11</b> Other useful data structures</a><ul>
<li class="chapter" data-level="7.11.1" data-path="datahandling.html"><a href="datahandling.html#matrix"><i class="fa fa-check"></i><b>7.11.1</b> Matrices</a></li>
<li class="chapter" data-level="7.11.2" data-path="datahandling.html"><a href="datahandling.html#orderedfactors"><i class="fa fa-check"></i><b>7.11.2</b> Ordered factors</a></li>
<li class="chapter" data-level="7.11.3" data-path="datahandling.html"><a href="datahandling.html#dates"><i class="fa fa-check"></i><b>7.11.3</b> Dates and times</a></li>
</ul></li>
<li class="chapter" data-level="7.12" data-path="datahandling.html"><a href="datahandling.html#miscdatahandling"><i class="fa fa-check"></i><b>7.12</b> Miscellaneous topics</a><ul>
<li class="chapter" data-level="7.12.1" data-path="datahandling.html"><a href="datahandling.html#the-problems-with-floating-point-arithmetic"><i class="fa fa-check"></i><b>7.12.1</b> The problems with floating point arithmetic</a></li>
<li class="chapter" data-level="7.12.2" data-path="datahandling.html"><a href="datahandling.html#recycling"><i class="fa fa-check"></i><b>7.12.2</b> The recycling rule</a></li>
<li class="chapter" data-level="7.12.3" data-path="datahandling.html"><a href="datahandling.html#environments"><i class="fa fa-check"></i><b>7.12.3</b> An introduction to environments</a></li>
<li class="chapter" data-level="7.12.4" data-path="datahandling.html"><a href="datahandling.html#attaching-a-data-frame"><i class="fa fa-check"></i><b>7.12.4</b> Attaching a data frame</a></li>
</ul></li>
<li class="chapter" data-level="7.13" data-path="datahandling.html"><a href="datahandling.html#summary-5"><i class="fa fa-check"></i><b>7.13</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="scripting.html"><a href="scripting.html"><i class="fa fa-check"></i><b>8</b> Basic programming</a><ul>
<li class="chapter" data-level="8.1" data-path="scripting.html"><a href="scripting.html#scripts"><i class="fa fa-check"></i><b>8.1</b> Scripts</a><ul>
<li class="chapter" data-level="8.1.1" data-path="scripting.html"><a href="scripting.html#why-use-scripts"><i class="fa fa-check"></i><b>8.1.1</b> Why use scripts?</a></li>
<li class="chapter" data-level="8.1.2" data-path="scripting.html"><a href="scripting.html#our-first-script"><i class="fa fa-check"></i><b>8.1.2</b> Our first script</a></li>
<li class="chapter" data-level="8.1.3" data-path="scripting.html"><a href="scripting.html#using-rstudio-to-write-scripts"><i class="fa fa-check"></i><b>8.1.3</b> Using Rstudio to write scripts</a></li>
<li class="chapter" data-level="8.1.4" data-path="scripting.html"><a href="scripting.html#commenting-your-script"><i class="fa fa-check"></i><b>8.1.4</b> Commenting your script</a></li>
<li class="chapter" data-level="8.1.5" data-path="scripting.html"><a href="scripting.html#differences-between-scripts-and-the-command-line"><i class="fa fa-check"></i><b>8.1.5</b> Differences between scripts and the command line</a></li>
<li class="chapter" data-level="8.1.6" data-path="scripting.html"><a href="scripting.html#done"><i class="fa fa-check"></i><b>8.1.6</b> Done!</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="scripting.html"><a href="scripting.html#loops"><i class="fa fa-check"></i><b>8.2</b> Loops</a><ul>
<li class="chapter" data-level="8.2.1" data-path="scripting.html"><a href="scripting.html#the-while-loop"><i class="fa fa-check"></i><b>8.2.1</b> The <code>while</code> loop</a></li>
<li class="chapter" data-level="8.2.2" data-path="scripting.html"><a href="scripting.html#for"><i class="fa fa-check"></i><b>8.2.2</b> The <code>for</code> loop</a></li>
<li class="chapter" data-level="8.2.3" data-path="scripting.html"><a href="scripting.html#a-more-realistic-example-of-a-loop"><i class="fa fa-check"></i><b>8.2.3</b> A more realistic example of a loop</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="scripting.html"><a href="scripting.html#if"><i class="fa fa-check"></i><b>8.3</b> Conditional statements</a></li>
<li class="chapter" data-level="8.4" data-path="scripting.html"><a href="scripting.html#functions"><i class="fa fa-check"></i><b>8.4</b> Writing functions</a><ul>
<li class="chapter" data-level="8.4.1" data-path="scripting.html"><a href="scripting.html#dotsargument"><i class="fa fa-check"></i><b>8.4.1</b> Function arguments revisited</a></li>
<li class="chapter" data-level="8.4.2" data-path="scripting.html"><a href="scripting.html#theres-more-to-functions-than-this"><i class="fa fa-check"></i><b>8.4.2</b> There’s more to functions than this</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="scripting.html"><a href="scripting.html#vectorised"><i class="fa fa-check"></i><b>8.5</b> Implicit loops</a></li>
<li class="chapter" data-level="8.6" data-path="scripting.html"><a href="scripting.html#summary-6"><i class="fa fa-check"></i><b>8.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-iv-statistical-theory.html"><a href="part-iv-statistical-theory.html"><i class="fa fa-check"></i>Part IV. Statistical theory</a><ul>
<li class="chapter" data-level="" data-path="part-iv-statistical-theory.html"><a href="part-iv-statistical-theory.html#on-the-limits-of-logical-reasoning"><i class="fa fa-check"></i>On the limits of logical reasoning</a></li>
<li class="chapter" data-level="" data-path="part-iv-statistical-theory.html"><a href="part-iv-statistical-theory.html#learning-without-making-assumptions-is-a-myth"><i class="fa fa-check"></i>Learning without making assumptions is a myth</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>9</b> Introduction to probability</a><ul>
<li class="chapter" data-level="9.1" data-path="probability.html"><a href="probability.html#probstats"><i class="fa fa-check"></i><b>9.1</b> How are probability and statistics different?</a></li>
<li class="chapter" data-level="9.2" data-path="probability.html"><a href="probability.html#probmeaning"><i class="fa fa-check"></i><b>9.2</b> What does probability mean?</a><ul>
<li class="chapter" data-level="9.2.1" data-path="probability.html"><a href="probability.html#the-frequentist-view"><i class="fa fa-check"></i><b>9.2.1</b> The frequentist view</a></li>
<li class="chapter" data-level="9.2.2" data-path="probability.html"><a href="probability.html#the-bayesian-view"><i class="fa fa-check"></i><b>9.2.2</b> The Bayesian view</a></li>
<li class="chapter" data-level="9.2.3" data-path="probability.html"><a href="probability.html#whats-the-difference-and-who-is-right"><i class="fa fa-check"></i><b>9.2.3</b> What’s the difference? And who is right?</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="probability.html"><a href="probability.html#basicprobability"><i class="fa fa-check"></i><b>9.3</b> Basic probability theory</a><ul>
<li class="chapter" data-level="9.3.1" data-path="probability.html"><a href="probability.html#introducing-probability-distributions"><i class="fa fa-check"></i><b>9.3.1</b> Introducing probability distributions</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="probability.html"><a href="probability.html#binomial"><i class="fa fa-check"></i><b>9.4</b> The binomial distribution</a><ul>
<li class="chapter" data-level="9.4.1" data-path="probability.html"><a href="probability.html#introducing-the-binomial"><i class="fa fa-check"></i><b>9.4.1</b> Introducing the binomial</a></li>
<li class="chapter" data-level="9.4.2" data-path="probability.html"><a href="probability.html#working-with-the-binomial-distribution-in-r"><i class="fa fa-check"></i><b>9.4.2</b> Working with the binomial distribution in R</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="probability.html"><a href="probability.html#normal"><i class="fa fa-check"></i><b>9.5</b> The normal distribution</a><ul>
<li class="chapter" data-level="9.5.1" data-path="probability.html"><a href="probability.html#density"><i class="fa fa-check"></i><b>9.5.1</b> Probability density</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="probability.html"><a href="probability.html#otherdists"><i class="fa fa-check"></i><b>9.6</b> Other useful distributions</a></li>
<li class="chapter" data-level="9.7" data-path="probability.html"><a href="probability.html#summary-7"><i class="fa fa-check"></i><b>9.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>10</b> Estimating unknown quantities from a sample</a><ul>
<li class="chapter" data-level="10.1" data-path="estimation.html"><a href="estimation.html#srs"><i class="fa fa-check"></i><b>10.1</b> Samples, populations and sampling</a><ul>
<li class="chapter" data-level="10.1.1" data-path="estimation.html"><a href="estimation.html#pop"><i class="fa fa-check"></i><b>10.1.1</b> Defining a population</a></li>
<li class="chapter" data-level="10.1.2" data-path="estimation.html"><a href="estimation.html#simple-random-samples"><i class="fa fa-check"></i><b>10.1.2</b> Simple random samples</a></li>
<li class="chapter" data-level="10.1.3" data-path="estimation.html"><a href="estimation.html#most-samples-are-not-simple-random-samples"><i class="fa fa-check"></i><b>10.1.3</b> Most samples are not simple random samples</a></li>
<li class="chapter" data-level="10.1.4" data-path="estimation.html"><a href="estimation.html#how-much-does-it-matter-if-you-dont-have-a-simple-random-sample"><i class="fa fa-check"></i><b>10.1.4</b> How much does it matter if you don’t have a simple random sample?</a></li>
<li class="chapter" data-level="10.1.5" data-path="estimation.html"><a href="estimation.html#population-parameters-and-sample-statistics"><i class="fa fa-check"></i><b>10.1.5</b> Population parameters and sample statistics</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="estimation.html"><a href="estimation.html#lawlargenumbers"><i class="fa fa-check"></i><b>10.2</b> The law of large numbers</a></li>
<li class="chapter" data-level="10.3" data-path="estimation.html"><a href="estimation.html#samplesandclt"><i class="fa fa-check"></i><b>10.3</b> Sampling distributions and the central limit theorem</a><ul>
<li class="chapter" data-level="10.3.1" data-path="estimation.html"><a href="estimation.html#samplingdists"><i class="fa fa-check"></i><b>10.3.1</b> Sampling distribution of the mean</a></li>
<li class="chapter" data-level="10.3.2" data-path="estimation.html"><a href="estimation.html#sampling-distributions-exist-for-any-sample-statistic"><i class="fa fa-check"></i><b>10.3.2</b> Sampling distributions exist for any sample statistic!</a></li>
<li class="chapter" data-level="10.3.3" data-path="estimation.html"><a href="estimation.html#clt"><i class="fa fa-check"></i><b>10.3.3</b> The central limit theorem</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="estimation.html"><a href="estimation.html#pointestimates"><i class="fa fa-check"></i><b>10.4</b> Estimating population parameters</a><ul>
<li class="chapter" data-level="10.4.1" data-path="estimation.html"><a href="estimation.html#estimating-the-population-mean"><i class="fa fa-check"></i><b>10.4.1</b> Estimating the population mean</a></li>
<li class="chapter" data-level="10.4.2" data-path="estimation.html"><a href="estimation.html#estimating-the-population-standard-deviation"><i class="fa fa-check"></i><b>10.4.2</b> Estimating the population standard deviation</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="estimation.html"><a href="estimation.html#ci"><i class="fa fa-check"></i><b>10.5</b> Estimating a confidence interval</a><ul>
<li class="chapter" data-level="10.5.1" data-path="estimation.html"><a href="estimation.html#a-slight-mistake-in-the-formula"><i class="fa fa-check"></i><b>10.5.1</b> A slight mistake in the formula</a></li>
<li class="chapter" data-level="10.5.2" data-path="estimation.html"><a href="estimation.html#interpreting-a-confidence-interval"><i class="fa fa-check"></i><b>10.5.2</b> Interpreting a confidence interval</a></li>
<li class="chapter" data-level="10.5.3" data-path="estimation.html"><a href="estimation.html#calculating-confidence-intervals-in-r"><i class="fa fa-check"></i><b>10.5.3</b> Calculating confidence intervals in R</a></li>
<li class="chapter" data-level="10.5.4" data-path="estimation.html"><a href="estimation.html#ciplots"><i class="fa fa-check"></i><b>10.5.4</b> Plotting confidence intervals in R</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="estimation.html"><a href="estimation.html#summary-8"><i class="fa fa-check"></i><b>10.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="hypothesistesting.html"><a href="hypothesistesting.html"><i class="fa fa-check"></i><b>11</b> Hypothesis testing</a><ul>
<li class="chapter" data-level="11.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#hypotheses"><i class="fa fa-check"></i><b>11.1</b> A menagerie of hypotheses</a><ul>
<li class="chapter" data-level="11.1.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#research-hypotheses-versus-statistical-hypotheses"><i class="fa fa-check"></i><b>11.1.1</b> Research hypotheses versus statistical hypotheses</a></li>
<li class="chapter" data-level="11.1.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#null-hypotheses-and-alternative-hypotheses"><i class="fa fa-check"></i><b>11.1.2</b> Null hypotheses and alternative hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#errortypes"><i class="fa fa-check"></i><b>11.2</b> Two types of errors</a></li>
<li class="chapter" data-level="11.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#teststatistics"><i class="fa fa-check"></i><b>11.3</b> Test statistics and sampling distributions</a></li>
<li class="chapter" data-level="11.4" data-path="hypothesistesting.html"><a href="hypothesistesting.html#decisionmaking"><i class="fa fa-check"></i><b>11.4</b> Making decisions</a><ul>
<li class="chapter" data-level="11.4.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#critical-regions-and-critical-values"><i class="fa fa-check"></i><b>11.4.1</b> Critical regions and critical values</a></li>
<li class="chapter" data-level="11.4.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#a-note-on-statistical-significance"><i class="fa fa-check"></i><b>11.4.2</b> A note on statistical “significance”</a></li>
<li class="chapter" data-level="11.4.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#onesidedtests"><i class="fa fa-check"></i><b>11.4.3</b> The difference between one sided and two sided tests</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="hypothesistesting.html"><a href="hypothesistesting.html#pvalue"><i class="fa fa-check"></i><b>11.5</b> The <span class="math inline">\(p\)</span> value of a test</a><ul>
<li class="chapter" data-level="11.5.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#a-softer-view-of-decision-making"><i class="fa fa-check"></i><b>11.5.1</b> A softer view of decision making</a></li>
<li class="chapter" data-level="11.5.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#the-probability-of-extreme-data"><i class="fa fa-check"></i><b>11.5.2</b> The probability of extreme data</a></li>
<li class="chapter" data-level="11.5.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#a-common-mistake"><i class="fa fa-check"></i><b>11.5.3</b> A common mistake</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="hypothesistesting.html"><a href="hypothesistesting.html#writeup"><i class="fa fa-check"></i><b>11.6</b> Reporting the results of a hypothesis test</a><ul>
<li class="chapter" data-level="11.6.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#the-issue"><i class="fa fa-check"></i><b>11.6.1</b> The issue</a></li>
<li class="chapter" data-level="11.6.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#two-proposed-solutions"><i class="fa fa-check"></i><b>11.6.2</b> Two proposed solutions</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="hypothesistesting.html"><a href="hypothesistesting.html#running-the-hypothesis-test-in-practice"><i class="fa fa-check"></i><b>11.7</b> Running the hypothesis test in practice</a></li>
<li class="chapter" data-level="11.8" data-path="hypothesistesting.html"><a href="hypothesistesting.html#effectsize"><i class="fa fa-check"></i><b>11.8</b> Effect size, sample size and power</a><ul>
<li class="chapter" data-level="11.8.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#the-power-function"><i class="fa fa-check"></i><b>11.8.1</b> The power function</a></li>
<li class="chapter" data-level="11.8.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#effect-size"><i class="fa fa-check"></i><b>11.8.2</b> Effect size</a></li>
<li class="chapter" data-level="11.8.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#increasing-the-power-of-your-study"><i class="fa fa-check"></i><b>11.8.3</b> Increasing the power of your study</a></li>
</ul></li>
<li class="chapter" data-level="11.9" data-path="hypothesistesting.html"><a href="hypothesistesting.html#nhstmess"><i class="fa fa-check"></i><b>11.9</b> Some issues to consider</a><ul>
<li class="chapter" data-level="11.9.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#neyman-versus-fisher"><i class="fa fa-check"></i><b>11.9.1</b> Neyman versus Fisher</a></li>
<li class="chapter" data-level="11.9.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#bayesians-versus-frequentists"><i class="fa fa-check"></i><b>11.9.2</b> Bayesians versus frequentists</a></li>
<li class="chapter" data-level="11.9.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#traps"><i class="fa fa-check"></i><b>11.9.3</b> Traps</a></li>
</ul></li>
<li class="chapter" data-level="11.10" data-path="hypothesistesting.html"><a href="hypothesistesting.html#summary-9"><i class="fa fa-check"></i><b>11.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-v-statistical-tools.html"><a href="part-v-statistical-tools.html"><i class="fa fa-check"></i>Part V. Statistical tools</a></li>
<li class="chapter" data-level="12" data-path="chisquare.html"><a href="chisquare.html"><i class="fa fa-check"></i><b>12</b> Categorical data analysis</a><ul>
<li class="chapter" data-level="12.1" data-path="chisquare.html"><a href="chisquare.html#goftest"><i class="fa fa-check"></i><b>12.1</b> The <span class="math inline">\(\chi^2\)</span> goodness-of-fit test</a><ul>
<li class="chapter" data-level="12.1.1" data-path="chisquare.html"><a href="chisquare.html#the-cards-data"><i class="fa fa-check"></i><b>12.1.1</b> The cards data</a></li>
<li class="chapter" data-level="12.1.2" data-path="chisquare.html"><a href="chisquare.html#the-null-hypothesis-and-the-alternative-hypothesis"><i class="fa fa-check"></i><b>12.1.2</b> The null hypothesis and the alternative hypothesis</a></li>
<li class="chapter" data-level="12.1.3" data-path="chisquare.html"><a href="chisquare.html#the-goodness-of-fit-test-statistic"><i class="fa fa-check"></i><b>12.1.3</b> The “goodness of fit” test statistic</a></li>
<li class="chapter" data-level="12.1.4" data-path="chisquare.html"><a href="chisquare.html#the-sampling-distribution-of-the-gof-statistic-advanced"><i class="fa fa-check"></i><b>12.1.4</b> The sampling distribution of the GOF statistic (advanced)</a></li>
<li class="chapter" data-level="12.1.5" data-path="chisquare.html"><a href="chisquare.html#degrees-of-freedom"><i class="fa fa-check"></i><b>12.1.5</b> Degrees of freedom</a></li>
<li class="chapter" data-level="12.1.6" data-path="chisquare.html"><a href="chisquare.html#testing-the-null-hypothesis"><i class="fa fa-check"></i><b>12.1.6</b> Testing the null hypothesis</a></li>
<li class="chapter" data-level="12.1.7" data-path="chisquare.html"><a href="chisquare.html#gofTestInR"><i class="fa fa-check"></i><b>12.1.7</b> Doing the test in R</a></li>
<li class="chapter" data-level="12.1.8" data-path="chisquare.html"><a href="chisquare.html#specifying-a-different-null-hypothesis"><i class="fa fa-check"></i><b>12.1.8</b> Specifying a different null hypothesis</a></li>
<li class="chapter" data-level="12.1.9" data-path="chisquare.html"><a href="chisquare.html#chisqreport"><i class="fa fa-check"></i><b>12.1.9</b> How to report the results of the test</a></li>
<li class="chapter" data-level="12.1.10" data-path="chisquare.html"><a href="chisquare.html#a-comment-on-statistical-notation-advanced"><i class="fa fa-check"></i><b>12.1.10</b> A comment on statistical notation (advanced)</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="chisquare.html"><a href="chisquare.html#chisqindependence"><i class="fa fa-check"></i><b>12.2</b> The <span class="math inline">\(\chi^2\)</span> test of independence (or association)</a><ul>
<li class="chapter" data-level="12.2.1" data-path="chisquare.html"><a href="chisquare.html#constructing-our-hypothesis-test"><i class="fa fa-check"></i><b>12.2.1</b> Constructing our hypothesis test</a></li>
<li class="chapter" data-level="12.2.2" data-path="chisquare.html"><a href="chisquare.html#AssocTestInR"><i class="fa fa-check"></i><b>12.2.2</b> Doing the test in R</a></li>
<li class="chapter" data-level="12.2.3" data-path="chisquare.html"><a href="chisquare.html#postscript"><i class="fa fa-check"></i><b>12.2.3</b> Postscript</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="chisquare.html"><a href="chisquare.html#yates"><i class="fa fa-check"></i><b>12.3</b> The continuity correction</a></li>
<li class="chapter" data-level="12.4" data-path="chisquare.html"><a href="chisquare.html#chisqeffectsize"><i class="fa fa-check"></i><b>12.4</b> Effect size</a></li>
<li class="chapter" data-level="12.5" data-path="chisquare.html"><a href="chisquare.html#chisqassumptions"><i class="fa fa-check"></i><b>12.5</b> Assumptions of the test(s)</a></li>
<li class="chapter" data-level="12.6" data-path="chisquare.html"><a href="chisquare.html#chisq.test"><i class="fa fa-check"></i><b>12.6</b> The most typical way to do chi-square tests in R</a></li>
<li class="chapter" data-level="12.7" data-path="chisquare.html"><a href="chisquare.html#fisherexacttest"><i class="fa fa-check"></i><b>12.7</b> The Fisher exact test</a></li>
<li class="chapter" data-level="12.8" data-path="chisquare.html"><a href="chisquare.html#mcnemar"><i class="fa fa-check"></i><b>12.8</b> The McNemar test</a><ul>
<li class="chapter" data-level="12.8.1" data-path="chisquare.html"><a href="chisquare.html#doing-the-mcnemar-test-in-r"><i class="fa fa-check"></i><b>12.8.1</b> Doing the McNemar test in R</a></li>
</ul></li>
<li class="chapter" data-level="12.9" data-path="chisquare.html"><a href="chisquare.html#whats-the-difference-between-mcnemar-and-independence"><i class="fa fa-check"></i><b>12.9</b> What’s the difference between McNemar and independence?</a></li>
<li class="chapter" data-level="12.10" data-path="chisquare.html"><a href="chisquare.html#summary-10"><i class="fa fa-check"></i><b>12.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ttest.html"><a href="ttest.html"><i class="fa fa-check"></i><b>13</b> Comparing two means</a><ul>
<li class="chapter" data-level="13.1" data-path="ttest.html"><a href="ttest.html#the-one-sample-z-test"><i class="fa fa-check"></i><b>13.1</b> The one-sample <span class="math inline">\(z\)</span>-test</a><ul>
<li class="chapter" data-level="13.1.1" data-path="ttest.html"><a href="ttest.html#the-inference-problem-that-the-test-addresses"><i class="fa fa-check"></i><b>13.1.1</b> The inference problem that the test addresses</a></li>
<li class="chapter" data-level="13.1.2" data-path="ttest.html"><a href="ttest.html#constructing-the-hypothesis-test"><i class="fa fa-check"></i><b>13.1.2</b> Constructing the hypothesis test</a></li>
<li class="chapter" data-level="13.1.3" data-path="ttest.html"><a href="ttest.html#a-worked-example-using-r"><i class="fa fa-check"></i><b>13.1.3</b> A worked example using R</a></li>
<li class="chapter" data-level="13.1.4" data-path="ttest.html"><a href="ttest.html#zassumptions"><i class="fa fa-check"></i><b>13.1.4</b> Assumptions of the <span class="math inline">\(z\)</span>-test</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ttest.html"><a href="ttest.html#onesamplettest"><i class="fa fa-check"></i><b>13.2</b> The one-sample <span class="math inline">\(t\)</span>-test</a><ul>
<li class="chapter" data-level="13.2.1" data-path="ttest.html"><a href="ttest.html#introducing-the-t-test"><i class="fa fa-check"></i><b>13.2.1</b> Introducing the <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="13.2.2" data-path="ttest.html"><a href="ttest.html#doing-the-test-in-r"><i class="fa fa-check"></i><b>13.2.2</b> Doing the test in R</a></li>
<li class="chapter" data-level="13.2.3" data-path="ttest.html"><a href="ttest.html#ttestoneassumptions"><i class="fa fa-check"></i><b>13.2.3</b> Assumptions of the one sample <span class="math inline">\(t\)</span>-test</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="ttest.html"><a href="ttest.html#studentttest"><i class="fa fa-check"></i><b>13.3</b> The independent samples <span class="math inline">\(t\)</span>-test (Student test)</a><ul>
<li class="chapter" data-level="13.3.1" data-path="ttest.html"><a href="ttest.html#the-data-1"><i class="fa fa-check"></i><b>13.3.1</b> The data</a></li>
<li class="chapter" data-level="13.3.2" data-path="ttest.html"><a href="ttest.html#introducing-the-test"><i class="fa fa-check"></i><b>13.3.2</b> Introducing the test</a></li>
<li class="chapter" data-level="13.3.3" data-path="ttest.html"><a href="ttest.html#a-pooled-estimate-of-the-standard-deviation"><i class="fa fa-check"></i><b>13.3.3</b> A “pooled estimate” of the standard deviation</a></li>
<li class="chapter" data-level="13.3.4" data-path="ttest.html"><a href="ttest.html#the-same-pooled-estimate-described-differently"><i class="fa fa-check"></i><b>13.3.4</b> The same pooled estimate, described differently</a></li>
<li class="chapter" data-level="13.3.5" data-path="ttest.html"><a href="ttest.html#completing-the-test"><i class="fa fa-check"></i><b>13.3.5</b> Completing the test</a></li>
<li class="chapter" data-level="13.3.6" data-path="ttest.html"><a href="ttest.html#doing-the-test-in-r-1"><i class="fa fa-check"></i><b>13.3.6</b> Doing the test in R</a></li>
<li class="chapter" data-level="13.3.7" data-path="ttest.html"><a href="ttest.html#positive-and-negative-t-values"><i class="fa fa-check"></i><b>13.3.7</b> Positive and negative <span class="math inline">\(t\)</span> values</a></li>
<li class="chapter" data-level="13.3.8" data-path="ttest.html"><a href="ttest.html#studentassumptions"><i class="fa fa-check"></i><b>13.3.8</b> Assumptions of the test</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="ttest.html"><a href="ttest.html#welchttest"><i class="fa fa-check"></i><b>13.4</b> The independent samples <span class="math inline">\(t\)</span>-test (Welch test)</a><ul>
<li class="chapter" data-level="13.4.1" data-path="ttest.html"><a href="ttest.html#doing-the-test-in-r-2"><i class="fa fa-check"></i><b>13.4.1</b> Doing the test in R</a></li>
<li class="chapter" data-level="13.4.2" data-path="ttest.html"><a href="ttest.html#assumptions-of-the-test"><i class="fa fa-check"></i><b>13.4.2</b> Assumptions of the test</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="ttest.html"><a href="ttest.html#pairedsamplesttest"><i class="fa fa-check"></i><b>13.5</b> The paired-samples <span class="math inline">\(t\)</span>-test</a><ul>
<li class="chapter" data-level="13.5.1" data-path="ttest.html"><a href="ttest.html#the-data-2"><i class="fa fa-check"></i><b>13.5.1</b> The data</a></li>
<li class="chapter" data-level="13.5.2" data-path="ttest.html"><a href="ttest.html#what-is-the-paired-samples-t-test"><i class="fa fa-check"></i><b>13.5.2</b> What is the paired samples <span class="math inline">\(t\)</span>-test?</a></li>
<li class="chapter" data-level="13.5.3" data-path="ttest.html"><a href="ttest.html#doing-the-test-in-r-part-1"><i class="fa fa-check"></i><b>13.5.3</b> Doing the test in R, part 1</a></li>
<li class="chapter" data-level="13.5.4" data-path="ttest.html"><a href="ttest.html#doing-the-test-in-r-part-2"><i class="fa fa-check"></i><b>13.5.4</b> Doing the test in R, part 2</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="ttest.html"><a href="ttest.html#one-sided-tests"><i class="fa fa-check"></i><b>13.6</b> One sided tests</a></li>
<li class="chapter" data-level="13.7" data-path="ttest.html"><a href="ttest.html#ttestfunction"><i class="fa fa-check"></i><b>13.7</b> Using the t.test() function</a></li>
<li class="chapter" data-level="13.8" data-path="ttest.html"><a href="ttest.html#cohensd"><i class="fa fa-check"></i><b>13.8</b> Effect size</a><ul>
<li class="chapter" data-level="13.8.1" data-path="ttest.html"><a href="ttest.html#cohens-d-from-one-sample"><i class="fa fa-check"></i><b>13.8.1</b> Cohen’s <span class="math inline">\(d\)</span> from one sample</a></li>
<li class="chapter" data-level="13.8.2" data-path="ttest.html"><a href="ttest.html#cohens-d-from-a-student-t-test"><i class="fa fa-check"></i><b>13.8.2</b> Cohen’s <span class="math inline">\(d\)</span> from a Student <span class="math inline">\(t\)</span> test</a></li>
<li class="chapter" data-level="13.8.3" data-path="ttest.html"><a href="ttest.html#cohens-d-from-a-welch-test"><i class="fa fa-check"></i><b>13.8.3</b> Cohen’s <span class="math inline">\(d\)</span> from a Welch test</a></li>
<li class="chapter" data-level="13.8.4" data-path="ttest.html"><a href="ttest.html#cohens-d-from-a-paired-samples-test"><i class="fa fa-check"></i><b>13.8.4</b> Cohen’s <span class="math inline">\(d\)</span> from a paired-samples test</a></li>
</ul></li>
<li class="chapter" data-level="13.9" data-path="ttest.html"><a href="ttest.html#shapiro"><i class="fa fa-check"></i><b>13.9</b> Checking the normality of a sample</a><ul>
<li class="chapter" data-level="13.9.1" data-path="ttest.html"><a href="ttest.html#qq-plots"><i class="fa fa-check"></i><b>13.9.1</b> QQ plots</a></li>
<li class="chapter" data-level="13.9.2" data-path="ttest.html"><a href="ttest.html#shapiro-wilk-tests"><i class="fa fa-check"></i><b>13.9.2</b> Shapiro-Wilk tests</a></li>
</ul></li>
<li class="chapter" data-level="13.10" data-path="ttest.html"><a href="ttest.html#wilcox"><i class="fa fa-check"></i><b>13.10</b> Testing non-normal data with Wilcoxon tests</a><ul>
<li class="chapter" data-level="13.10.1" data-path="ttest.html"><a href="ttest.html#two-sample-wilcoxon-test"><i class="fa fa-check"></i><b>13.10.1</b> Two sample Wilcoxon test</a></li>
<li class="chapter" data-level="13.10.2" data-path="ttest.html"><a href="ttest.html#one-sample-wilcoxon-test"><i class="fa fa-check"></i><b>13.10.2</b> One sample Wilcoxon test</a></li>
</ul></li>
<li class="chapter" data-level="13.11" data-path="ttest.html"><a href="ttest.html#summary-11"><i class="fa fa-check"></i><b>13.11</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="anova.html"><a href="anova.html"><i class="fa fa-check"></i><b>14</b> Comparing several means (one-way ANOVA)</a><ul>
<li class="chapter" data-level="14.1" data-path="anova.html"><a href="anova.html#anxifree"><i class="fa fa-check"></i><b>14.1</b> An illustrative data set</a></li>
<li class="chapter" data-level="14.2" data-path="anova.html"><a href="anova.html#anovaintro"><i class="fa fa-check"></i><b>14.2</b> How ANOVA works</a><ul>
<li class="chapter" data-level="14.2.1" data-path="anova.html"><a href="anova.html#two-formulas-for-the-variance-of-y"><i class="fa fa-check"></i><b>14.2.1</b> Two formulas for the variance of <span class="math inline">\(Y\)</span></a></li>
<li class="chapter" data-level="14.2.2" data-path="anova.html"><a href="anova.html#from-variances-to-sums-of-squares"><i class="fa fa-check"></i><b>14.2.2</b> From variances to sums of squares</a></li>
<li class="chapter" data-level="14.2.3" data-path="anova.html"><a href="anova.html#from-sums-of-squares-to-the-f-test"><i class="fa fa-check"></i><b>14.2.3</b> From sums of squares to the <span class="math inline">\(F\)</span>-test</a></li>
<li class="chapter" data-level="14.2.4" data-path="anova.html"><a href="anova.html#anovamodel"><i class="fa fa-check"></i><b>14.2.4</b> The model for the data and the meaning of <span class="math inline">\(F\)</span> (advanced)</a></li>
<li class="chapter" data-level="14.2.5" data-path="anova.html"><a href="anova.html#anovacalc"><i class="fa fa-check"></i><b>14.2.5</b> A worked example</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="anova.html"><a href="anova.html#introduceaov"><i class="fa fa-check"></i><b>14.3</b> Running an ANOVA in R</a><ul>
<li class="chapter" data-level="14.3.1" data-path="anova.html"><a href="anova.html#using-the-aov-function-to-specify-your-anova"><i class="fa fa-check"></i><b>14.3.1</b> Using the <code>aov()</code> function to specify your ANOVA</a></li>
<li class="chapter" data-level="14.3.2" data-path="anova.html"><a href="anova.html#aovobjects"><i class="fa fa-check"></i><b>14.3.2</b> Understanding what the <code>aov()</code> function produces</a></li>
<li class="chapter" data-level="14.3.3" data-path="anova.html"><a href="anova.html#running-the-hypothesis-tests-for-the-anova"><i class="fa fa-check"></i><b>14.3.3</b> Running the hypothesis tests for the ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="anova.html"><a href="anova.html#etasquared"><i class="fa fa-check"></i><b>14.4</b> Effect size</a></li>
<li class="chapter" data-level="14.5" data-path="anova.html"><a href="anova.html#posthoc"><i class="fa fa-check"></i><b>14.5</b> Multiple comparisons and post hoc tests</a><ul>
<li class="chapter" data-level="14.5.1" data-path="anova.html"><a href="anova.html#running-pairwise-t-tests"><i class="fa fa-check"></i><b>14.5.1</b> Running “pairwise” <span class="math inline">\(t\)</span>-tests</a></li>
<li class="chapter" data-level="14.5.2" data-path="anova.html"><a href="anova.html#corrections-for-multiple-testing"><i class="fa fa-check"></i><b>14.5.2</b> Corrections for multiple testing</a></li>
<li class="chapter" data-level="14.5.3" data-path="anova.html"><a href="anova.html#bonferroni-corrections"><i class="fa fa-check"></i><b>14.5.3</b> Bonferroni corrections</a></li>
<li class="chapter" data-level="14.5.4" data-path="anova.html"><a href="anova.html#holm-corrections"><i class="fa fa-check"></i><b>14.5.4</b> Holm corrections</a></li>
<li class="chapter" data-level="14.5.5" data-path="anova.html"><a href="anova.html#writing-up-the-post-hoc-test"><i class="fa fa-check"></i><b>14.5.5</b> Writing up the post hoc test</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="anova.html"><a href="anova.html#anovaassumptions"><i class="fa fa-check"></i><b>14.6</b> Assumptions of one-way ANOVA</a><ul>
<li class="chapter" data-level="14.6.1" data-path="anova.html"><a href="anova.html#how-robust-is-anova"><i class="fa fa-check"></i><b>14.6.1</b> How robust is ANOVA?</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="anova.html"><a href="anova.html#levene"><i class="fa fa-check"></i><b>14.7</b> Checking the homogeneity of variance assumption</a><ul>
<li class="chapter" data-level="14.7.1" data-path="anova.html"><a href="anova.html#running-the-levenes-test-in-r"><i class="fa fa-check"></i><b>14.7.1</b> Running the Levene’s test in R</a></li>
<li class="chapter" data-level="14.7.2" data-path="anova.html"><a href="anova.html#additional-comments"><i class="fa fa-check"></i><b>14.7.2</b> Additional comments</a></li>
</ul></li>
<li class="chapter" data-level="14.8" data-path="anova.html"><a href="anova.html#welchoneway"><i class="fa fa-check"></i><b>14.8</b> Removing the homogeneity of variance assumption</a></li>
<li class="chapter" data-level="14.9" data-path="anova.html"><a href="anova.html#anovanormality"><i class="fa fa-check"></i><b>14.9</b> Checking the normality assumption</a></li>
<li class="chapter" data-level="14.10" data-path="anova.html"><a href="anova.html#kruskalwallis"><i class="fa fa-check"></i><b>14.10</b> Removing the normality assumption</a><ul>
<li class="chapter" data-level="14.10.1" data-path="anova.html"><a href="anova.html#the-logic-behind-the-kruskal-wallis-test"><i class="fa fa-check"></i><b>14.10.1</b> The logic behind the Kruskal-Wallis test</a></li>
<li class="chapter" data-level="14.10.2" data-path="anova.html"><a href="anova.html#additional-details"><i class="fa fa-check"></i><b>14.10.2</b> Additional details</a></li>
<li class="chapter" data-level="14.10.3" data-path="anova.html"><a href="anova.html#how-to-run-the-kruskal-wallis-test-in-r"><i class="fa fa-check"></i><b>14.10.3</b> How to run the Kruskal-Wallis test in R</a></li>
</ul></li>
<li class="chapter" data-level="14.11" data-path="anova.html"><a href="anova.html#anovaandt"><i class="fa fa-check"></i><b>14.11</b> On the relationship between ANOVA and the Student <span class="math inline">\(t\)</span> test</a></li>
<li class="chapter" data-level="14.12" data-path="anova.html"><a href="anova.html#summary-12"><i class="fa fa-check"></i><b>14.12</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>15</b> Linear regression</a><ul>
<li class="chapter" data-level="15.1" data-path="regression.html"><a href="regression.html#introregression"><i class="fa fa-check"></i><b>15.1</b> What is a linear regression model?</a></li>
<li class="chapter" data-level="15.2" data-path="regression.html"><a href="regression.html#regressionestimation"><i class="fa fa-check"></i><b>15.2</b> Estimating a linear regression model</a><ul>
<li class="chapter" data-level="15.2.1" data-path="regression.html"><a href="regression.html#lm"><i class="fa fa-check"></i><b>15.2.1</b> Using the <code>lm()</code> function</a></li>
<li class="chapter" data-level="15.2.2" data-path="regression.html"><a href="regression.html#interpreting-the-estimated-model"><i class="fa fa-check"></i><b>15.2.2</b> Interpreting the estimated model</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="regression.html"><a href="regression.html#multipleregression"><i class="fa fa-check"></i><b>15.3</b> Multiple linear regression</a><ul>
<li class="chapter" data-level="15.3.1" data-path="regression.html"><a href="regression.html#doing-it-in-r"><i class="fa fa-check"></i><b>15.3.1</b> Doing it in R</a></li>
<li class="chapter" data-level="15.3.2" data-path="regression.html"><a href="regression.html#formula-for-the-general-case"><i class="fa fa-check"></i><b>15.3.2</b> Formula for the general case</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="regression.html"><a href="regression.html#r2"><i class="fa fa-check"></i><b>15.4</b> Quantifying the fit of the regression model</a><ul>
<li class="chapter" data-level="15.4.1" data-path="regression.html"><a href="regression.html#the-r2-value"><i class="fa fa-check"></i><b>15.4.1</b> The <span class="math inline">\(R^2\)</span> value</a></li>
<li class="chapter" data-level="15.4.2" data-path="regression.html"><a href="regression.html#the-relationship-between-regression-and-correlation"><i class="fa fa-check"></i><b>15.4.2</b> The relationship between regression and correlation</a></li>
<li class="chapter" data-level="15.4.3" data-path="regression.html"><a href="regression.html#the-adjusted-r2-value"><i class="fa fa-check"></i><b>15.4.3</b> The adjusted <span class="math inline">\(R^2\)</span> value</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="regression.html"><a href="regression.html#regressiontests"><i class="fa fa-check"></i><b>15.5</b> Hypothesis tests for regression models</a><ul>
<li class="chapter" data-level="15.5.1" data-path="regression.html"><a href="regression.html#testing-the-model-as-a-whole"><i class="fa fa-check"></i><b>15.5.1</b> Testing the model as a whole</a></li>
<li class="chapter" data-level="15.5.2" data-path="regression.html"><a href="regression.html#tests-for-individual-coefficients"><i class="fa fa-check"></i><b>15.5.2</b> Tests for individual coefficients</a></li>
<li class="chapter" data-level="15.5.3" data-path="regression.html"><a href="regression.html#regressionsummary"><i class="fa fa-check"></i><b>15.5.3</b> Running the hypothesis tests in R</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="regression.html"><a href="regression.html#corrhyp"><i class="fa fa-check"></i><b>15.6</b> Testing the significance of a correlation</a><ul>
<li class="chapter" data-level="15.6.1" data-path="regression.html"><a href="regression.html#hypothesis-tests-for-a-single-correlation"><i class="fa fa-check"></i><b>15.6.1</b> Hypothesis tests for a single correlation</a></li>
<li class="chapter" data-level="15.6.2" data-path="regression.html"><a href="regression.html#corrhyp2"><i class="fa fa-check"></i><b>15.6.2</b> Hypothesis tests for all pairwise correlations</a></li>
</ul></li>
<li class="chapter" data-level="15.7" data-path="regression.html"><a href="regression.html#regressioncoefs"><i class="fa fa-check"></i><b>15.7</b> Regarding regression coefficients</a><ul>
<li class="chapter" data-level="15.7.1" data-path="regression.html"><a href="regression.html#confidence-intervals-for-the-coefficients"><i class="fa fa-check"></i><b>15.7.1</b> Confidence intervals for the coefficients</a></li>
<li class="chapter" data-level="15.7.2" data-path="regression.html"><a href="regression.html#calculating-standardised-regression-coefficients"><i class="fa fa-check"></i><b>15.7.2</b> Calculating standardised regression coefficients</a></li>
</ul></li>
<li class="chapter" data-level="15.8" data-path="regression.html"><a href="regression.html#regressionassumptions"><i class="fa fa-check"></i><b>15.8</b> Assumptions of regression</a></li>
<li class="chapter" data-level="15.9" data-path="regression.html"><a href="regression.html#regressiondiagnostics"><i class="fa fa-check"></i><b>15.9</b> Model checking</a><ul>
<li class="chapter" data-level="15.9.1" data-path="regression.html"><a href="regression.html#three-kinds-of-residuals"><i class="fa fa-check"></i><b>15.9.1</b> Three kinds of residuals</a></li>
<li class="chapter" data-level="15.9.2" data-path="regression.html"><a href="regression.html#regressionoutliers"><i class="fa fa-check"></i><b>15.9.2</b> Three kinds of anomalous data</a></li>
<li class="chapter" data-level="15.9.3" data-path="regression.html"><a href="regression.html#regressionnormality"><i class="fa fa-check"></i><b>15.9.3</b> Checking the normality of the residuals</a></li>
<li class="chapter" data-level="15.9.4" data-path="regression.html"><a href="regression.html#regressionlinearity"><i class="fa fa-check"></i><b>15.9.4</b> Checking the linearity of the relationship</a></li>
<li class="chapter" data-level="15.9.5" data-path="regression.html"><a href="regression.html#regressionhomogeneity"><i class="fa fa-check"></i><b>15.9.5</b> Checking the homogeneity of variance</a></li>
<li class="chapter" data-level="15.9.6" data-path="regression.html"><a href="regression.html#regressioncollinearity"><i class="fa fa-check"></i><b>15.9.6</b> Checking for collinearity</a></li>
</ul></li>
<li class="chapter" data-level="15.10" data-path="regression.html"><a href="regression.html#modelselreg"><i class="fa fa-check"></i><b>15.10</b> Model selection</a><ul>
<li class="chapter" data-level="15.10.1" data-path="regression.html"><a href="regression.html#backward-elimination"><i class="fa fa-check"></i><b>15.10.1</b> Backward elimination</a></li>
<li class="chapter" data-level="15.10.2" data-path="regression.html"><a href="regression.html#forward-selection"><i class="fa fa-check"></i><b>15.10.2</b> Forward selection</a></li>
<li class="chapter" data-level="15.10.3" data-path="regression.html"><a href="regression.html#a-caveat"><i class="fa fa-check"></i><b>15.10.3</b> A caveat</a></li>
<li class="chapter" data-level="15.10.4" data-path="regression.html"><a href="regression.html#comparing-two-regression-models"><i class="fa fa-check"></i><b>15.10.4</b> Comparing two regression models</a></li>
</ul></li>
<li class="chapter" data-level="15.11" data-path="regression.html"><a href="regression.html#summary-13"><i class="fa fa-check"></i><b>15.11</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="anova2.html"><a href="anova2.html"><i class="fa fa-check"></i><b>16</b> Factorial ANOVA</a><ul>
<li class="chapter" data-level="16.1" data-path="anova2.html"><a href="anova2.html#factorialanovasimple"><i class="fa fa-check"></i><b>16.1</b> Factorial ANOVA 1: balanced designs, no interactions</a><ul>
<li class="chapter" data-level="16.1.1" data-path="anova2.html"><a href="anova2.html#factanovahyp"><i class="fa fa-check"></i><b>16.1.1</b> What hypotheses are we testing?</a></li>
<li class="chapter" data-level="16.1.2" data-path="anova2.html"><a href="anova2.html#running-the-analysis-in-r"><i class="fa fa-check"></i><b>16.1.2</b> Running the analysis in R</a></li>
<li class="chapter" data-level="16.1.3" data-path="anova2.html"><a href="anova2.html#how-are-the-sum-of-squares-calculated"><i class="fa fa-check"></i><b>16.1.3</b> How are the sum of squares calculated?</a></li>
<li class="chapter" data-level="16.1.4" data-path="anova2.html"><a href="anova2.html#what-are-our-degrees-of-freedom"><i class="fa fa-check"></i><b>16.1.4</b> What are our degrees of freedom?</a></li>
<li class="chapter" data-level="16.1.5" data-path="anova2.html"><a href="anova2.html#factorial-anova-versus-one-way-anovas"><i class="fa fa-check"></i><b>16.1.5</b> Factorial ANOVA versus one-way ANOVAs</a></li>
<li class="chapter" data-level="16.1.6" data-path="anova2.html"><a href="anova2.html#what-kinds-of-outcomes-does-this-analysis-capture"><i class="fa fa-check"></i><b>16.1.6</b> What kinds of outcomes does this analysis capture?</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="anova2.html"><a href="anova2.html#interactions"><i class="fa fa-check"></i><b>16.2</b> Factorial ANOVA 2: balanced designs, interactions allowed</a><ul>
<li class="chapter" data-level="16.2.1" data-path="anova2.html"><a href="anova2.html#what-exactly-is-an-interaction-effect"><i class="fa fa-check"></i><b>16.2.1</b> What exactly <em>is an interaction effect?</em></a></li>
<li class="chapter" data-level="16.2.2" data-path="anova2.html"><a href="anova2.html#calculating-sums-of-squares-for-the-interaction"><i class="fa fa-check"></i><b>16.2.2</b> Calculating sums of squares for the interaction</a></li>
<li class="chapter" data-level="16.2.3" data-path="anova2.html"><a href="anova2.html#degrees-of-freedom-for-the-interaction"><i class="fa fa-check"></i><b>16.2.3</b> Degrees of freedom for the interaction</a></li>
<li class="chapter" data-level="16.2.4" data-path="anova2.html"><a href="anova2.html#running-the-anova-in-r"><i class="fa fa-check"></i><b>16.2.4</b> Running the ANOVA in R</a></li>
<li class="chapter" data-level="16.2.5" data-path="anova2.html"><a href="anova2.html#interpreting-the-results"><i class="fa fa-check"></i><b>16.2.5</b> Interpreting the results</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="anova2.html"><a href="anova2.html#effectsizefactorialanova"><i class="fa fa-check"></i><b>16.3</b> Effect size, estimated means, and confidence intervals</a><ul>
<li class="chapter" data-level="16.3.1" data-path="anova2.html"><a href="anova2.html#effect-sizes"><i class="fa fa-check"></i><b>16.3.1</b> Effect sizes</a></li>
<li class="chapter" data-level="16.3.2" data-path="anova2.html"><a href="anova2.html#estimated-group-means"><i class="fa fa-check"></i><b>16.3.2</b> Estimated group means</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="anova2.html"><a href="anova2.html#factorialanovaassumptions"><i class="fa fa-check"></i><b>16.4</b> Assumption checking</a><ul>
<li class="chapter" data-level="16.4.1" data-path="anova2.html"><a href="anova2.html#levene-test-for-homogeneity-of-variance"><i class="fa fa-check"></i><b>16.4.1</b> Levene test for homogeneity of variance</a></li>
<li class="chapter" data-level="16.4.2" data-path="anova2.html"><a href="anova2.html#normality-of-residuals"><i class="fa fa-check"></i><b>16.4.2</b> Normality of residuals</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="anova2.html"><a href="anova2.html#omnibusF"><i class="fa fa-check"></i><b>16.5</b> The <span class="math inline">\(F\)</span> test as a model comparison</a><ul>
<li class="chapter" data-level="16.5.1" data-path="anova2.html"><a href="anova2.html#the-f-test-comparing-two-models"><i class="fa fa-check"></i><b>16.5.1</b> The <span class="math inline">\(F\)</span> test comparing two models</a></li>
</ul></li>
<li class="chapter" data-level="16.6" data-path="anova2.html"><a href="anova2.html#anovalm"><i class="fa fa-check"></i><b>16.6</b> ANOVA as a linear model</a><ul>
<li class="chapter" data-level="16.6.1" data-path="anova2.html"><a href="anova2.html#some-data"><i class="fa fa-check"></i><b>16.6.1</b> Some data</a></li>
<li class="chapter" data-level="16.6.2" data-path="anova2.html"><a href="anova2.html#anova-with-binary-factors-as-a-regression-model"><i class="fa fa-check"></i><b>16.6.2</b> ANOVA with binary factors as a regression model</a></li>
<li class="chapter" data-level="16.6.3" data-path="anova2.html"><a href="anova2.html#changingbaseline"><i class="fa fa-check"></i><b>16.6.3</b> Changing the baseline category</a></li>
<li class="chapter" data-level="16.6.4" data-path="anova2.html"><a href="anova2.html#how-to-encode-non-binary-factors-as-contrasts"><i class="fa fa-check"></i><b>16.6.4</b> How to encode non binary factors as contrasts</a></li>
<li class="chapter" data-level="16.6.5" data-path="anova2.html"><a href="anova2.html#the-equivalence-between-anova-and-regression-for-non-binary-factors"><i class="fa fa-check"></i><b>16.6.5</b> The equivalence between ANOVA and regression for non-binary factors</a></li>
<li class="chapter" data-level="16.6.6" data-path="anova2.html"><a href="anova2.html#degrees-of-freedom-as-parameter-counting"><i class="fa fa-check"></i><b>16.6.6</b> Degrees of freedom as parameter counting!</a></li>
<li class="chapter" data-level="16.6.7" data-path="anova2.html"><a href="anova2.html#a-postscript"><i class="fa fa-check"></i><b>16.6.7</b> A postscript</a></li>
</ul></li>
<li class="chapter" data-level="16.7" data-path="anova2.html"><a href="anova2.html#contrasts"><i class="fa fa-check"></i><b>16.7</b> Different ways to specify contrasts</a><ul>
<li class="chapter" data-level="16.7.1" data-path="anova2.html"><a href="anova2.html#treatment-contrasts"><i class="fa fa-check"></i><b>16.7.1</b> Treatment contrasts</a></li>
<li class="chapter" data-level="16.7.2" data-path="anova2.html"><a href="anova2.html#helmert-contrasts"><i class="fa fa-check"></i><b>16.7.2</b> Helmert contrasts</a></li>
<li class="chapter" data-level="16.7.3" data-path="anova2.html"><a href="anova2.html#sum-to-zero-contrasts"><i class="fa fa-check"></i><b>16.7.3</b> Sum to zero contrasts</a></li>
<li class="chapter" data-level="16.7.4" data-path="anova2.html"><a href="anova2.html#viewing-and-setting-the-default-contrasts-in-r"><i class="fa fa-check"></i><b>16.7.4</b> Viewing and setting the default contrasts in R</a></li>
<li class="chapter" data-level="16.7.5" data-path="anova2.html"><a href="anova2.html#setting-the-contrasts-for-a-single-factor"><i class="fa fa-check"></i><b>16.7.5</b> Setting the contrasts for a single factor</a></li>
<li class="chapter" data-level="16.7.6" data-path="anova2.html"><a href="anova2.html#setting-the-contrasts-for-a-single-analysis"><i class="fa fa-check"></i><b>16.7.6</b> Setting the contrasts for a single analysis</a></li>
</ul></li>
<li class="chapter" data-level="16.8" data-path="anova2.html"><a href="anova2.html#posthoc2"><i class="fa fa-check"></i><b>16.8</b> Post hoc tests</a></li>
<li class="chapter" data-level="16.9" data-path="anova2.html"><a href="anova2.html#plannedcomparisons"><i class="fa fa-check"></i><b>16.9</b> The method of planned comparisons</a></li>
<li class="chapter" data-level="16.10" data-path="anova2.html"><a href="anova2.html#unbalancedanova"><i class="fa fa-check"></i><b>16.10</b> Factorial ANOVA 3: unbalanced designs</a><ul>
<li class="chapter" data-level="16.10.1" data-path="anova2.html"><a href="anova2.html#the-coffee-data"><i class="fa fa-check"></i><b>16.10.1</b> The coffee data</a></li>
<li class="chapter" data-level="16.10.2" data-path="anova2.html"><a href="anova2.html#standard-anova-does-not-exist-for-unbalanced-designs"><i class="fa fa-check"></i><b>16.10.2</b> “Standard ANOVA” does not exist for unbalanced designs</a></li>
<li class="chapter" data-level="16.10.3" data-path="anova2.html"><a href="anova2.html#type-i-sum-of-squares"><i class="fa fa-check"></i><b>16.10.3</b> Type I sum of squares</a></li>
<li class="chapter" data-level="16.10.4" data-path="anova2.html"><a href="anova2.html#type-iii-sum-of-squares"><i class="fa fa-check"></i><b>16.10.4</b> Type III sum of squares</a></li>
<li class="chapter" data-level="16.10.5" data-path="anova2.html"><a href="anova2.html#type-ii-sum-of-squares"><i class="fa fa-check"></i><b>16.10.5</b> Type II sum of squares</a></li>
<li class="chapter" data-level="16.10.6" data-path="anova2.html"><a href="anova2.html#effect-sizes-and-non-additive-sums-of-squares"><i class="fa fa-check"></i><b>16.10.6</b> Effect sizes (and non-additive sums of squares)</a></li>
</ul></li>
<li class="chapter" data-level="16.11" data-path="anova2.html"><a href="anova2.html#summary-14"><i class="fa fa-check"></i><b>16.11</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="part-vi-endings-alternatives-and-prospects.html"><a href="part-vi-endings-alternatives-and-prospects.html"><i class="fa fa-check"></i>Part VI. Endings, alternatives and prospects</a></li>
<li class="chapter" data-level="17" data-path="bayes.html"><a href="bayes.html"><i class="fa fa-check"></i><b>17</b> Bayesian statistics</a><ul>
<li class="chapter" data-level="17.1" data-path="bayes.html"><a href="bayes.html#basicbayes"><i class="fa fa-check"></i><b>17.1</b> Probabilistic reasoning by rational agents</a><ul>
<li class="chapter" data-level="17.1.1" data-path="bayes.html"><a href="bayes.html#priors-what-you-believed-before"><i class="fa fa-check"></i><b>17.1.1</b> Priors: what you believed before</a></li>
<li class="chapter" data-level="17.1.2" data-path="bayes.html"><a href="bayes.html#likelihoods-theories-about-the-data"><i class="fa fa-check"></i><b>17.1.2</b> Likelihoods: theories about the data</a></li>
<li class="chapter" data-level="17.1.3" data-path="bayes.html"><a href="bayes.html#the-joint-probability-of-data-and-hypothesis"><i class="fa fa-check"></i><b>17.1.3</b> The joint probability of data and hypothesis</a></li>
<li class="chapter" data-level="17.1.4" data-path="bayes.html"><a href="bayes.html#updating-beliefs-using-bayes-rule"><i class="fa fa-check"></i><b>17.1.4</b> Updating beliefs using Bayes’ rule</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="bayes.html"><a href="bayes.html#bayesianhypothesistests"><i class="fa fa-check"></i><b>17.2</b> Bayesian hypothesis tests</a><ul>
<li class="chapter" data-level="17.2.1" data-path="bayes.html"><a href="bayes.html#the-bayes-factor"><i class="fa fa-check"></i><b>17.2.1</b> The Bayes factor</a></li>
<li class="chapter" data-level="17.2.2" data-path="bayes.html"><a href="bayes.html#interpreting-bayes-factors"><i class="fa fa-check"></i><b>17.2.2</b> Interpreting Bayes factors</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="bayes.html"><a href="bayes.html#whybayes"><i class="fa fa-check"></i><b>17.3</b> Why be a Bayesian?</a><ul>
<li class="chapter" data-level="17.3.1" data-path="bayes.html"><a href="bayes.html#statistics-that-mean-what-you-think-they-mean"><i class="fa fa-check"></i><b>17.3.1</b> Statistics that mean what you think they mean</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="bayes.html"><a href="bayes.html#evidentiary-standards-you-can-believe"><i class="fa fa-check"></i><b>17.4</b> Evidentiary standards you can believe</a></li>
<li class="chapter" data-level="17.5" data-path="bayes.html"><a href="bayes.html#the-p-value-is-a-lie."><i class="fa fa-check"></i><b>17.5</b> The <span class="math inline">\(p\)</span>-value is a lie.</a><ul>
<li class="chapter" data-level="17.5.1" data-path="bayes.html"><a href="bayes.html#is-it-really-this-bad"><i class="fa fa-check"></i><b>17.5.1</b> Is it really this bad?</a></li>
</ul></li>
<li class="chapter" data-level="17.6" data-path="bayes.html"><a href="bayes.html#bayescontingency"><i class="fa fa-check"></i><b>17.6</b> Bayesian analysis of contingency tables</a><ul>
<li class="chapter" data-level="17.6.1" data-path="bayes.html"><a href="bayes.html#the-orthodox-text"><i class="fa fa-check"></i><b>17.6.1</b> The orthodox text</a></li>
<li class="chapter" data-level="17.6.2" data-path="bayes.html"><a href="bayes.html#the-bayesian-test"><i class="fa fa-check"></i><b>17.6.2</b> The Bayesian test</a></li>
<li class="chapter" data-level="17.6.3" data-path="bayes.html"><a href="bayes.html#writing-up-the-results"><i class="fa fa-check"></i><b>17.6.3</b> Writing up the results</a></li>
<li class="chapter" data-level="17.6.4" data-path="bayes.html"><a href="bayes.html#other-sampling-plans"><i class="fa fa-check"></i><b>17.6.4</b> Other sampling plans</a></li>
</ul></li>
<li class="chapter" data-level="17.7" data-path="bayes.html"><a href="bayes.html#ttestbf"><i class="fa fa-check"></i><b>17.7</b> Bayesian <span class="math inline">\(t\)</span>-tests</a><ul>
<li class="chapter" data-level="17.7.1" data-path="bayes.html"><a href="bayes.html#independent-samples-t-test"><i class="fa fa-check"></i><b>17.7.1</b> Independent samples <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="17.7.2" data-path="bayes.html"><a href="bayes.html#paired-samples-t-test"><i class="fa fa-check"></i><b>17.7.2</b> Paired samples <span class="math inline">\(t\)</span>-test</a></li>
</ul></li>
<li class="chapter" data-level="17.8" data-path="bayes.html"><a href="bayes.html#bayesregression"><i class="fa fa-check"></i><b>17.8</b> Bayesian regression</a><ul>
<li class="chapter" data-level="17.8.1" data-path="bayes.html"><a href="bayes.html#a-quick-refresher"><i class="fa fa-check"></i><b>17.8.1</b> A quick refresher</a></li>
<li class="chapter" data-level="17.8.2" data-path="bayes.html"><a href="bayes.html#the-bayesian-version"><i class="fa fa-check"></i><b>17.8.2</b> The Bayesian version</a></li>
<li class="chapter" data-level="17.8.3" data-path="bayes.html"><a href="bayes.html#finding-the-best-model"><i class="fa fa-check"></i><b>17.8.3</b> Finding the best model</a></li>
<li class="chapter" data-level="17.8.4" data-path="bayes.html"><a href="bayes.html#extracting-bayes-factors-for-all-included-terms"><i class="fa fa-check"></i><b>17.8.4</b> Extracting Bayes factors for all included terms</a></li>
</ul></li>
<li class="chapter" data-level="17.9" data-path="bayes.html"><a href="bayes.html#bayesanova"><i class="fa fa-check"></i><b>17.9</b> Bayesian ANOVA</a><ul>
<li class="chapter" data-level="17.9.1" data-path="bayes.html"><a href="bayes.html#a-quick-refresher-1"><i class="fa fa-check"></i><b>17.9.1</b> A quick refresher</a></li>
<li class="chapter" data-level="17.9.2" data-path="bayes.html"><a href="bayes.html#the-bayesian-version-1"><i class="fa fa-check"></i><b>17.9.2</b> The Bayesian version</a></li>
<li class="chapter" data-level="17.9.3" data-path="bayes.html"><a href="bayes.html#constructing-bayesian-type-ii-tests"><i class="fa fa-check"></i><b>17.9.3</b> Constructing Bayesian Type II tests</a></li>
</ul></li>
<li class="chapter" data-level="17.10" data-path="bayes.html"><a href="bayes.html#summary-15"><i class="fa fa-check"></i><b>17.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="epilogue.html"><a href="epilogue.html"><i class="fa fa-check"></i>Epilogue</a><ul>
<li class="chapter" data-level="" data-path="epilogue.html"><a href="epilogue.html#the-undiscovered-statistics"><i class="fa fa-check"></i>The undiscovered statistics</a><ul>
<li class="chapter" data-level="" data-path="epilogue.html"><a href="epilogue.html#omissions-within-the-topics-covered"><i class="fa fa-check"></i>Omissions within the topics covered</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="epilogue.html"><a href="epilogue.html#statistical-models-missing-from-the-book"><i class="fa fa-check"></i>Statistical models missing from the book</a></li>
<li class="chapter" data-level="" data-path="epilogue.html"><a href="epilogue.html#other-ways-of-doing-inference"><i class="fa fa-check"></i>Other ways of doing inference</a><ul>
<li class="chapter" data-level="" data-path="epilogue.html"><a href="epilogue.html#miscellaneous-topics"><i class="fa fa-check"></i>Miscellaneous topics</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="epilogue.html"><a href="epilogue.html#learning-the-basics-and-learning-them-in-r"><i class="fa fa-check"></i>Learning the basics, and learning them in R</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://learningstatisticswithr.com/book/" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Learning statistics with R: A tutorial for psychology students and other beginners. (Version 0.6.1)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayes" class="section level1">
<h1><span class="header-section-number">Chapter 17</span> Bayesian statistics</h1>
<blockquote>
<p><em>In our reasonings concerning matter of fact, there are all imaginable degrees of assurance, from the highest certainty to the lowest species of moral evidence. A wise man, therefore, proportions his belief to the evidence.</em> – David Hume<a href="#fn254" class="footnoteRef" id="fnref254"><sup>254</sup></a>.</p>
</blockquote>
<p>The ideas I’ve presented to you in this book describe inferential statistics from the frequentist perspective. I’m not alone in doing this. In fact, almost every textbook given to undergraduate psychology students presents the opinions of the frequentist statistician as <em>the</em> theory of inferential statistics, the one true way to do things. I have taught this way for practical reasons. The frequentist view of statistics dominated the academic field of statistics for most of the 20th century, and this dominance is even more extreme among applied scientists. It was and is current practice among psychologists to use frequentist methods. Because frequentist methods are ubiquitous in scientific papers, every student of statistics needs to understand those methods, otherwise they will be unable to make sense of what those papers are saying! Unfortunately – in my opinion at least – the current practice in psychology is often misguided, and the reliance on frequentist methods is partly to blame. In this chapter I explain why I think this, and provide an introduction to Bayesian statistics, an approach that I think is generally superior to the orthodox approach.</p>
<p>This chapter comes in two parts. In Sections <a href="bayes.html#basicbayes">17.1</a> through <a href="bayes.html#whybayes">17.3</a> I talk about what Bayesian statistics are all about, covering the basic mathematical rules for how it works as well as an explanation for why I think the Bayesian approach is so useful. Afterwards, I provide a brief overview of how you can do Bayesian versions of chi-square tests (Section <a href="bayes.html#bayescontingency">17.6</a>), <span class="math inline">\(t\)</span>-tests (Section <a href="bayes.html#ttestbf">17.7</a>), regression (Section <a href="bayes.html#bayesregression">17.8</a>) and ANOVA (Section <a href="bayes.html#bayesanova">17.9</a>).</p>
<div id="basicbayes" class="section level2">
<h2><span class="header-section-number">17.1</span> Probabilistic reasoning by rational agents</h2>
<p>From a Bayesian perspective, statistical inference is all about <em>belief revision</em>. I start out with a set of candidate hypotheses <span class="math inline">\(h\)</span> about the world. I don’t know which of these hypotheses is true, but do I have some beliefs about which hypotheses are plausible and which are not. When I observe the data <span class="math inline">\(d\)</span>, I have to revise those beliefs. If the data are consistent with a hypothesis, my belief in that hypothesis is strengthened. If the data inconsistent with the hypothesis, my belief in that hypothesis is weakened. That’s it! At the end of this section I’ll give a precise description of how Bayesian reasoning works, but first I want to work through a simple example in order to introduce the key ideas. Consider the following reasoning problem:</p>
<blockquote>
<p><em>I’m carrying an umbrella. Do you think it will rain?</em></p>
</blockquote>
<p>In this problem, I have presented you with a single piece of data (<span class="math inline">\(d =\)</span> I’m carrying the umbrella), and I’m asking you to tell me your beliefs about whether it’s raining. You have two possible <strong><em>hypotheses</em></strong>, <span class="math inline">\(h\)</span>: either it rains today or it does not. How should you solve this problem?</p>
<div id="priors-what-you-believed-before" class="section level3">
<h3><span class="header-section-number">17.1.1</span> Priors: what you believed before</h3>
<p>The first thing you need to do ignore what I told you about the umbrella, and write down your pre-existing beliefs about rain. This is important: if you want to be honest about how your beliefs have been revised in the light of new evidence, then you <em>must</em> say something about what you believed before those data appeared! So, what might you believe about whether it will rain today? You probably know that I live in Australia, and that much of Australia is hot and dry. And in fact you’re right: the city of Adelaide where I live has a Mediterranean climate, very similar to southern California, southern Europe or northern Africa. I’m writing this in January, and so you can assume it’s the middle of summer. In fact, you might have decided to take a quick look on Wikipedia<a href="#fn255" class="footnoteRef" id="fnref255"><sup>255</sup></a> and discovered that Adelaide gets an average of 4.4 days of rain across the 31 days of January. Without knowing anything else, you might conclude that the probability of January rain in Adelaide is about 15%, and the probability of a dry day is 85%. If this is really what you believe about Adelaide rainfall (and now that I’ve told it to you, I’m betting that this really <em>is</em> what you believe) then what I have written here is your <strong><em>prior distribution</em></strong>, written <span class="math inline">\(P(h)\)</span>:</p>
<table>
<thead>
<tr class="header">
<th align="left">Hypothesis</th>
<th align="left">Degree of Belief</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Rainy day</td>
<td align="left">0.15</td>
</tr>
<tr class="even">
<td align="left">Dry day</td>
<td align="left">0.85</td>
</tr>
</tbody>
</table>
</div>
<div id="likelihoods-theories-about-the-data" class="section level3">
<h3><span class="header-section-number">17.1.2</span> Likelihoods: theories about the data</h3>
<p>To solve the reasoning problem, you need a theory about my behaviour. When does Dan carry an umbrella? You might guess that I’m not a complete idiot,<a href="#fn256" class="footnoteRef" id="fnref256"><sup>256</sup></a> and I try to carry umbrellas only on rainy days. On the other hand, you also know that I have young kids, and you wouldn’t be all that surprised to know that I’m pretty forgetful about this sort of thing. Let’s suppose that on rainy days I remember my umbrella about 30% of the time (I really am awful at this). But let’s say that on dry days I’m only about 5% likely to be carrying an umbrella. So you might write out a little table like this:</p>
<table>
<thead>
<tr class="header">
<th align="left">Hypothesis</th>
<th align="left">Umbrella</th>
<th align="left">No umbrella</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Rainy day</td>
<td align="left">0.30</td>
<td align="left">0.70</td>
</tr>
<tr class="even">
<td align="left">Dry day</td>
<td align="left">0.05</td>
<td align="left">0.95</td>
</tr>
</tbody>
</table>
<p>It’s important to remember that each cell in this table describes your beliefs about what data <span class="math inline">\(d\)</span> will be observed, <em>given</em> the truth of a particular hypothesis <span class="math inline">\(h\)</span>. This “conditional probability” is written <span class="math inline">\(P(d|h)\)</span>, which you can read as “the probability of <span class="math inline">\(d\)</span> given <span class="math inline">\(h\)</span>”. In Bayesian statistics, this is referred to as <strong><em>likelihood</em></strong> of data <span class="math inline">\(d\)</span> given hypothesis <span class="math inline">\(h\)</span>.<a href="#fn257" class="footnoteRef" id="fnref257"><sup>257</sup></a></p>
</div>
<div id="the-joint-probability-of-data-and-hypothesis" class="section level3">
<h3><span class="header-section-number">17.1.3</span> The joint probability of data and hypothesis</h3>
<p>At this point, all the elements are in place. Having written down the priors and the likelihood, you have all the information you need to do Bayesian reasoning. The question now becomes, <em>how</em> do we use this information? As it turns out, there’s a very simple equation that we can use here, but it’s important that you understand why we use it, so I’m going to try to build it up from more basic ideas.</p>
<p>Let’s start out with one of the rules of probability theory. I listed it way back in Table <a href="probability.html#tab:probrules">9.1</a>, but I didn’t make a big deal out of it at the time and you probably ignored it. The rule in question is the one that talks about the probability that <em>two</em> things are true. In our example, you might want to calculate the probability that today is rainy (i.e., hypothesis <span class="math inline">\(h\)</span> is true) <em>and</em> I’m carrying an umbrella (i.e., data <span class="math inline">\(d\)</span> is observed). The <strong><em>joint probability</em></strong> of the hypothesis and the data is written <span class="math inline">\(P(d,h)\)</span>, and you can calculate it by multiplying the prior <span class="math inline">\(P(h)\)</span> by the likelihood <span class="math inline">\(P(d|h)\)</span>. Mathematically, we say that: <span class="math display">\[
P(d,h) = P(d|h) P(h)
\]</span></p>
<p>So, what is the probability that today is a rainy day <em>and</em> I remember to carry an umbrella? As we discussed earlier, the prior tells us that the probability of a rainy day is 15%, and the likelihood tells us that the probability of me remembering my umbrella on a rainy day is 30%. So the probability that both of these things are true is calculated by multiplying the two:</p>
<p><span class="math display">\[
\begin{array}
P(\mbox{rainy}, \mbox{umbrella}) &amp; = &amp; P(\mbox{umbrella} | \mbox{rainy}) \times P(\mbox{rainy}) \\
&amp; = &amp; 0.30 \times 0.15 \\
&amp; = &amp; 0.045
\end{array}
\]</span></p>
<p>In other words, before being told anything about what actually happened, you think that there is a 4.5% probability that today will be a rainy day and that I will remember an umbrella. However, there are of course <em>four</em> possible things that could happen, right? So let’s repeat the exercise for all four. If we do that, we end up with the following table:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="left">Umbrella</th>
<th align="left">No-umbrella</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Rainy</td>
<td align="left">0.045</td>
<td align="left">0.105</td>
</tr>
<tr class="even">
<td>Dry</td>
<td align="left">0.0425</td>
<td align="left">0.8075</td>
</tr>
</tbody>
</table>
<p>This table captures all the information about which of the four possibilities are likely. To really get the full picture, though, it helps to add the row totals and column totals. That gives us this table:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="left">Umbrella</th>
<th align="left">No-umbrella</th>
<th align="left">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Rainy</td>
<td align="left">0.0450</td>
<td align="left">0.1050</td>
<td align="left">0.15</td>
</tr>
<tr class="even">
<td>Dry</td>
<td align="left">0.0425</td>
<td align="left">0.8075</td>
<td align="left">0.85</td>
</tr>
<tr class="odd">
<td>Total</td>
<td align="left">0.0875</td>
<td align="left">0.9125</td>
<td align="left">1</td>
</tr>
</tbody>
</table>
<p>This is a very useful table, so it’s worth taking a moment to think about what all these numbers are telling us. First, notice that the row sums aren’t telling us anything new at all. For example, the first row tells us that if we ignore all this umbrella business, the chance that today will be a rainy day is 15%. That’s not surprising, of course: that’s our prior. The important thing isn’t the number itself: rather, the important thing is that it gives us some confidence that our calculations are sensible! Now take a look at the column sums, and notice that they tell us something that we haven’t explicitly stated yet. In the same way that the row sums tell us the probability of rain, the column sums tell us the probability of me carrying an umbrella. Specifically, the first column tells us that on average (i.e., ignoring whether it’s a rainy day or not), the probability of me carrying an umbrella is 8.75%. Finally, notice that when we sum across all four logically-possible events, everything adds up to 1. In other words, what we have written down is a proper probability distribution defined over all possible combinations of data and hypothesis.</p>
<p>Now, because this table is so useful, I want to make sure you understand what all the elements correspond to, and how they written:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="left">Umbrella</th>
<th align="left">No-umbrella</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Rainy</td>
<td align="left"><span class="math inline">\(P\)</span>(Umbrella, Rainy)</td>
<td align="left"><span class="math inline">\(P\)</span>(No-umbrella, Rainy)</td>
<td><span class="math inline">\(P\)</span>(Rainy)</td>
</tr>
<tr class="even">
<td>Dry</td>
<td align="left"><span class="math inline">\(P\)</span>(Umbrella, Dry)</td>
<td align="left"><span class="math inline">\(P\)</span>(No-umbrella, Dry)</td>
<td><span class="math inline">\(P\)</span>(Dry)</td>
</tr>
<tr class="odd">
<td></td>
<td align="left"><span class="math inline">\(P\)</span>(Umbrella)</td>
<td align="left"><span class="math inline">\(P\)</span>(No-umbrella)</td>
<td></td>
</tr>
</tbody>
</table>
<p>Finally, let’s use “proper” statistical notation. In the rainy day problem, the data corresponds to the observation that I do or do not have an umbrella. So we’ll let <span class="math inline">\(d_1\)</span> refer to the possibility that you observe me carrying an umbrella, and <span class="math inline">\(d_2\)</span> refers to you observing me not carrying one. Similarly, <span class="math inline">\(h_1\)</span> is your hypothesis that today is rainy, and <span class="math inline">\(h_2\)</span> is the hypothesis that it is not. Using this notation, the table looks like this:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="left"><span class="math inline">\(d_1\)</span></th>
<th align="left"><span class="math inline">\(d_2\)</span></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(h_1\)</span></td>
<td align="left"><span class="math inline">\(P(h_1, d_1)\)</span></td>
<td align="left"><span class="math inline">\(P(h_1, d_2)\)</span></td>
<td><span class="math inline">\(P(h_1)\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(h_2\)</span></td>
<td align="left"><span class="math inline">\(P(h_2, d_1)\)</span></td>
<td align="left"><span class="math inline">\(P(h_2, d_2)\)</span></td>
<td><span class="math inline">\(P(h_2)\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td align="left"><span class="math inline">\(P(d_1)\)</span></td>
<td align="left"><span class="math inline">\(P(d_2)\)</span></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<div id="updating-beliefs-using-bayes-rule" class="section level3">
<h3><span class="header-section-number">17.1.4</span> Updating beliefs using Bayes’ rule</h3>
<p>The table we laid out in the last section is a very powerful tool for solving the rainy day problem, because it considers all four logical possibilities and states exactly how confident you are in each of them before being given any data. It’s now time to consider what happens to our beliefs when we are actually given the data. In the rainy day problem, you are told that I really <em>am</em> carrying an umbrella. This is something of a surprising event: according to our table, the probability of me carrying an umbrella is only 8.75%. But that makes sense, right? A guy carrying an umbrella on a summer day in a hot dry city is pretty unusual, and so you really weren’t expecting that. Nevertheless, the problem tells you that it is true. No matter how unlikely you thought it was, you must now adjust your beliefs to accommodate the fact that you now <em>know</em> that I have an umbrella.<a href="#fn258" class="footnoteRef" id="fnref258"><sup>258</sup></a> To reflect this new knowledge, our <em>revised</em> table must have the following numbers:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="left">Umbrella</th>
<th align="left">No-umbrella</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Rainy</td>
<td align="left"></td>
<td align="left">0</td>
</tr>
<tr class="even">
<td>Dry</td>
<td align="left"></td>
<td align="left">0</td>
</tr>
<tr class="odd">
<td>Total</td>
<td align="left">1</td>
<td align="left">0</td>
</tr>
</tbody>
</table>
<p>In other words, the facts have eliminated any possibility of “no umbrella”, so we have to put zeros into any cell in the table that implies that I’m not carrying an umbrella. Also, you know for a fact that I am carrying an umbrella, so the column sum on the left must be 1 to correctly describe the fact that <span class="math inline">\(P(\mbox{umbrella})=1\)</span>.</p>
<p>What two numbers should we put in the empty cells? Again, let’s not worry about the maths, and instead think about our intuitions. When we wrote out our table the first time, it turned out that those two cells had almost identical numbers, right? We worked out that the joint probability of “rain and umbrella” was 4.5%, and the joint probability of “dry and umbrella” was 4.25%. In other words, before I told you that I am in fact carrying an umbrella, you’d have said that these two events were almost identical in probability, yes? But notice that <em>both</em> of these possibilities are consistent with the fact that I actually am carrying an umbrella. From the perspective of these two possibilities, very little has changed. I hope you’d agree that it’s <em>still</em> true that these two possibilities are equally plausible. So what we expect to see in our final table is some numbers that preserve the fact that “rain and umbrella” is <em>slightly</em> more plausible than “dry and umbrella”, while still ensuring that numbers in the table add up. Something like this, perhaps?</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="left">Umbrella</th>
<th align="left">No-umbrella</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Rainy</td>
<td align="left">0.514</td>
<td align="left">0</td>
</tr>
<tr class="even">
<td>Dry</td>
<td align="left">0.486</td>
<td align="left">0</td>
</tr>
<tr class="odd">
<td>Total</td>
<td align="left">1</td>
<td align="left">0</td>
</tr>
</tbody>
</table>
<p>What this table is telling you is that, after being told that I’m carrying an umbrella, you believe that there’s a 51.4% chance that today will be a rainy day, and a 48.6% chance that it won’t. That’s the answer to our problem! The <strong><em>posterior probability</em></strong> of rain <span class="math inline">\(P(h|d)\)</span> given that I am carrying an umbrella is 51.4%</p>
<p>How did I calculate these numbers? You can probably guess. To work out that there was a 0.514 probability of “rain”, all I did was take the 0.045 probability of “rain and umbrella” and divide it by the 0.0875 chance of “umbrella”. This produces a table that satisfies our need to have everything sum to 1, and our need not to interfere with the relative plausibility of the two events that are actually consistent with the data. To say the same thing using fancy statistical jargon, what I’ve done here is divide the joint probability of the hypothesis and the data <span class="math inline">\(P(d,h)\)</span> by the <strong><em>marginal probability</em></strong> of the data <span class="math inline">\(P(d)\)</span>, and this is what gives us the posterior probability of the hypothesis <em>given</em> that we know the data have been observed. To write this as an equation:<a href="#fn259" class="footnoteRef" id="fnref259"><sup>259</sup></a> <span class="math display">\[
P(h | d) = \frac{P(d,h)}{P(d)}
\]</span></p>
<p>However, remember what I said at the start of the last section, namely that the joint probability <span class="math inline">\(P(d,h)\)</span> is calculated by multiplying the prior <span class="math inline">\(P(h)\)</span> by the likelihood <span class="math inline">\(P(d|h)\)</span>. In real life, the things we actually know how to write down are the priors and the likelihood, so let’s substitute those back into the equation. This gives us the following formula for the posterior probability:</p>
<p><span class="math display">\[
P(h | d) = \frac{P(d|h) P(h)}{P(d)}
\]</span></p>
<p>And this formula, folks, is known as <strong><em>Bayes’ rule</em></strong>. It describes how a learner starts out with prior beliefs about the plausibility of different hypotheses, and tells you how those beliefs should be revised in the face of data. In the Bayesian paradigm, all statistical inference flows from this one simple rule.</p>
</div>
</div>
<div id="bayesianhypothesistests" class="section level2">
<h2><span class="header-section-number">17.2</span> Bayesian hypothesis tests</h2>
<p>In Chapter <a href="hypothesistesting.html#hypothesistesting">11</a> I described the orthodox approach to hypothesis testing. It took an entire chapter to describe, because null hypothesis testing is a very elaborate contraption that people find very hard to make sense of. In contrast, the Bayesian approach to hypothesis testing is incredibly simple. Let’s pick a setting that is closely analogous to the orthodox scenario. There are two hypotheses that we want to compare, a null hypothesis <span class="math inline">\(h_0\)</span> and an alternative hypothesis <span class="math inline">\(h_1\)</span>. Prior to running the experiment we have some beliefs <span class="math inline">\(P(h)\)</span> about which hypotheses are true. We run an experiment and obtain data <span class="math inline">\(d\)</span>. Unlike frequentist statistics Bayesian statistics does allow to talk about the probability that the null hypothesis is true. Better yet, it allows us to calculate the <strong><em>posterior probability of the null hypothesis</em></strong>, using Bayes’ rule:</p>
<p><span class="math display">\[
P(h_0 | d) = \frac{P(d|h_0) P(h_0)}{P(d)}
\]</span></p>
<p>This formula tells us exactly how much belief we should have in the null hypothesis after having observed the data <span class="math inline">\(d\)</span>. Similarly, we can work out how much belief to place in the alternative hypothesis using essentially the same equation. All we do is change the subscript:</p>
<p><span class="math display">\[
P(h_1 | d) = \frac{P(d|h_1) P(h_1)}{P(d)}
\]</span></p>
<p>It’s all so simple that I feel like an idiot even bothering to write these equations down, since all I’m doing is copying Bayes rule from the previous section.<a href="#fn260" class="footnoteRef" id="fnref260"><sup>260</sup></a></p>
<div id="the-bayes-factor" class="section level3">
<h3><span class="header-section-number">17.2.1</span> The Bayes factor</h3>
<p>In practice, most Bayesian data analysts tend not to talk in terms of the raw posterior probabilities <span class="math inline">\(P(h_0|d)\)</span> and <span class="math inline">\(P(h_1|d)\)</span>. Instead, we tend to talk in terms of the <strong><em>posterior odds</em></strong> ratio. Think of it like betting. Suppose, for instance, the posterior probability of the null hypothesis is 25%, and the posterior probability of the alternative is 75%. The alternative hypothesis is three times as probable as the null, so we say that the <em>odds</em> are 3:1 in favour of the alternative. Mathematically, all we have to do to calculate the posterior odds is divide one posterior probability by the other:</p>
<p><span class="math display">\[
\frac{P(h_1 | d)}{P(h_0 | d)} = \frac{0.75}{0.25} = 3
\]</span></p>
<p>Or, to write the same thing in terms of the equations above: <span class="math display">\[
\frac{P(h_1 | d)}{P(h_0 | d)} = \frac{P(d|h_1)}{P(d|h_0)} \times \frac{P(h_1)}{P(h_0)}
\]</span></p>
<p>Actually, this equation is worth expanding on. There are three different terms here that you should know. On the left hand side, we have the posterior odds, which tells you what you believe about the relative plausibilty of the null hypothesis and the alternative hypothesis <em>after</em> seeing the data. On the right hand side, we have the <strong><em>prior odds</em></strong>, which indicates what you thought <em>before</em> seeing the data. In the middle, we have the <strong><em>Bayes factor</em></strong>, which describes the amount of evidence provided by the data: <span class="math display">\[
\begin{array}{ccccc}\displaystyle
\frac{P(h_1 | d)}{P(h_0 | d)} &amp;=&amp; \displaystyle\frac{P(d|h_1)}{P(d|h_0)} &amp;\times&amp; \displaystyle\frac{P(h_1)}{P(h_0)} \\[6pt] \\[-2pt]
\uparrow &amp;&amp; \uparrow &amp;&amp; \uparrow \\[6pt]
\mbox{Posterior odds} &amp;&amp; \mbox{Bayes factor} &amp;&amp; \mbox{Prior odds}
\end{array}
\]</span> The Bayes factor (sometimes abbreviated as <strong><em>BF</em></strong>) has a special place in the Bayesian hypothesis testing, because it serves a similar role to the <span class="math inline">\(p\)</span>-value in orthodox hypothesis testing: it quantifies the strength of evidence provided by the data, and as such it is the Bayes factor that people tend to report when running a Bayesian hypothesis test. The reason for reporting Bayes factors rather than posterior odds is that different researchers will have different priors. Some people might have a strong bias to believe the null hypothesis is true, others might have a strong bias to believe it is false. Because of this, the polite thing for an applied researcher to do is report the Bayes factor. That way, anyone reading the paper can multiply the Bayes factor by their own <em>personal</em> prior odds, and they can work out for themselves what the posterior odds would be. In any case, by convention we like to pretend that we give equal consideration to both the null hypothesis and the alternative, in which case the prior odds equals 1, and the posterior odds becomes the same as the Bayes factor.</p>
</div>
<div id="interpreting-bayes-factors" class="section level3">
<h3><span class="header-section-number">17.2.2</span> Interpreting Bayes factors</h3>
<p>One of the really nice things about the Bayes factor is the numbers are inherently meaningful. If you run an experiment and you compute a Bayes factor of 4, it means that the evidence provided by your data corresponds to betting odds of 4:1 in favour of the alternative. However, there have been some attempts to quantify the standards of evidence that would be considered meaningful in a scientific context. The two most widely used are from <span class="citation">Jeffreys (<a href="#ref-Jeffreys1961">1961</a>)</span> and <span class="citation">Kass and Raftery (<a href="#ref-Kass1995">1995</a>)</span>. Of the two, I tend to prefer the <span class="citation">Kass and Raftery (<a href="#ref-Kass1995">1995</a>)</span> table because it’s a bit more conservative. So here it is:</p>
<table>
<thead>
<tr class="header">
<th align="left">Bayes factor</th>
<th align="left">Interpretation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1 - 3</td>
<td align="left">Negligible evidence</td>
</tr>
<tr class="even">
<td align="left">3 - 20</td>
<td align="left">Positive evidence</td>
</tr>
<tr class="odd">
<td align="left">20 - 150</td>
<td align="left">Strong evidence</td>
</tr>
<tr class="even">
<td align="left">$&gt;$150</td>
<td align="left">Very strong evidence</td>
</tr>
</tbody>
</table>
<p>And to be perfectly honest, I think that even the Kass and Raftery standards are being a bit charitable. If it were up to me, I’d have called the “positive evidence” category “weak evidence”. To me, anything in the range 3:1 to 20:1 is “weak” or “modest” evidence at best. But there are no hard and fast rules here: what counts as strong or weak evidence depends entirely on how conservative you are, and upon the standards that your community insists upon before it is willing to label a finding as “true”.</p>
<p>In any case, note that all the numbers listed above make sense if the Bayes factor is greater than 1 (i.e., the evidence favours the alternative hypothesis). However, one big practical advantage of the Bayesian approach relative to the orthodox approach is that it also allows you to quantify evidence <em>for</em> the null. When that happens, the Bayes factor will be less than 1. You can choose to report a Bayes factor less than 1, but to be honest I find it confusing. For example, suppose that the likelihood of the data under the null hypothesis <span class="math inline">\(P(d|h_0)\)</span> is equal to 0.2, and the corresponding likelihood <span class="math inline">\(P(d|h_0)\)</span> under the alternative hypothesis is 0.1. Using the equations given above, Bayes factor here would be:</p>
<p><span class="math display">\[ 
\mbox{BF} = \frac{P(d|h_1)}{P(d|h_0)} = \frac{0.1}{0.2} = 0.5
\]</span></p>
<p>Read literally, this result tells is that the evidence in favour of the alternative is 0.5 to 1. I find this hard to understand. To me, it makes a lot more sense to turn the equation “upside down”, and report the amount op evidence in favour of the <em>null</em>. In other words, what we calculate is this:</p>
<p><span class="math display">\[ 
\mbox{BF}^\prime = \frac{P(d|h_0)}{P(d|h_1)} = \frac{0.2}{0.1} = 2
\]</span></p>
<p>And what we would report is a Bayes factor of 2:1 in favour of the null. Much easier to understand, and you can interpret this using the table above.</p>
</div>
</div>
<div id="whybayes" class="section level2">
<h2><span class="header-section-number">17.3</span> Why be a Bayesian?</h2>
<p>Up to this point I’ve focused exclusively on the logic underpinning Bayesian statistics. We’ve talked about the idea of “probability as a degree of belief”, and what it implies about how a rational agent should reason about the world. The question that you have to answer for yourself is this: how do <em>you</em> want to do your statistics? Do you want to be an orthodox statistician, relying on sampling distributions and <span class="math inline">\(p\)</span>-values to guide your decisions? Or do you want to be a Bayesian, relying on Bayes factors and the rules for rational belief revision? And to be perfectly honest, I can’t answer this question for you. Ultimately it depends on what you think is right. It’s your call, and your call alone. That being said, I can talk a little about why <em>I</em> prefer the Bayesian approach.</p>
<div id="statistics-that-mean-what-you-think-they-mean" class="section level3">
<h3><span class="header-section-number">17.3.1</span> Statistics that mean what you think they mean</h3>
<blockquote>
<p><em>You keep using that word. I do not think it means what you think it means</em><br />
– Inigo Montoya, The Princess Bride<a href="#fn261" class="footnoteRef" id="fnref261"><sup>261</sup></a></p>
</blockquote>
<p>To me, one of the biggest advantages to the Bayesian approach is that it answers the right questions. Within the Bayesian framework, it is perfectly sensible and allowable to refer to “the probability that a hypothesis is true”. You can even try to calculate this probability. Ultimately, isn’t that what you <em>want</em> your statistical tests to tell you? To an actual human being, this would seem to be the whole <em>point</em> of doing statistics: to determine what is true and what isn’t. Any time that you aren’t exactly sure about what the truth is, you should use the language of probability theory to say things like “there is an 80% chance that Theory A is true, but a 20% chance that Theory B is true instead”.</p>
<p>This seems so obvious to a human, yet it is explicitly forbidden within the orthodox framework. To a frequentist, such statements are a nonsense because “the theory is true” is not a repeatable event. A theory is true or it is not, and no probabilistic statements are allowed, no matter how much you might want to make them. There’s a reason why, back in Section <a href="hypothesistesting.html#pvalue">11.5</a>, I repeatedly warned you <em>not</em> to interpret the <span class="math inline">\(p\)</span>-value as the probability of that the null hypothesis is true. There’s a reason why almost every textbook on statstics is forced to repeat that warning. It’s because people desperately <em>want</em> that to be the correct interpretation. Frequentist dogma notwithstanding, a lifetime of experience of teaching undergraduates and of doing data analysis on a daily basis suggests to me that most actual humans thing that “the probability that the hypothesis is true” is not only meaningful, it’s the thing we care <em>most</em> about. It’s such an appealing idea that even trained statisticians fall prey to the mistake of trying to interpret a <span class="math inline">\(p\)</span>-value this way. For example, here is a quote from an official Newspoll report in 2013, explaining how to interpret their (frequentist) data analysis:<a href="#fn262" class="footnoteRef" id="fnref262"><sup>262</sup></a></p>
<blockquote>
<p>Throughout the report, where relevant, statistically significant changes have been noted. All significance tests have been based on the 95 percent level of confidence. <strong>This means that if a change is noted as being statistically significant, there is a 95 percent probability that a real change has occurred</strong>, and is not simply due to chance variation. (emphasis added)</p>
</blockquote>
<p>Nope! That’s <em>not</em> what <span class="math inline">\(p&lt;.05\)</span> means. That’s <em>not</em> what 95% confidence means to a frequentist statistician. The bolded section is just plain wrong. Orthodox methods cannot tell you that “there is a 95% chance that a real change has occurred”, because this is not the kind of event to which frequentist probabilities may be assigned. To an ideological frequentist, this sentence should be meaningless. Even if you’re a more pragmatic frequentist, it’s still the wrong definition of a <span class="math inline">\(p\)</span>-value. It is simply not an allowed or correct thing to say if you want to rely on orthodox statistical tools.</p>
<p>On the other hand, let’s suppose you are a Bayesian. Although the bolded passage is the wrong definition of a <span class="math inline">\(p\)</span>-value, it’s pretty much exactly what a Bayesian means when they say that the posterior probability of the alternative hypothesis is greater than 95%. And here’s the thing. If the Bayesian posterior is actually thing you <em>want</em> to report, why are you even trying to use orthodox methods? If you want to make Bayesian claims, all you have to do is be a Bayesian and use Bayesian tools.</p>
<p>Speaking for myself, I found this to be a the most liberating thing about switching to the Bayesian view. Once you’ve made the jump, you no longer have to wrap your head around counterinuitive definitions of <span class="math inline">\(p\)</span>-values. You don’t have to bother remembering why you can’t say that you’re 95% confident that the true mean lies within some interval. All you have to do is be honest about what you believed before you ran the study, and then report what you learned from doing it. Sounds nice, doesn’t it? To me, this is the big promise of the Bayesian approach: you do the analysis you really want to do, and express what you really believe the data are telling you.</p>
</div>
</div>
<div id="evidentiary-standards-you-can-believe" class="section level2">
<h2><span class="header-section-number">17.4</span> Evidentiary standards you can believe</h2>
<blockquote>
<p><em>If [<span class="math inline">\(p\)</span>] is below .02 it is strongly indicated that the [null] hypothesis fails to account for the whole of the facts. We shall not often be astray if we draw a conventional line at .05 and consider that [smaller values of <span class="math inline">\(p\)</span>] indicate a real discrepancy.</em><br />
– Sir Ronald <span class="citation">Fisher (<a href="#ref-Fisher1925">1925</a>)</span></p>
</blockquote>
<p>Consider the quote above by Sir Ronald Fisher, one of the founders of what has become the orthodox approach to statistics. If anyone has ever been entitled to express an opinion about the intended function of <span class="math inline">\(p\)</span>-values, it’s Fisher. In this passage, taken from his classic guide <em>Statistical Methods for Research Workers</em>, he’s pretty clear about what it means to reject a null hypothesis at <span class="math inline">\(p&lt;.05\)</span>. In his opinion, if we take <span class="math inline">\(p&lt;.05\)</span> to mean there is “a real effect”, then “we shall not often be astray”. This view is hardly unusual: in my experience, most practitioners express views very similar to Fisher’s. In essence, the <span class="math inline">\(p&lt;.05\)</span> convention is assumed to represent a fairly stringent evidentiary standard.</p>
<p>Well, how true is that? One way to approach this question is to try to convert <span class="math inline">\(p\)</span>-values to Bayes factors, and see how the two compare. It’s not an easy thing to do because a <span class="math inline">\(p\)</span>-value is a fundamentally different kind of calculation to a Bayes factor, and they don’t measure the same thing. However, there have been some attempts to work out the relationship between the two, and it’s somewhat surprising. For example, <span class="citation">Johnson (<a href="#ref-Johnson2013">2013</a>)</span> presents a pretty compelling case that (for <span class="math inline">\(t\)</span>-tests at least) the <span class="math inline">\(p&lt;.05\)</span> threshold corresponds roughly to a Bayes factor of somewhere between 3:1 and 5:1 in favour of the alternative. If that’s right, then Fisher’s claim is a bit of a stretch. Let’s suppose that the null hypothesis is true about half the time (i.e., the prior probability of <span class="math inline">\(H_0\)</span> is 0.5), and we use those numbers to work out the posterior probability of the null hypothesis given that it has been rejected at <span class="math inline">\(p&lt;.05\)</span>. Using the data from <span class="citation">Johnson (<a href="#ref-Johnson2013">2013</a>)</span>, we see that if you reject the null at <span class="math inline">\(p&lt;.05\)</span>, you’ll be correct about 80% of the time. I don’t know about you, but in my opinion an evidentiary standard that ensures you’ll be wrong on 20% of your decisions isn’t good enough. The fact remains that, quite contrary to Fisher’s claim, if you reject at <span class="math inline">\(p&lt;.05\)</span> you shall quite often go astray. It’s not a very stringent evidentiary threshold at all.</p>
</div>
<div id="the-p-value-is-a-lie." class="section level2">
<h2><span class="header-section-number">17.5</span> The <span class="math inline">\(p\)</span>-value is a lie.</h2>
<blockquote>
<p><em>The cake is a lie.</em><br />
<em>The cake is a lie.</em><br />
<em>The cake is a lie.</em><br />
<em>The cake is a lie.</em><br />
– Portal<a href="#fn263" class="footnoteRef" id="fnref263"><sup>263</sup></a></p>
</blockquote>
<p>Okay, at this point you might be thinking that the real problem is not with orthodox statistics, just the <span class="math inline">\(p&lt;.05\)</span> standard. In one sense, that’s true. The recommendation that <span class="citation">Johnson (<a href="#ref-Johnson2013">2013</a>)</span> gives is not that “everyone must be a Bayesian now”. Instead, the suggestion is that it would be wiser to shift the conventional standard to something like a <span class="math inline">\(p&lt;.01\)</span> level. That’s not an unreasonable view to take, but in my view the problem is a little more severe than that. In my opinion, there’s a fairly big problem built into the way most (but not all) orthodox hypothesis tests are constructed. They are grossly naive about how humans actually do research, and because of this most <span class="math inline">\(p\)</span>-values are wrong.</p>
<p>Sounds like an absurd claim, right? Well, consider the following scenario. You’ve come up with a really exciting research hypothesis and you design a study to test it. You’re very diligent, so you run a power analysis to work out what your sample size should be, and you run the study. You run your hypothesis test and out pops a <span class="math inline">\(p\)</span>-value of 0.072. Really bloody annoying, right?</p>
<p>What should you do? Here are some possibilities:</p>
<ol style="list-style-type: decimal">
<li>You conclude that there is no effect, and try to publish it as a null result</li>
<li>You guess that there might be an effect, and try to publish it as a “borderline significant” result</li>
<li>You give up and try a new study</li>
<li>You collect some more data to see if the <span class="math inline">\(p\)</span> value goes up or (preferably!) drops below the “magic” criterion of <span class="math inline">\(p&lt;.05\)</span></li>
</ol>
<p>Which would <em>you</em> choose? Before reading any further, I urge you to take some time to think about it. Be honest with yourself. But don’t stress about it too much, because you’re screwed no matter what you choose. Based on my own experiences as an author, reviewer and editor, as well as stories I’ve heard from others, here’s what will happen in each case:</p>
<ul>
<li><p>Let’s start with option 1. If you try to publish it as a null result, the paper will struggle to be published. Some reviewers will think that <span class="math inline">\(p=.072\)</span> is not really a null result. They’ll argue it’s borderline significant. Other reviewers will agree it’s a null result, but will claim that even though some null results <em>are</em> publishable, yours isn’t. One or two reviewers might even be on your side, but you’ll be fighting an uphill battle to get it through.</p></li>
<li><p>Okay, let’s think about option number 2. Suppose you try to publish it as a borderline significant result. Some reviewers will claim that it’s a null result and should not be published. Others will claim that the evidence is ambiguous, and that you should collect more data until you get a clear significant result. Again, the publication process does not favour you.</p></li>
<li><p>Given the difficulties in publishing an “ambiguous” result like <span class="math inline">\(p=.072\)</span>, option number 3 might seem tempting: give up and do something else. But that’s a recipe for career suicide. If you give up and try a new project else every time you find yourself faced with ambiguity, your work will never be published. And if you’re in academia without a publication record you can lose your job. So that option is out.</p></li>
<li><p>It looks like you’re stuck with option 4. You don’t have conclusive results, so you decide to collect some more data and re-run the analysis. Seems sensible, but unfortunately for you, if you do this all of your <span class="math inline">\(p\)</span>-values are now incorrect. <em>All</em> of them. Not just the <span class="math inline">\(p\)</span>-values that you calculated for <em>this</em> study. All of them. All the <span class="math inline">\(p\)</span>-values you calculated in the past and all the <span class="math inline">\(p\)</span>-values you will calculate in the future. Fortunately, no-one will notice. You’ll get published, and you’ll have lied.</p></li>
</ul>
<p>Wait, what? How can that last part be true? I mean, it sounds like a perfectly reasonable strategy doesn’t it? You collected some data, the results weren’t conclusive, so now what you want to do is collect more data until the the results <em>are</em> conclusive. What’s wrong with that?</p>
<p>Honestly, there’s nothing wrong with it. It’s a reasonable, sensible and rational thing to do. In real life, this is exactly what every researcher does. Unfortunately, the theory of null hypothesis testing as I described it in Chapter <a href="hypothesistesting.html#hypothesistesting">11</a> <em>forbids</em> you from doing this.<a href="#fn264" class="footnoteRef" id="fnref264"><sup>264</sup></a> The reason is that the theory assumes that the experiment is finished and all the data are in. And because it assumes the experiment is over, it only considers <em>two</em> possible decisions. If you’re using the conventional <span class="math inline">\(p&lt;.05\)</span> threshold, those decisions are:</p>
<table>
<thead>
<tr class="header">
<th align="left">Outcome</th>
<th align="left">Action</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(p\)</span> less than .05</td>
<td align="left">Reject the null</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(p\)</span> greater than .05</td>
<td align="left">Retain the null</td>
</tr>
</tbody>
</table>
<p>What <em>you’re</em> doing is adding a third possible action to the decision making problem. Specifically, what you’re doing is using the <span class="math inline">\(p\)</span>-value itself as a reason to justify continuing the experiment. And as a consequence you’ve transformed the decision-making procedure into one that looks more like this:</p>
<table>
<thead>
<tr class="header">
<th align="left">Outcome</th>
<th align="left">Action</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(p\)</span> less than .05</td>
<td align="left">Stop the experiment and reject the null</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(p\)</span> between .05 and .1</td>
<td align="left">Continue the experiment</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(p\)</span> greater than .1</td>
<td align="left">Stop the experiment and retain the null</td>
</tr>
</tbody>
</table>
<p>The “basic” theory of null hypothesis testing isn’t built to handle this sort of thing, not in the form I described back in Chapter <a href="hypothesistesting.html#hypothesistesting">11</a>. If you’re the kind of person who would choose to “collect more data” in real life, it implies that you are <em>not</em> making decisions in accordance with the rules of null hypothesis testing. Even if you happen to arrive at the same decision as the hypothesis test, you aren’t following the decision <em>process</em> it implies, and it’s this failure to follow the process that is causing the problem.<a href="#fn265" class="footnoteRef" id="fnref265"><sup>265</sup></a> Your <span class="math inline">\(p\)</span>-values are a lie.</p>
<p>Worse yet, they’re a lie in a dangerous way, because they’re all <em>too small</em>. To give you a sense of just how bad it can be, consider the following (worst case) scenario. Imagine you’re a really super-enthusiastic researcher on a tight budget who didn’t pay any attention to my warnings above. You design a study comparing two groups. You desperately want to see a significant result at the <span class="math inline">\(p&lt;.05\)</span> level, but you really don’t want to collect any more data than you have to (because it’s expensive). In order to cut costs, you start collecting data, but every time a new observation arrives you run a <span class="math inline">\(t\)</span>-test on your data. If the <span class="math inline">\(t\)</span>-tests says <span class="math inline">\(p&lt;.05\)</span> then you stop the experiment and report a significant result. If not, you keep collecting data. You keep doing this until you reach your pre-defined spending limit for this experiment. Let’s say that limit kicks in at <span class="math inline">\(N=1000\)</span> observations. As it turns out, the truth of the matter is that there is no real effect to be found: the null hypothesis is true. So, what’s the chance that you’ll make it to the end of the experiment and (correctly) conclude that there is no effect? In an ideal world, the answer here should be 95%. After all, the whole <em>point</em> of the <span class="math inline">\(p&lt;.05\)</span> criterion is to control the Type I error rate at 5%, so what we’d hope is that there’s only a 5% chance of falsely rejecting the null hypothesis in this situation. However, there’s no guarantee that will be true. You’re breaking the rules: you’re running tests repeatedly, “peeking” at your data to see if you’ve gotten a significant result, and all bets are off.</p>
<div class="figure"><span id="fig:type1"></span>
<img src="C:/Users/pcmis/Documents/rbook/bookdown/img/bayes/adapt.png" alt="How badly can things go wrong if you re-run your tests every time new data arrive? If you are a frequentist, the answer is &quot;very wrong&quot;."  />
<p class="caption">
Figure 17.1: How badly can things go wrong if you re-run your tests every time new data arrive? If you are a frequentist, the answer is “very wrong”.
</p>
</div>
<p>So how bad is it? The answer is shown as the solid black line in Figure <a href="bayes.html#fig:type1">17.1</a>, and it’s <em>astoundingly</em> bad. If you peek at your data after every single observation, there is a 49% chance that you will make a Type I error. That’s, um, quite a bit bigger than the 5% that it’s supposed to be. By way of comparison, imagine that you had used the following strategy. Start collecting data. Every single time an observation arrives, run a <em>Bayesian</em> <span class="math inline">\(t\)</span>-test (Section <a href="bayes.html#ttestbf">17.7</a> and look at the Bayes factor. I’ll assume that <span class="citation">Johnson (<a href="#ref-Johnson2013">2013</a>)</span> is right, and I’ll treat a Bayes factor of 3:1 as roughly equivalent to a <span class="math inline">\(p\)</span>-value of .05.<a href="#fn266" class="footnoteRef" id="fnref266"><sup>266</sup></a> This time around, our trigger happy researcher uses the following procedure: if the Bayes factor is 3:1 or more in favour of the null, stop the experiment and retain the null. If it is 3:1 or more in favour of the alternative, stop the experiment and reject the null. Otherwise continue testing. Now, just like last time, let’s assume that the null hypothesis is true. What happens? As it happens, I ran the simulations for this scenario too, and the results are shown as the dashed line in Figure <a href="bayes.html#fig:type1">17.1</a>. It turns out that the Type I error rate is much much lower than the 49% rate that we were getting by using the orthodox <span class="math inline">\(t\)</span>-test.</p>
<p>In some ways, this is remarkable. The entire <em>point</em> of orthodox null hypothesis testing is to control the Type I error rate. Bayesian methods aren’t actually designed to do this at all. Yet, as it turns out, when faced with a “trigger happy” researcher who keeps running hypothesis tests as the data come in, the Bayesian approach is much more effective. Even the 3:1 standard, which most Bayesians would consider unacceptably lax, is much safer than the <span class="math inline">\(p&lt;.05\)</span> rule.</p>
<div id="is-it-really-this-bad" class="section level3">
<h3><span class="header-section-number">17.5.1</span> Is it really this bad?</h3>
<p>The example I gave in the previous section is a pretty extreme situation. In real life, people don’t run hypothesis tests every time a new observation arrives. So it’s not fair to say that the <span class="math inline">\(p&lt;.05\)</span> threshold “really” corresponds to a 49% Type I error rate (i.e., <span class="math inline">\(p=.49\)</span>). But the fact remains that if you want your <span class="math inline">\(p\)</span>-values to be honest, then you either have to switch to a completely different way of doing hypothesis tests, or you must enforce a strict rule: <em>no peeking</em>. You are <em>not</em> allowed to use the data to decide when to terminate the experiment. You are <em>not</em> allowed to look at a “borderline” <span class="math inline">\(p\)</span>-value and decide to collect more data. You aren’t even allowed to change your data analyis strategy after looking at data. You are strictly required to follow these rules, otherwise the <span class="math inline">\(p\)</span>-values you calculate will be nonsense.</p>
<p>And yes, these rules are surprisingly strict. As a class exercise a couple of years back, I asked students to think about this scenario. Suppose you started running your study with the intention of collecting <span class="math inline">\(N=80\)</span> people. When the study starts out you follow the rules, refusing to look at the data or run any tests. But when you reach <span class="math inline">\(N=50\)</span> your willpower gives in… and you take a peek. Guess what? You’ve got a significant result! Now, sure, you know you <em>said</em> that you’d keep running the study out to a sample size of <span class="math inline">\(N=80\)</span>, but it seems sort of pointless now, right? The result is significant with a sample size of <span class="math inline">\(N=50\)</span>, so wouldn’t it be wasteful and inefficient to keep collecting data? Aren’t you tempted to stop? Just a little? Well, keep in mind that if you do, your Type I error rate at <span class="math inline">\(p&lt;.05\)</span> just ballooned out to 8%. When you report <span class="math inline">\(p&lt;.05\)</span> in your paper, what you’re <em>really</em> saying is <span class="math inline">\(p&lt;.08\)</span>. That’s how bad the consequences of “just one peek” can be.</p>
<p>Now consider this … the scientific literature is filled with <span class="math inline">\(t\)</span>-tests, ANOVAs, regressions and chi-square tests. When I wrote this book I didn’t pick these tests arbitrarily. The reason why these four tools appear in most introductory statistics texts is that these are the bread and butter tools of science. None of these tools include a correction to deal with “data peeking”: they all assume that you’re not doing it. But how realistic is that assumption? In real life, how many people do you think have “peeked” at their data before the experiment was finished and adapted their subsequent behaviour after seeing what the data looked like? Except when the sampling procedure is fixed by an external constraint, I’m guessing the answer is “most people have done it”. If that has happened, you can infer that the reported <span class="math inline">\(p\)</span>-values are wrong. Worse yet, because we don’t know what decision process they actually followed, we have no way to know what the <span class="math inline">\(p\)</span>-values <em>should</em> have been. You can’t compute a <span class="math inline">\(p\)</span>-value when you don’t know the decision making procedure that the researcher used. And so the reported <span class="math inline">\(p\)</span>-value remains a lie.</p>
<p>Given all of the above, what is the take home message? It’s not that Bayesian methods are foolproof. If a researcher is determined to cheat, they can always do so. Bayes’ rule cannot stop people from lying, nor can it stop them from rigging an experiment. That’s not my point here. My point is the same one I made at the very beginning of the book in Section <a href="why-do-we-learn-statistics.html#whywhywhy">1.1</a>: the reason why we run statistical tests is to protect us from ourselves. And the reason why “data peeking” is such a concern is that it’s so tempting, <em>even for honest researchers</em>. A theory for statistical inference has to acknowledge this. Yes, you might try to defend <span class="math inline">\(p\)</span>-values by saying that it’s the fault of the researcher for not using them properly. But to my mind that misses the point. A theory of statistical inference that is so completely naive about humans that it doesn’t even consider the possibility that the researcher might <em>look at their own data</em> isn’t a theory worth having. In essence, my point is this:</p>
<blockquote>
<p><em>Good laws have their origins in bad morals.</em><br />
– Ambrosius Macrobius<a href="#fn267" class="footnoteRef" id="fnref267"><sup>267</sup></a></p>
</blockquote>
<p>Good rules for statistical testing have to acknowledge human frailty. None of us are without sin. None of us are beyond temptation. A good system for statistical inference should still work even when it is used by actual humans. Orthodox null hypothesis testing does not.<a href="#fn268" class="footnoteRef" id="fnref268"><sup>268</sup></a></p>
</div>
</div>
<div id="bayescontingency" class="section level2">
<h2><span class="header-section-number">17.6</span> Bayesian analysis of contingency tables</h2>
<p>Time to change gears. Up to this point I’ve been talking about what Bayesian inference is and why you might consider using it. I now want to briefly describe how to do Bayesian versions of various statistical tests. The discussions in the next few sections are not as detailed as I’d like, but I hope they’re enough to help you get started. So let’s begin.</p>
<p>The first kind of statistical inference problem I discussed in this book appeared in Chapter <a href="chisquare.html#chisquare">12</a>, in which we discussed categorical data analysis problems. In that chapter I talked about several different statistical problems that you might be interested in, but the one that appears most often in real life is the analysis of <em>contingency tables</em>. In this kind of data analysis situation, we have a cross-tabulation of one variable against another one, and the goal is to find out if there is some <em>association</em> between these variables. The data set I used to illustrate this problem is found in the <code>chapek9.Rdata</code> file, and it contains a single data frame <code>chapek9</code></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="kw">file.path</span>(projecthome, <span class="st">&quot;data&quot;</span>,<span class="st">&quot;chapek9.Rdata&quot;</span>))
<span class="kw">head</span>(chapek9)</code></pre></div>
<pre><code>##   species choice
## 1   robot flower
## 2   human   data
## 3   human   data
## 4   human   data
## 5   robot   data
## 6   human flower</code></pre>
<p>In this data set, we supposedly sampled 180 beings and measured two things. First, we checked whether they were humans or robots, as captured by the <code>species</code> variable. Second, we asked them to nominate whether they most preferred flowers, puppies, or data. When we produce the cross-tabulation, we get this as the results:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">crosstab &lt;-<span class="st"> </span><span class="kw">xtabs</span>( <span class="op">~</span><span class="st"> </span>species <span class="op">+</span><span class="st"> </span>choice, chapek9 )
crosstab</code></pre></div>
<pre><code>##        choice
## species puppy flower data
##   robot    13     30   44
##   human    15     13   65</code></pre>
<p>Surprisingly, the humans seemed to show a much stronger preference for data than the robots did. At the time we speculated that this might have been because the questioner was a large robot carrying a gun, and the humans might have been scared.</p>
<div id="the-orthodox-text" class="section level3">
<h3><span class="header-section-number">17.6.1</span> The orthodox text</h3>
<p>Just to refresh your memory, here’s how we analysed these data back in <a href="mailto:Chapter@refch">Chapter@refch</a>:chisquare. Because we want to determine if there is some <em>association</em> between <code>species</code> and <code>choice</code>, we used the <code>associationTest()</code> function in the <code>lsr</code> package to run a chi-square test of association. The results looked like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(lsr)
<span class="kw">associationTest</span>( <span class="op">~</span>species <span class="op">+</span><span class="st"> </span>choice, chapek9 )</code></pre></div>
<pre><code>## 
##      Chi-square test of categorical association
## 
## Variables:   species, choice 
## 
## Hypotheses: 
##    null:        variables are independent of one another
##    alternative: some contingency exists between variables
## 
## Observed contingency table:
##        choice
## species puppy flower data
##   robot    13     30   44
##   human    15     13   65
## 
## Expected contingency table under the null hypothesis:
##        choice
## species puppy flower data
##   robot  13.5   20.8 52.7
##   human  14.5   22.2 56.3
## 
## Test results: 
##    X-squared statistic:  10.722 
##    degrees of freedom:  2 
##    p-value:  0.005 
## 
## Other information: 
##    estimated effect size (Cramer&#39;s v):  0.244</code></pre>
<p>Because we found a small <span class="math inline">\(p\)</span> value (in this case <span class="math inline">\(p&lt;.01\)</span>), we concluded that the data are inconsistent with the null hypothesis of no association, and we rejected it.</p>
</div>
<div id="the-bayesian-test" class="section level3">
<h3><span class="header-section-number">17.6.2</span> The Bayesian test</h3>
<p>How do we run an equivalent test as a Bayesian? Well, like every other bloody thing in statistics, there’s a lot of different ways you <em>could</em> do it. However, for the sake of everyone’s sanity, throughout this chapter I’ve decided to rely on one R package to do the work. Specifically, I’m going to use the <code>BayesFactor</code> package written by Jeff Rouder and Rich Morey, which as of this writing is in version 0.9.10.</p>
<p>For the analysis of contingency tables, the <code>BayesFactor</code> package contains a function called <code>contingencyTableBF()</code>. The data that you need to give to this function is the contingency table itself (i.e., the <code>crosstab</code> variable above), so you might be expecting to use a command like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>( BayesFactor )           <span class="co"># ...because we have to load the package</span>
<span class="kw">contingencyTableBF</span>( crosstab )   <span class="co"># ...because that makes sense, right?</span></code></pre></div>
<p>However, if you try this you’ll get an error message. This is because the <code>contingencyTestBF()</code> function needs one other piece of information from you: it needs to know what <em>sampling plan</em> you used to run your experiment. You can specify the sampling plan using the <code>sampleType</code> argument. So I should probably tell you what your options are! The <code>contingencyTableBF()</code> function distinguishes between four different types of experiment:</p>
<ul>
<li><strong>Fixed sample size</strong>. Suppose that in our <code>chapek9</code> example, our experiment was designed like this: we deliberately set out to test 180 people, but we didn’t try to control the number of humans or robots, nor did we try to control the choices they made. In this design, the total number of observations <span class="math inline">\(N\)</span> is fixed, but everything else is random. This is referred to as “joint multinomial” sampling, and if that’s what you did you should specify <code>sampleType = &quot;jointMulti&quot;</code>. In the case of the <code>chapek9</code> data, that’s actually what I had in mind when I invented the data set.</li>
<li><strong>Fixed row (or column) totals</strong>. A different kind of design might work like this. We decide ahead of time that we want 180 people, but we try to be a little more systematic about it. Specifically, the <em>experimenter</em> constrains it so that we get a predetermined number of humans and robots (e.g., 90 of each). In this design, <em>either</em> the row totals or the column totals are fixed, but not both. This is referred to as “independent multinomial” sampling, and if that’s what you did you should specify <code>sampleType = &quot;indepMulti&quot;</code>.</li>
<li><strong>Both row and column totals fixed</strong>. Another logical possibility is that you designed the experiment so that <em>both</em> the row totals and the column totals are fixed. This doesn’t make any sense at all in the <code>chapek9</code> example, but there are other deisgns that can work this way. Suppose that I show you a collection of 20 toys, and then given them 10 stickers that say <code>boy</code> and another 10 that say <code>girl</code>. I then give them 10 <code>blue</code> stickers and 10 <code>pink</code> stickers. I then ask you to put the stickers on the 20 toys such that every toy has a colour and every toy has a gender. No matter how you assign the stickers, the total number of pink and blue toys will be 10, as will the number of boys and girls. In this design <em>both</em> the rows and columns of the contingency table are fixed. This is referred to as “hypergeometric” sampling, and if that’s what you’ve done you should specify <code>sampleType = &quot;hypergeom&quot;</code>.</li>
<li><strong>Nothing is fixed</strong>. Finally, it might be the case that <em>nothing</em> is fixed. Not the row columns, not the column totals, and not the total sample size either. For instance, in the <code>chapek9</code> scenario, suppose what I’d done is run the study for a fixed length of <em>time</em>. By chance, it turned out that I got 180 people to turn up to study, but it could easily have been something else. This is referred to as “Poisson” sampling, and if that’s what you’ve done you should specify <code>sampleType=&quot;poisson&quot;</code>.</li>
</ul>
<p>Okay, so now we have enough knowledge to actually run a test. For the <code>chapek9</code> data, I implied that we designed the study such that the total sample size <span class="math inline">\(N\)</span> was fixed, so we should set <code>sampleType = &quot;jointMulti&quot;</code>. The command that we need is,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>( BayesFactor )</code></pre></div>
<pre><code>## Warning: package &#39;BayesFactor&#39; was built under R version 3.5.2</code></pre>
<pre><code>## Loading required package: coda</code></pre>
<pre><code>## Warning: package &#39;coda&#39; was built under R version 3.5.2</code></pre>
<pre><code>## Loading required package: Matrix</code></pre>
<pre><code>## ************
## Welcome to BayesFactor 0.9.12-4.2. If you have questions, please contact Richard Morey (richarddmorey@gmail.com).
## 
## Type BFManual() to open the manual.
## ************</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">contingencyTableBF</span>( crosstab, <span class="dt">sampleType =</span> <span class="st">&quot;jointMulti&quot;</span> )</code></pre></div>
<pre><code>## Bayes factor analysis
## --------------
## [1] Non-indep. (a=1) : 15.92684 ±0%
## 
## Against denominator:
##   Null, independence, a = 1 
## ---
## Bayes factor type: BFcontingencyTable, joint multinomial</code></pre>
<p>As with most R commands, the output initially looks suspiciously similar to utter gibberish. Fortunately, it’s actually pretty simple once you get past the initial impression. Firstly, note that the stuff at the top and bottom are irrelevant fluff. You already know that you’re doing a Bayes factor analysis. You already know that you’re analysing a contingency table, and you already know that you specified a joint multinomial sampling plan. So let’s strip that out and take a look at what’s left over:</p>
<pre><code>[1] Non-indep. (a=1) : 15.92684 @plusorminus0%

Against denominator:
  Null, independence, a = 1 </code></pre>
<p>Let’s also ignore those two <code>a=1</code> bits, since they’re technical details that you don’t need to know about at this stage.<a href="#fn269" class="footnoteRef" id="fnref269"><sup>269</sup></a> The rest of the output is actually pretty straightforward. At the bottom, the output defines the null hypothesis for you: in this case, the null hypothesis is that there is no relationship between <code>species</code> and <code>choice</code>. Or, to put it another way, the null hypothesis is that these two variables are <em>independent</em>. Now if you look at the line above it, you might (correctly) guess that the <code>Non-indep.</code> part refers to the <em>alternative</em> hypothesis. In this case, the alternative is that there <em>is</em> a relationship between <code>species</code> and <code>choice</code>: that is, they are not independent. So the only thing left in the output is the bit that reads</p>
<pre><code>15.92684 @plusorminus0%</code></pre>
<p>The 15.9 part is the Bayes factor, and it’s telling you that the odds for the alternative hypothesis against the null are about 16:1. The <span class="math inline">\(\pm0\%\)</span> part is not very interesting: essentially, all it’s telling you is that R has calculated an exact Bayes factor, so the uncertainty about the Bayes factor is 0%.<a href="#fn270" class="footnoteRef" id="fnref270"><sup>270</sup></a> In any case, the data are telling us that we have moderate evidence for the alternative hypothesis.</p>
</div>
<div id="writing-up-the-results" class="section level3">
<h3><span class="header-section-number">17.6.3</span> Writing up the results</h3>
<p>When writing up the results, my experience has been that there aren’t quite so many “rules” for how you “should” report Bayesian hypothesis tests. That might change in the future if Bayesian methods become standard and some task force starts writing up style guides, but in the meantime I would suggest using some common sense. For example, I would avoid writing this:</p>
<blockquote>
<p>A Bayesian test of association found a significant result (BF=15.92)</p>
</blockquote>
<p>To my mind, this write up is unclear. Even assuming that you’ve already reported the relevant descriptive statistics, there are a number of things I am unhappy with. First, the concept of “statistical significance” is pretty closely tied with <span class="math inline">\(p\)</span>-values, so it reads slightly strangely. Second, the “BF=15.92” part will only make sense to people who already understand Bayesian methods, and not everyone does. Third, it is somewhat unclear exactly which test was run and what software was used to do so.</p>
<p>On the other hand, unless precision is <em>extremely</em> important, I think that this is taking things a step too far:</p>
<blockquote>
<p>We ran a Bayesian test of association  using version 0.9.10-1 of the BayesFactor package  using default priors and a joint multinomial sampling plan. The resulting Bayes factor of 15.92 to 1 in favour of the alternative hypothesis indicates that there is moderately strong evidence for the non-independence of species and choice.</p>
</blockquote>
<p>Everything about that passage is correct, of course. <span class="citation">Morey and Rouder (<a href="#ref-Morey2015">2015</a>)</span> built their Bayesian tests of association using the paper by <span class="citation">Gunel and Dickey (<a href="#ref-Gunel1974">1974</a>)</span>, the specific test we used assumes that the experiment relied on a joint multinomial sampling plan, and indeed the Bayes factor of 15.92 is moderately strong evidence. It’s just far too wordy.</p>
<p>In most situations you just don’t need that much information. My preference is usually to go for something a little briefer. First, if you’re reporting multiple Bayes factor analyses in your write up, then somewhere you only need to cite the software once, at the beginning of the results section. So you might have one sentence like this:</p>
<blockquote>
<p>All analyses were conducted using the BayesFactor package in R , and unless otherwise stated default parameter values were used</p>
</blockquote>
<p>Notice that I don’t bother including the version number? That’s because the citation itself includes that information (go check my reference list if you don’t believe me). There’s no need to clutter up your results with redundant information that almost no-one will actually need. When you get to the actual test you can get away with this:</p>
<blockquote>
<p>A test of association produced a Bayes factor of 16:1 in favour of a relationship between species and choice.</p>
</blockquote>
<p>Short and sweet. I’ve rounded 15.92 to 16, because there’s not really any important difference between 15.92:1 and 16:1. I spelled out “Bayes factor” rather than truncating it to “BF” because not everyone knows the abbreviation. I indicated exactly what the effect is (i.e., “a relationship between species and choice”) and how strong the evidence was. I <em>didn’t</em> bother indicating whether this was “moderate” evidence or “strong” evidence, because the odds themselves tell you! There’s nothing stopping you from including that information, and I’ve done so myself on occasions, but you don’t strictly need it. Similarly, I didn’t bother to indicate that I ran the “joint multinomial” sampling plan, because I’m assuming that the method section of my write up would make clear how the experiment was designed. (I might change my mind about that if the method section was ambiguous.) Neither did I bother indicating that this was a <em>Bayesian</em> test of association: if your reader can’t work that out from the fact that you’re reporting a Bayes factor and the fact that you’re citing the <code>BayesFactor</code> package for all your analyses, then there’s no chance they’ll understand anything you’ve written. Besides, if you keep writing the word “Bayes” over and over again it starts to look stupid. Bayes Bayes Bayes Bayes Bayes. See?</p>
</div>
<div id="other-sampling-plans" class="section level3">
<h3><span class="header-section-number">17.6.4</span> Other sampling plans</h3>
<p>Up to this point all I’ve shown you is how to use the <code>contingencyTableBF()</code> function for the joint multinomial sampling plan (i.e., when the total sample size <span class="math inline">\(N\)</span> is fixed, but nothing else is). For the Poisson sampling plan (i.e., nothing fixed), the command you need is identical except for the <code>sampleType</code> argument:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">contingencyTableBF</span>(crosstab, <span class="dt">sampleType =</span> <span class="st">&quot;poisson&quot;</span> )</code></pre></div>
<pre><code>## Bayes factor analysis
## --------------
## [1] Non-indep. (a=1) : 28.20757 ±0%
## 
## Against denominator:
##   Null, independence, a = 1 
## ---
## Bayes factor type: BFcontingencyTable, poisson</code></pre>
<p>Notice that the Bayes factor of 28:1 here is <em>not</em> the identical to the Bayes factor of 16:1 that we obtained from the last test. The sampling plan actually does matter.</p>
<p>What about the design in which the row columns (or column totals) are fixed? As I mentioned earlier, this corresponds to the “independent multinomial” sampling plan. Again, you need to specify the <code>sampleType</code> argument, but this time you need to specify whether you fixed the rows or the columns. For example, suppose I deliberately sampled 87 humans and 93 robots, then I would need to indicate that the <code>fixedMargin</code> of the contingency table is the <code>&quot;rows&quot;</code>. So the command I would use is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">contingencyTableBF</span>(crosstab, <span class="dt">sampleType =</span> <span class="st">&quot;indepMulti&quot;</span>, <span class="dt">fixedMargin=</span><span class="st">&quot;rows&quot;</span>)</code></pre></div>
<pre><code>## Bayes factor analysis
## --------------
## [1] Non-indep. (a=1) : 8.605897 ±0%
## 
## Against denominator:
##   Null, independence, a = 1 
## ---
## Bayes factor type: BFcontingencyTable, independent multinomial</code></pre>
<p>Again, the Bayes factor is different, with the evidence for the alternative dropping to a mere 9:1. As you might expect, the answers would be diffrent again if it were the columns of the contingency table that the experimental design fixed.</p>
<p>Finally, if we turn to hypergeometric sampling in which everything is fixed, we get…</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">contingencyTableBF</span>(crosstab, <span class="dt">sampleType =</span> <span class="st">&quot;hypergeom&quot;</span>)
<span class="co">#Error in contingencyHypergeometric(as.matrix(data2), a) : </span>
<span class="co">#  hypergeometric contingency tables restricted to 2 x 2 tables; see help for contingencyTableBF()</span></code></pre></div>
<p>… an error message. Okay, some quick reading through the help files hints that support for larger contingency tables is coming, but it’s not been implemented yet. In the meantime, let’s imagine we have data from the “toy labelling” experiment I described earlier in this section. Specifically, let’s say our data look like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">toys &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">stringsAsFactors=</span><span class="ot">FALSE</span>,
        <span class="dt">gender =</span> <span class="kw">c</span>(<span class="st">&quot;girl&quot;</span>, <span class="st">&quot;boy&quot;</span>),
        <span class="dt">pink =</span> <span class="kw">c</span>(<span class="dv">8</span>, <span class="dv">2</span>),
        <span class="dt">blue =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">8</span>)
        )</code></pre></div>
<p>The Bayesian test with hypergeometric sampling gives us this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">contingencyTableBF</span>(toys, <span class="dt">sampleType =</span> <span class="st">&quot;hypergeom&quot;</span>)

<span class="co">#Bayes factor analysis</span>
<span class="co">#--------------</span>
<span class="co">#[1] Non-indep. (a=1) : 8.294321 @plusorminus0%</span>
<span class="co">#</span>
<span class="co">#Against denominator:</span>
<span class="co">#  Null, independence, a = 1 </span>
<span class="co">#---</span>
<span class="co">#Bayes factor type: BFcontingencyTable, hypergeometric</span></code></pre></div>
<p>The Bayes factor of 8:1 provides modest evidence that the labels were being assigned in a way that correlates gender with colour, but it’s not conclusive.</p>
</div>
</div>
<div id="ttestbf" class="section level2">
<h2><span class="header-section-number">17.7</span> Bayesian <span class="math inline">\(t\)</span>-tests</h2>
<p>The second type of statistical inference problem discussed in this book is the comparison between two means, discussed in some detail in the chapter on <span class="math inline">\(t\)</span>-tests (Chapter <a href="ttest.html#ttest">13</a>. If you can remember back that far, you’ll recall that there are several versions of the <span class="math inline">\(t\)</span>-test. The <code>BayesFactor</code> package contains a function called <code>ttestBF()</code> that is flexible enough to run several different versions of the <span class="math inline">\(t\)</span>-test. I’ll talk a little about Bayesian versions of the independent samples <span class="math inline">\(t\)</span>-tests and the paired samples <span class="math inline">\(t\)</span>-test in this section.</p>
<div id="independent-samples-t-test" class="section level3">
<h3><span class="header-section-number">17.7.1</span> Independent samples <span class="math inline">\(t\)</span>-test</h3>
<p>The most common type of <span class="math inline">\(t\)</span>-test is the independent samples <span class="math inline">\(t\)</span>-test, and it arises when you have data that look something like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="kw">file.path</span>(projecthome, <span class="st">&quot;data&quot;</span>,<span class="st">&quot;harpo.Rdata&quot;</span>))
<span class="kw">head</span>(harpo)</code></pre></div>
<pre><code>##   grade      tutor
## 1    65  Anastasia
## 2    72 Bernadette
## 3    66 Bernadette
## 4    74  Anastasia
## 5    73  Anastasia
## 6    71 Bernadette</code></pre>
<p>In this data set, we have two groups of students, those who received lessons from Anastasia and those who took their classes with Bernadette. The question we want to answer is whether there’s any difference in the grades received by these two groups of student. Back in <a href="mailto:Chapter@refch">Chapter@refch</a>:ttest I suggested you could analyse this kind of data using the <code>independentSamplesTTest()</code> function in the <code>lsr</code> package. For example, if you want to run a Student’s <span class="math inline">\(t\)</span>-test, you’d use a command like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">independentSamplesTTest</span>(
    <span class="dt">formula =</span> grade <span class="op">~</span><span class="st"> </span>tutor, 
    <span class="dt">data =</span> harpo, 
    <span class="dt">var.equal =</span> <span class="ot">TRUE</span> 
 )</code></pre></div>
<pre><code>## 
##    Student&#39;s independent samples t-test 
## 
## Outcome variable:   grade 
## Grouping variable:  tutor 
## 
## Descriptive statistics: 
##             Anastasia Bernadette
##    mean        74.533     69.056
##    std dev.     8.999      5.775
## 
## Hypotheses: 
##    null:        population means equal for both groups
##    alternative: different population means in each group
## 
## Test results: 
##    t-statistic:  2.115 
##    degrees of freedom:  31 
##    p-value:  0.043 
## 
## Other information: 
##    two-sided 95% confidence interval:  [0.197, 10.759] 
##    estimated effect size (Cohen&#39;s d):  0.74</code></pre>
<p>Like most of the functions that I wrote for this book, the <code>independentSamplesTTest()</code> is very wordy. It prints out a bunch of descriptive statistics and a reminder of what the null and alternative hypotheses are, before finally getting to the test results. I wrote it that way deliberately, in order to help make things a little clearer for people who are new to statistics.</p>
<p>Again, we obtain a <span class="math inline">\(p\)</span>-value less than 0.05, so we reject the null hypothesis.</p>
<p>What does the Bayesian version of the <span class="math inline">\(t\)</span>-test look like? Using the <code>ttestBF()</code> function, we can obtain a Bayesian analog of Student’s independent samples <span class="math inline">\(t\)</span>-test using the following command:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ttestBF</span>( <span class="dt">formula =</span> grade <span class="op">~</span><span class="st"> </span>tutor, <span class="dt">data =</span> harpo )</code></pre></div>
<pre><code>## Bayes factor analysis
## --------------
## [1] Alt., r=0.707 : 1.754927 ±0%
## 
## Against denominator:
##   Null, mu1-mu2 = 0 
## ---
## Bayes factor type: BFindepSample, JZS</code></pre>
<p>Notice that format of this command is pretty standard. As usual we have a <code>formula</code> argument in which we specify the outcome variable on the left hand side and the grouping variable on the right. The <code>data</code> argument is used to specify the data frame containing the variables. However, notice that there’s no analog of the <code>var.equal</code> argument. This is because the <code>BayesFactor</code> package does not include an analog of the Welch test, only the Student test.<a href="#fn271" class="footnoteRef" id="fnref271"><sup>271</sup></a> In any case, when you run this command you get this as the output:</p>
<p>So what does all this mean? Just as we saw with the <code>contingencyTableBF()</code> function, the output is pretty dense. But, just like last time, there’s not a lot of information here that you actually need to process. Firstly, let’s examine the bottom line. The <code>BFindepSample</code> part just tells you that you ran an independent samples <span class="math inline">\(t\)</span>-test, and the <code>JZS</code> part is technical information that is a little beyond the scope of this book.<a href="#fn272" class="footnoteRef" id="fnref272"><sup>272</sup></a> Clearly, there’s nothing to worry about in that part. In the line above, the text <code>Null, mu1-mu2 = 0</code> is just telling you that the null hypothesis is that there are no differences between means. But you already knew that. So the only part that really matters is this line here:</p>
<pre><code>[1] Alt., r=0.707 : 1.754927 @plusorminus0%</code></pre>
<p>Ignore the <code>r=0.707</code> part: it refers to a technical detail that we won’t worry about in this chapter.<a href="#fn273" class="footnoteRef" id="fnref273"><sup>273</sup></a> Instead, you should focus on the part that reads <code>1.754927</code>. This is the Bayes factor: the evidence provided by these data are about 1.8:1 in favour of the alternative.</p>
<p>Before moving on, it’s worth highlighting the difference between the orthodox test results and the Bayesian one. According to the orthodox test, we obtained a significant result, though only barely. Nevertheless, many people would happily accept <span class="math inline">\(p=.043\)</span> as reasonably strong evidence for an effect. In contrast, notice that the Bayesian test doesn’t even reach 2:1 odds in favour of an effect, and would be considered very weak evidence at best. In my experience that’s a pretty typical outcome. Bayesian methods usually require more evidence before rejecting the null.</p>
</div>
<div id="paired-samples-t-test" class="section level3">
<h3><span class="header-section-number">17.7.2</span> Paired samples <span class="math inline">\(t\)</span>-test</h3>
<p>Back in Section <a href="ttest.html#pairedsamplesttest">13.5</a> I discussed the <code>chico</code> data frame in which students grades were measured on two tests, and we were interested in finding out whether grades went up from test 1 to test 2. Because every student did both tests, the tool we used to analyse the data was a paired samples <span class="math inline">\(t\)</span>-test. To remind you of what the data look like, here’s the first few cases:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="kw">file.path</span>(projecthome, <span class="st">&quot;data&quot;</span>,<span class="st">&quot;chico.Rdata&quot;</span>))
<span class="kw">head</span>(chico)</code></pre></div>
<pre><code>##         id grade_test1 grade_test2
## 1 student1        42.9        44.6
## 2 student2        51.8        54.0
## 3 student3        71.7        72.3
## 4 student4        51.6        53.4
## 5 student5        63.5        63.8
## 6 student6        58.0        59.3</code></pre>
<p>We originally analysed the data using the <code>pairedSamplesTTest()</code> function in the <code>lsr</code> package, but this time we’ll use the <code>ttestBF()</code> function from the <code>BayesFactor</code> package to do the same thing. The easiest way to do it with this data set is to use the <code>x</code> argument to specify one variable and the <code>y</code> argument to specify the other. All we need to do then is specify <code>paired=TRUE</code> to tell R that this is a paired samples test. So here’s our command:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ttestBF</span>(
    <span class="dt">x =</span> chico<span class="op">$</span>grade_test1,
    <span class="dt">y =</span> chico<span class="op">$</span>grade_test2,
    <span class="dt">paired =</span> <span class="ot">TRUE</span>
 )</code></pre></div>
<pre><code>## Bayes factor analysis
## --------------
## [1] Alt., r=0.707 : 5992.05 ±0%
## 
## Against denominator:
##   Null, mu = 0 
## ---
## Bayes factor type: BFoneSample, JZS</code></pre>
<p>At this point, I hope you can read this output without any difficulty. The data provide evidence of about 6000:1 in favour of the alternative. We could probably reject the null with some confidence!</p>
</div>
</div>
<div id="bayesregression" class="section level2">
<h2><span class="header-section-number">17.8</span> Bayesian regression</h2>
<p>Okay, so now we’ve seen Bayesian equivalents to orthodox chi-square tests and <span class="math inline">\(t\)</span>-tests. What’s next? If I were to follow the same progression that I used when developing the orthodox tests you’d expect to see ANOVA next, but I think it’s a little clearer if we start with regression.</p>
<div id="a-quick-refresher" class="section level3">
<h3><span class="header-section-number">17.8.1</span> A quick refresher</h3>
<p>In Chapter <a href="regression.html#regression">15</a> I used the <code>parenthood</code> data to illustrate the basic ideas behind regression. To remind you of what that data set looks like, here’s the first six observations:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="kw">file.path</span>(projecthome, <span class="st">&quot;data&quot;</span>,<span class="st">&quot;parenthood.Rdata&quot;</span>))
<span class="kw">head</span>(parenthood)</code></pre></div>
<pre><code>##   dan.sleep baby.sleep dan.grump day
## 1      7.59      10.18        56   1
## 2      7.91      11.66        60   2
## 3      5.14       7.92        82   3
## 4      7.71       9.61        55   4
## 5      6.68       9.75        67   5
## 6      5.99       5.04        72   6</code></pre>
<p>Back in Chapter <a href="regression.html#regression">15</a> I proposed a theory in which my grumpiness (<code>dan.grump</code>) on any given day is related to the amount of sleep I got the night before (<code>dan.sleep</code>), and possibly to the amount of sleep our baby got (<code>baby.sleep</code>), though probably not to the <code>day</code> on which we took the measurement. We tested this using a regression model. In order to estimate the regression model we used the <code>lm()</code> function, like so:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model &lt;-<span class="st"> </span><span class="kw">lm</span>( 
  <span class="dt">formula =</span> dan.grump <span class="op">~</span><span class="st"> </span>dan.sleep <span class="op">+</span><span class="st"> </span>day <span class="op">+</span><span class="st"> </span>baby.sleep,
  <span class="dt">data =</span> parenthood
)</code></pre></div>
<p>The hypothesis tests for each of the terms in the regression model were extracted using the <code>summary()</code> function as shown below:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(model)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = dan.grump ~ dan.sleep + day + baby.sleep, data = parenthood)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -10.906  -2.284  -0.295   2.652  11.880 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 126.278707   3.242492  38.945   &lt;2e-16 ***
## dan.sleep    -8.969319   0.560007 -16.016   &lt;2e-16 ***
## day          -0.004403   0.015262  -0.288    0.774    
## baby.sleep    0.015747   0.272955   0.058    0.954    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.375 on 96 degrees of freedom
## Multiple R-squared:  0.8163, Adjusted R-squared:  0.8105 
## F-statistic: 142.2 on 3 and 96 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>When interpreting the results, each row in this table corresponds to one of the possible predictors. The <code>(Intercept)</code> term isn’t usually interesting, though it is highly significant. The important thing for our purposes is the fact that <code>dan.sleep</code> is significant at <span class="math inline">\(p&lt;.001\)</span> and neither of the other variables are.</p>
</div>
<div id="the-bayesian-version" class="section level3">
<h3><span class="header-section-number">17.8.2</span> The Bayesian version</h3>
<p>Okay, so how do we do the same thing using the <code>BayesFactor</code> package? The easiest way is to use the <code>regressionBF()</code> function instead of <code>lm()</code>. As before, we use <code>formula</code> to indicate what the full regression model looks like, and the <code>data</code> argument to specify the data frame. So the command is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">regressionBF</span>(
  <span class="dt">formula =</span> dan.grump <span class="op">~</span><span class="st"> </span>dan.sleep <span class="op">+</span><span class="st"> </span>day <span class="op">+</span><span class="st"> </span>baby.sleep,
  <span class="dt">data =</span> parenthood
)</code></pre></div>
<pre><code>## 
  |                                                                       
  |                                                                 |   0%
  |                                                                       
  |=========                                                        |  14%
  |                                                                       
  |===================                                              |  29%
  |                                                                       
  |============================                                     |  43%
  |                                                                       
  |=====================================                            |  57%
  |                                                                       
  |==============================================                   |  71%
  |                                                                       
  |========================================================         |  86%
  |                                                                       
  |=================================================================| 100%</code></pre>
<pre><code>## Bayes factor analysis
## --------------
## [1] dan.sleep                    : 1.622545e+34 ±0.01%
## [2] day                          : 0.2724027    ±0%
## [3] baby.sleep                   : 10018411     ±0%
## [4] dan.sleep + day              : 1.016576e+33 ±0%
## [5] dan.sleep + baby.sleep       : 9.77022e+32  ±0%
## [6] day + baby.sleep             : 2340755      ±0%
## [7] dan.sleep + day + baby.sleep : 7.835625e+31 ±0%
## 
## Against denominator:
##   Intercept only 
## ---
## Bayes factor type: BFlinearModel, JZS</code></pre>
<p>So that’s pretty straightforward: it’s exactly what we’ve been doing throughout the book. The output, however, is a little different from what you get from <code>lm()</code>. The format of this is pretty familiar. At the bottom we have some techical rubbish, and at the top we have some information about the Bayes factors. What’s new is the fact that we seem to have <em>lots</em> of Bayes factors here. What’s all this about?</p>
<p>The trick to understanding this output is to recognise that if we’re interested in working out which of the 3 predictor variables are related to <code>dan.grump</code>, there are actually 8 possible regression models that could be considered. One possibility is the <em>intercept only model</em>, in which none of the three variables have an effect. At the other end of the spectrum is the <em>full model</em> in which all three variables matter. So what <code>regressionBF()</code> does is treat the <em>intercept only</em> model as the null hypothesis, and print out the Bayes factors for all other models when compared against that null. For example, if we look at line 4 in the table, we see that the evidence is about <span class="math inline">\(10^{33}\)</span> to 1 in favour of the claim that a model that includes both <code>dan.sleep</code> and <code>day</code> is better than the intercept only model. Or if we look at line 1, we can see that the odds are about <span class="math inline">\(1.6 \times 10^{34}\)</span> that a model containing the <code>dan.sleep</code> variable (but no others) is better than the intercept only model.</p>
</div>
<div id="finding-the-best-model" class="section level3">
<h3><span class="header-section-number">17.8.3</span> Finding the best model</h3>
<p>In practice, this isn’t super helpful. In most situations the intercept only model is one that you don’t really care about at all. What I find helpful is to start out by working out which model is the <em>best</em> one, and then seeing how well all the alternatives compare to it. Here’s how you do that. In this case, it’s easy enough to see that the best model is actually the one that contains <code>dan.sleep</code> only (line 1), because it has the largest Bayes factor. However, if you’ve got a lot of possible models in the output, it’s handy to know that you can use the <code>head()</code> function to pick out the best few models. First, we have to go back and save the Bayes factor information to a variable:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">models &lt;-<span class="st"> </span><span class="kw">regressionBF</span>(
  <span class="dt">formula =</span> dan.grump <span class="op">~</span><span class="st"> </span>dan.sleep <span class="op">+</span><span class="st"> </span>day <span class="op">+</span><span class="st"> </span>baby.sleep,
  <span class="dt">data =</span> parenthood
)</code></pre></div>
<pre><code>## 
  |                                                                       
  |                                                                 |   0%
  |                                                                       
  |=========                                                        |  14%
  |                                                                       
  |===================                                              |  29%
  |                                                                       
  |============================                                     |  43%
  |                                                                       
  |=====================================                            |  57%
  |                                                                       
  |==============================================                   |  71%
  |                                                                       
  |========================================================         |  86%
  |                                                                       
  |=================================================================| 100%</code></pre>
<p>Let’s say I want to see the best three models. To do this, I use the <code>head()</code> function specifying <code>n=3</code>, and here’s what I get as the result:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>( models, <span class="dt">n =</span> <span class="dv">3</span>)</code></pre></div>
<pre><code>## Bayes factor analysis
## --------------
## [1] dan.sleep              : 1.622545e+34 ±0.01%
## [2] dan.sleep + day        : 1.016576e+33 ±0%
## [3] dan.sleep + baby.sleep : 9.77022e+32  ±0%
## 
## Against denominator:
##   Intercept only 
## ---
## Bayes factor type: BFlinearModel, JZS</code></pre>
<p>This is telling us that the model in line 1 (i.e., <code>dan.grump ~ dan.sleep</code>) is the best one. That’s <em>almost</em> what I’m looking for, but it’s still comparing all the models against the intercept only model. That seems silly. What I’d like to know is how big the difference is between the best model and the other good models. For that, there’s this trick:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>( models<span class="op">/</span><span class="kw">max</span>(models), <span class="dt">n =</span> <span class="dv">3</span>)</code></pre></div>
<pre><code>## Bayes factor analysis
## --------------
## [1] dan.sleep              : 1         ±0%
## [2] dan.sleep + day        : 0.0626532 ±0.01%
## [3] dan.sleep + baby.sleep : 0.0602154 ±0.01%
## 
## Against denominator:
##   dan.grump ~ dan.sleep 
## ---
## Bayes factor type: BFlinearModel, JZS</code></pre>
<p>Notice the bit at the bottom showing that the “denominator” has changed. What that means is that the Bayes factors are now comparing each of those 3 models listed against the <code>dan.grump ~ dan.sleep</code> model. Obviously, the Bayes factor in the first line is exactly 1, since that’s just comparing the best model to itself. More to the point, the other two Bayes factors are both less than 1, indicating that they’re all worse than that model. The Bayes factors of 0.06 to 1 imply that the odds for the best model over the second best model are about 16:1. You can work this out by simple arithmetic (i.e., <span class="math inline">\(0.06 / 1 \approx 16\)</span>), but the other way to do it is to directly compare the models. To see what I mean, here’s the original output:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">models</code></pre></div>
<pre><code>## Bayes factor analysis
## --------------
## [1] dan.sleep                    : 1.622545e+34 ±0.01%
## [2] day                          : 0.2724027    ±0%
## [3] baby.sleep                   : 10018411     ±0%
## [4] dan.sleep + day              : 1.016576e+33 ±0%
## [5] dan.sleep + baby.sleep       : 9.77022e+32  ±0%
## [6] day + baby.sleep             : 2340755      ±0%
## [7] dan.sleep + day + baby.sleep : 7.835625e+31 ±0%
## 
## Against denominator:
##   Intercept only 
## ---
## Bayes factor type: BFlinearModel, JZS</code></pre>
<p>The best model corresponds to row 1 in this table, and the second best model corresponds to row 4. All you have to do to compare these two models is this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">models[<span class="dv">1</span>] <span class="op">/</span><span class="st"> </span>models[<span class="dv">4</span>]</code></pre></div>
<pre><code>## Bayes factor analysis
## --------------
## [1] dan.sleep : 15.96088 ±0.01%
## 
## Against denominator:
##   dan.grump ~ dan.sleep + day 
## ---
## Bayes factor type: BFlinearModel, JZS</code></pre>
<p>And there you have it. You’ve found the regression model with the highest Bayes factor (i.e., <code>dan.grump ~ dan.sleep</code>), and you know that the evidence for that model over the next best alternative (i.e., <code>dan.grump ~ dan.sleep + day</code>) is about 16:1.</p>
</div>
<div id="extracting-bayes-factors-for-all-included-terms" class="section level3">
<h3><span class="header-section-number">17.8.4</span> Extracting Bayes factors for all included terms</h3>
<p>Okay, let’s say you’ve settled on a specific regression model. What Bayes factors should you report? In this example, I’m going to pretend that you decided that <code>dan.grump ~ dan.sleep + baby.sleep</code> is the model you think is best. Sometimes it’s sensible to do this, even when it’s not the one with the highest Bayes factor. Usually this happens because you have a substantive theoretical reason to prefer one model over the other. However, in this case I’m doing it because I want to use a model with more than one predictor as my example!</p>
<p>Having figured out which model you prefer, it can be really useful to call the <code>regressionBF()</code> function and specifying <code>whichModels=&quot;top&quot;</code>. You use your “preferred” model as the <code>formula</code> argument, and then the output will show you the Bayes factors that result when you try to drop predictors from this model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">regressionBF</span>( 
 <span class="dt">formula =</span> dan.grump <span class="op">~</span><span class="st"> </span>dan.sleep <span class="op">+</span><span class="st"> </span>baby.sleep,
 <span class="dt">data =</span> parenthood,
 <span class="dt">whichModels =</span> <span class="st">&quot;top&quot;</span>
)</code></pre></div>
<pre><code>## 
  |                                                                       
  |                                                                 |   0%
  |                                                                       
  |======================                                           |  33%
  |                                                                       
  |===========================================                      |  67%
  |                                                                       
  |=================================================================| 100%</code></pre>
<pre><code>## Bayes factor top-down analysis
## --------------
## When effect is omitted from dan.sleep + baby.sleep , BF is...
## [1] Omit baby.sleep : 16.60705     ±0.01%
## [2] Omit dan.sleep  : 1.025403e-26 ±0.01%
## 
## Against denominator:
##   dan.grump ~ dan.sleep + baby.sleep 
## ---
## Bayes factor type: BFlinearModel, JZS</code></pre>
<p>Okay, so now you can see the results a bit more clearly. The Bayes factor when you try to drop the <code>dan.sleep</code> predictor is about <span class="math inline">\(10^{-26}\)</span>, which is very strong evidence that you <em>shouldn’t</em> drop it. On the other hand, the Bayes factor actually goes up to 17 if you drop <code>baby.sleep</code>, so you’d usually say that’s pretty strong evidence for dropping that one.</p>
</div>
</div>
<div id="bayesanova" class="section level2">
<h2><span class="header-section-number">17.9</span> Bayesian ANOVA</h2>
<p>As you can tell, the <code>BayesFactor</code> package is pretty flexible, and it can do Bayesian versions of pretty much everything in this book. In fact, it can do a few other neat things that I haven’t covered in the book at all. However, I have to stop somewhere, and so there’s only one other topic I want to cover: Bayesian ANOVA.</p>
<div id="a-quick-refresher-1" class="section level3">
<h3><span class="header-section-number">17.9.1</span> A quick refresher</h3>
<p>As with the other examples, I think it’s useful to start with a reminder of how I discussed ANOVA earlier in the book. First, let’s remind ourselves of what the data were. The example I used originally is the <code>clin.trial</code> data frame, which looks like this</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="kw">file.path</span>(projecthome, <span class="st">&quot;data&quot;</span>,<span class="st">&quot;clinicaltrial.Rdata&quot;</span>))
<span class="kw">head</span>(clin.trial)</code></pre></div>
<pre><code>##       drug    therapy mood.gain
## 1  placebo no.therapy       0.5
## 2  placebo no.therapy       0.3
## 3  placebo no.therapy       0.1
## 4 anxifree no.therapy       0.6
## 5 anxifree no.therapy       0.4
## 6 anxifree no.therapy       0.2</code></pre>
<p>To run our orthodox analysis in earlier chapters we used the <code>aov()</code> function to do all the heavy lifting. In Chapter <a href="anova2.html#anova2">16</a> I recommended using the <code>Anova()</code> function from the <code>car</code> package to produce the ANOVA table, because it uses Type II tests by default. If you’ve forgotten what “Type II tests” are, it might be a good idea to re-read Section <a href="anova2.html#unbalancedanova">16.10</a>, because it will become relevant again in a moment. In any case, here’s what our analysis looked like:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(car)
model &lt;-<span class="st"> </span><span class="kw">aov</span>( mood.gain <span class="op">~</span><span class="st"> </span>drug <span class="op">*</span><span class="st"> </span>therapy, <span class="dt">data =</span> clin.trial )
<span class="kw">Anova</span>(model)            </code></pre></div>
<pre><code>## Anova Table (Type II tests)
## 
## Response: mood.gain
##              Sum Sq Df F value    Pr(&gt;F)    
## drug         3.4533  2 31.7143 1.621e-05 ***
## therapy      0.4672  1  8.5816   0.01262 *  
## drug:therapy 0.2711  2  2.4898   0.12460    
## Residuals    0.6533 12                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>That’s pretty clearly showing us evidence for a main effect of <code>drug</code> at <span class="math inline">\(p&lt;.001\)</span>, an effect of <code>therapy</code> at <span class="math inline">\(p&lt;.05\)</span> and no interaction.</p>
</div>
<div id="the-bayesian-version-1" class="section level3">
<h3><span class="header-section-number">17.9.2</span> The Bayesian version</h3>
<p>How do we do the same thing using Bayesian methods? The <code>BayesFactor</code> package contains a function called <code>anovaBF()</code> that does this for you. It uses a pretty standard <code>formula</code> and <code>data</code> structure, so the command should look really familiar. Just like we did with regression, it will be useful to save the output to a variable:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">models &lt;-<span class="st"> </span><span class="kw">anovaBF</span>( 
 <span class="dt">formula =</span> mood.gain <span class="op">~</span><span class="st"> </span>drug <span class="op">*</span><span class="st"> </span>therapy,
 <span class="dt">data =</span> clin.trial
)</code></pre></div>
<pre><code>## 
  |                                                                       
  |                                                                 |   0%
  |                                                                       
  |================                                                 |  25%
  |                                                                       
  |================================                                 |  50%
  |                                                                       
  |=================================================                |  75%
  |                                                                       
  |=================================================================| 100%</code></pre>
<p>The output is quite different to the traditional ANOVA, but it’s not too bad once you understand what you’re looking for. Let’s take a look:</p>
<pre><code>models</code></pre>
<p>This looks very similar to the output we obtained from the <code>regressionBF()</code> function, and with good reason. Remember what I said back in Section <a href="anova2.html#anovalm">16.6</a>: under the hood, ANOVA is no different to regression, and both are just different examples of a linear model. Becasue of this, the <code>anovaBF()</code> reports the output in much the same way. For instance, if we want to identify the best model we could use the same commands that we used in the last section. One variant that I find quite useful is this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">models<span class="op">/</span><span class="kw">max</span>(models)</code></pre></div>
<pre><code>## Bayes factor analysis
## --------------
## [1] drug                          : 0.346168    ±1.34%
## [2] therapy                       : 0.001029907 ±1.34%
## [3] drug + therapy                : 1           ±0%
## [4] drug + therapy + drug:therapy : 0.9769504   ±1.99%
## 
## Against denominator:
##   mood.gain ~ drug + therapy 
## ---
## Bayes factor type: BFlinearModel, JZS</code></pre>
<p>By “dividing” the <code>models</code> output by the best model (i.e., <code>max(models)</code>), what R is doing is using the best model (which in this case is <code>drugs + therapy</code>) as the denominator, which gives you a pretty good sense of how close the competitors are. For instance, the model that contains the interaction term is almost as good as the model without the interaction, since the Bayes factor is 0.98. In other words, the data do not clearly indicate whether there is or is not an interaction.</p>
</div>
<div id="constructing-bayesian-type-ii-tests" class="section level3">
<h3><span class="header-section-number">17.9.3</span> Constructing Bayesian Type II tests</h3>
<p>Okay, that’s all well and good, you might be thinking, but what do I report as the alternative to the <span class="math inline">\(p\)</span>-value? In the classical ANOVA table, you get a single <span class="math inline">\(p\)</span>-value for every predictor in the model, so you can talk about the significance of each effect. What’s the Bayesian analog of this?</p>
<p>It’s a good question, but the answer is tricky. Remember what I said in Section <a href="anova2.html#unbalancedanova">16.10</a> about ANOVA being complicated. Even in the classical version of ANOVA there are several different “things” that ANOVA might correspond to. Specifically, I discussed how you get different <span class="math inline">\(p\)</span>-values depending on whether you use Type I tests, Type II tests or Type III tests. To work out which Bayes factor is analogous to “the” <span class="math inline">\(p\)</span>-value in a classical ANOVA, you need to work out which version of ANOVA you want an analog for. For the purposes of this section, I’ll assume you want Type II tests, because those are the ones I think are most sensible in general. As I discussed back in Section <a href="anova2.html#unbalancedanova">16.10</a>, Type II tests for a two-way ANOVA are reasonably straightforward, but if you have forgotten that section it wouldn’t be a bad idea to read it again before continuing.</p>
<p>Assuming you’ve had a refresher on Type II tests, let’s have a look at how to pull them from the Bayes factor table. Suppose we want to test the main effect of <code>drug</code>. The null hypothesis for this test corresponds to a model that includes an effect of <code>therapy</code>, but no effect of <code>drug</code>. The alternative hypothesis is the model that includes both. In other words, what we want is the Bayes factor corresponding to this comparison:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knitr<span class="op">::</span><span class="kw">kable</span>(tibble<span class="op">::</span><span class="kw">tribble</span>(
                   <span class="op">~</span>V1,                            <span class="op">~</span>V2,
         <span class="st">&quot;Null model:&quot;</span>,        <span class="st">&quot;`mood.gain ~ therapy`&quot;</span>,
  <span class="st">&quot;Alternative model:&quot;</span>, <span class="st">&quot;`mood.gain ~ therapy + drug`&quot;</span>
  ), <span class="dt">col.names =</span> <span class="kw">c</span>(<span class="st">&quot;&quot;</span>, <span class="st">&quot;&quot;</span>))</code></pre></div>
<table>
<tbody>
<tr class="odd">
<td align="left">Null model:</td>
<td align="left"><code>mood.gain ~ therapy</code></td>
</tr>
<tr class="even">
<td align="left">Alternative model:</td>
<td align="left"><code>mood.gain ~ therapy + drug</code></td>
</tr>
</tbody>
</table>
<p>As it happens, we can read the answer to this straight off the table because it corresponds to a comparison between the model in line 2 of the table and the model in line 3: the Bayes factor in this case represents evidence <em>for</em> the null of 0.001 to 1. Or, more helpfully, the odds are about 1000 to 1 against the null.</p>
<p>The main effect of <code>therapy</code> can be calculated in much the same way. In this case, the null model is the one that contains only an effect of drug, and the alternative is the model that contains both. So the relevant comparison is between lines 2 and 1 in the table. The odds in favour of the null here are only 0.35 to 1. Again, I find it useful to frame things the other way around, so I’d refer to this as evidence of about 3 to 1 in favour of an effect of <code>therapy</code>.</p>
<p>Finally, in order to test an interaction effect, the null model here is one that contains both main effects but no interaction. The alternative model adds the interaction. That is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knitr<span class="op">::</span><span class="kw">kable</span>(tibble<span class="op">::</span><span class="kw">tribble</span>(
                   <span class="op">~</span>V1,                            <span class="op">~</span>V2,
         <span class="st">&quot;Null model:&quot;</span>,        <span class="st">&quot;`mood.gain ~ drug + therapy`&quot;</span>,
  <span class="st">&quot;Alternative model:&quot;</span>, <span class="st">&quot;`mood.gain ~ drug + therapy + drug:therapy`&quot;</span>
  ), <span class="dt">col.names =</span> <span class="kw">c</span>(<span class="st">&quot;&quot;</span>, <span class="st">&quot;&quot;</span>))</code></pre></div>
<table>
<tbody>
<tr class="odd">
<td align="left">Null model:</td>
<td align="left"><code>mood.gain ~ drug + therapy</code></td>
</tr>
<tr class="even">
<td align="left">Alternative model:</td>
<td align="left"><code>mood.gain ~ drug + therapy + drug:therapy</code></td>
</tr>
</tbody>
</table>
<p>If we look those two models up in the table, we see that this comparison is between the models on lines 3 and 4 of the table. The odds of 0.98 to 1 imply that these two models are fairly evenly matched.</p>
<p>You might be thinking that this is all pretty laborious, and I’ll concede that’s true. At some stage I might consider adding a function to the <code>lsr</code> package that would automate this process and construct something like a “Bayesian Type II ANOVA table” from the output of the <code>anovaBF()</code> function. However, I haven’t had time to do this yet, nor have I made up my mind about whether it’s really a good idea to do this. In the meantime, I thought I should show you the trick for how I do this in practice. The command that I use when I want to grab the right Bayes factors for a Type II ANOVA is this one:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">max</span>(models)<span class="op">/</span>models</code></pre></div>
<pre><code>##                 denominator
## numerator            drug  therapy drug + therapy
##   drug + therapy 2.888771 970.9615              1
##                 denominator
## numerator        drug + therapy + drug:therapy
##   drug + therapy                      1.023593</code></pre>
<p>The output isn’t quite so pretty as the last one, but the nice thing is that you can read off everything you need. The best model is <code>drug + therapy</code>, so all the other models are being compared to that. What’s the Bayes factor <em>for</em> the main effect of <code>drug</code>? The relevant null hypothesis is the one that contains only <code>therapy</code>, and the Bayes factor in question is 954:1. The main effect of <code>therapy</code> is weaker, and the evidence here is only 2.8:1. Finally, the evidence <em>against</em> an interaction is very weak, at 1.01:1.</p>
<p>Reading the results off this table is sort of counterintuitive, because you have to read off the answers from the “wrong” part of the table. For instance, the evidence for an effect of <code>drug</code> can be read from the column labelled <code>therapy</code>, which is pretty damned weird. To be fair to the authors of the package, I don’t think they ever intended for the <code>anovaBF()</code> function to be used this way. My understanding<a href="#fn274" class="footnoteRef" id="fnref274"><sup>274</sup></a> is that their view is simply that you should find the best model and report that model: there’s no inherent reason why a Bayesian ANOVA should try to follow the exact same design as an orthodox ANOVA.<a href="#fn275" class="footnoteRef" id="fnref275"><sup>275</sup></a></p>
<p>In any case, if you know what you’re looking for, you can look at this table and then report the results of the Bayesian analysis in a way that is pretty closely analogous to how you’d report a regular Type II ANOVA. As I mentioned earlier, there’s still no convention on how to do that, but I usually go for something like this:</p>
<blockquote>
<p>A Bayesian Type II ANOVA found evidence for main effects of drug (Bayes factor: 954:1) and therapy (Bayes factor: 3:1), but no clear evidence for or against an interaction (Bayes factor: 1:1).</p>
</blockquote>
</div>
</div>
<div id="summary-15" class="section level2">
<h2><span class="header-section-number">17.10</span> Summary</h2>
<p>The first half of this chapter was focused primarily on the theoretical underpinnings of Bayesian statistics. I introduced the mathematics for how Bayesian inference works (Section <a href="bayes.html#basicbayes">17.1</a>), and gave a very basic overview of how Bayesian hypothesis testing is typically done (Section <a href="bayes.html#bayesianhypothesistests">17.2</a>). Finally, I devoted some space to talking about why I think Bayesian methods are worth using (Section <a href="bayes.html#whybayes">17.3</a>.</p>
<p>The second half of the chapter was a lot more practical, and focused on tools provided by the <code>BayesFactor</code> package. Specifically, I talked about using the <code>contingencyTableBF()</code> function to do Bayesian analogs of chi-square tests (Section <a href="bayes.html#bayescontingency">17.6</a>, the <code>ttestBF()</code> function to do Bayesian <span class="math inline">\(t\)</span>-tests, (Section <a href="bayes.html#ttestbf">17.7</a>), the <code>regressionBF()</code> function to do Bayesian regressions, and finally the <code>anovaBF()</code> function for Bayesian ANOVA.</p>
<p>If you’re interested in learning more about the Bayesian approach, there are many good books you could look into. John Kruschke’s book <em>Doing Bayesian Data Analysis</em> is a pretty good place to start <span class="citation">(Kruschke <a href="#ref-Kruschke2011">2011</a>)</span>, and is a nice mix of theory and practice. His approach is a little different to the “Bayes factor” approach that I’ve discussed here, so you won’t be covering the same ground. If you’re a cognitive psychologist, you might want to check out Michael Lee and E.J. Wagenmakers’ book <em>Bayesian Cognitive Modeling</em> <span class="citation">(Lee and Wagenmakers <a href="#ref-Lee2014">2014</a>)</span>. I picked these two because I think they’re especially useful for people in my discipline, but there’s a lot of good books out there, so look around!</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Jeffreys1961">
<p>Jeffreys, Harold. 1961. <em>The Theory of Probability</em>. 3rd ed. Oxford.</p>
</div>
<div id="ref-Kass1995">
<p>Kass, Robert E., and Adrian E. Raftery. 1995. “Bayes Factors.” <em>Journal of the American Statistical Association</em> 90: 773–95.</p>
</div>
<div id="ref-Fisher1925">
<p>Fisher, R. 1925. <em>Statistical Methods for Research Workers</em>. Edinburgh, UK: Oliver; Boyd.</p>
</div>
<div id="ref-Johnson2013">
<p>Johnson, Valen E. 2013. “Revised Standards for Statistical Evidence.” <em>Proceedings of the National Academy of Sciences</em>, no. 48: 19313–7.</p>
</div>
<div id="ref-Morey2015">
<p>Morey, Richard D., and Jeffrey N. Rouder. 2015. <em>BayesFactor: Computation of Bayes Factors for Common Designs</em>. <a href="http://CRAN.R-project.org/package=BayesFactor" class="uri">http://CRAN.R-project.org/package=BayesFactor</a>.</p>
</div>
<div id="ref-Gunel1974">
<p>Gunel, Erdogan, and James Dickey. 1974. “Bayes Factors for Independence in Contingency Tables.” <em>Biometrika</em>, 545–57.</p>
</div>
<div id="ref-Kruschke2011">
<p>Kruschke, J. K. 2011. <em>Doing Bayesian Data Analysis: A Tutorial with R and BUGS</em>. Burlington, MA: Academic Press.</p>
</div>
<div id="ref-Lee2014">
<p>Lee, Michael D, and Eric-Jan Wagenmakers. 2014. <em>Bayesian Cognitive Modeling: A Practical Course</em>. Cambridge University Press.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="254">
<li id="fn254"><p><a href="http://en.wikiquote.org/wiki/David_Hume" class="uri">http://en.wikiquote.org/wiki/David_Hume</a><a href="bayes.html#fnref254">↩</a></p></li>
<li id="fn255"><p><a href="http://en.wikipedia.org/wiki/Climate_of_Adelaide" class="uri">http://en.wikipedia.org/wiki/Climate_of_Adelaide</a><a href="bayes.html#fnref255">↩</a></p></li>
<li id="fn256"><p>It’s a leap of faith, I know, but let’s run with it okay?<a href="bayes.html#fnref256">↩</a></p></li>
<li id="fn257"><p>Um. I hate to bring this up, but some statisticians would object to me using the word “likelihood” here. The problem is that the word “likelihood” has a very specific meaning in frequentist statistics, and it’s not quite the same as what it means in Bayesian statistics. As far as I can tell, Bayesians didn’t originally have any agreed upon name for the likelihood, and so it became common practice for people to use the frequentist terminology. This wouldn’t have been a problem, except for the fact that the way that Bayesians use the word turns out to be quite different to the way frequentists do. This isn’t the place for yet another lengthy history lesson, but to put it crudely: when a Bayesian says “<em>a</em> likelihood function” they’re usually referring one of the <em>rows</em> of the table. When a frequentist says the same thing, they’re referring to the same table, but to them “<em>a</em> likelihood function” almost always refers to one of the <em>columns</em>. This distinction matters in some contexts, but it’s not important for our purposes.<a href="bayes.html#fnref257">↩</a></p></li>
<li id="fn258"><p>If we were being a bit more sophisticated, we could extend the example to accommodate the possibility that I’m lying about the umbrella. But let’s keep things simple, shall we?<a href="bayes.html#fnref258">↩</a></p></li>
<li id="fn259"><p>You might notice that this equation is actually a restatement of the same basic rule I listed at the start of the last section. If you multiply both sides of the equation by <span class="math inline">\(P(d)\)</span>, then you get <span class="math inline">\(P(d) P(h| d) = P(d,h)\)</span>, which is the rule for how joint probabilities are calculated. So I’m not actually introducing any “new” rules here, I’m just using the same rule in a different way.<a href="bayes.html#fnref259">↩</a></p></li>
<li id="fn260"><p>Obviously, this is a highly simplified story. All the complexity of real life Bayesian hypothesis testing comes down to how you calculate the likelihood <span class="math inline">\(P(d|h)\)</span> when the hypothesis <span class="math inline">\(h\)</span> is a complex and vague thing. I’m not going to talk about those complexities in this book, but I do want to highlight that although this simple story is true as far as it goes, real life is messier than I’m able to cover in an introductory stats textbook.<a href="bayes.html#fnref260">↩</a></p></li>
<li id="fn261"><p><a href="http://www.imdb.com/title/tt0093779/quotes" class="uri">http://www.imdb.com/title/tt0093779/quotes</a>. I should note in passing that I’m not the first person to use this quote to complain about frequentist methods. Rich Morey and colleagues had the idea first. I’m shamelessly stealing it because it’s such an awesome pull quote to use in this context and I refuse to miss any opportunity to quote <em>The Princess Bride</em>.<a href="bayes.html#fnref261">↩</a></p></li>
<li id="fn262"><p><a href="http://about.abc.net.au/reports-publications/appreciation-survey-summary-report-2013/" class="uri">http://about.abc.net.au/reports-publications/appreciation-survey-summary-report-2013/</a><a href="bayes.html#fnref262">↩</a></p></li>
<li id="fn263"><p><a href="http://knowyourmeme.com/memes/the-cake-is-a-lie" class="uri">http://knowyourmeme.com/memes/the-cake-is-a-lie</a><a href="bayes.html#fnref263">↩</a></p></li>
<li id="fn264"><p>In the interests of being completely honest, I should acknowledge that not all orthodox statistical tests that rely on this silly assumption. There are a number of <em>sequential analysis</em> tools that are sometimes used in clinical trials and the like. These methods are built on the assumption that data are analysed as they arrive, and these tests aren’t horribly broken in the way I’m complaining about here. However, sequential analysis methods are constructed in a very different fashion to the “standard” version of null hypothesis testing. They don’t make it into any introductory textbooks, and they’re not very widely used in the psychological literature. The concern I’m raising here is valid for every single orthodox test I’ve presented so far, and for almost every test I’ve seen reported in the papers I read.<a href="bayes.html#fnref264">↩</a></p></li>
<li id="fn265"><p>A related problem: <a href="http://xkcd.com/1478/" class="uri">http://xkcd.com/1478/</a><a href="bayes.html#fnref265">↩</a></p></li>
<li id="fn266"><p>Some readers might wonder why I picked 3:1 rather than 5:1, given that <span class="citation">Johnson (<a href="#ref-Johnson2013">2013</a>)</span> suggests that <span class="math inline">\(p=.05\)</span> lies somewhere in that range. I did so in order to be charitable to the <span class="math inline">\(p\)</span>-value. If I’d chosen a 5:1 Bayes factor instead, the results would look even better for the Bayesian approach.<a href="bayes.html#fnref266">↩</a></p></li>
<li id="fn267"><p><a href="http://www.quotationspage.com/quotes/Ambrosius_Macrobius/" class="uri">http://www.quotationspage.com/quotes/Ambrosius_Macrobius/</a><a href="bayes.html#fnref267">↩</a></p></li>
<li id="fn268"><p>Okay, I just <em>know</em> that some knowledgeable frequentists will read this and start complaining about this section. Look, I’m not dumb. I absolutely know that if you adopt a sequential analysis perspective you can avoid these errors within the orthodox framework. I also know that you can explictly design studies with interim analyses in mind. So yes, in one sense I’m attacking a “straw man” version of orthodox methods. However, the straw man that I’m attacking is the one that <em>is used by almost every single practitioner</em>. If it ever reaches the point where sequential methods become the norm among experimental psychologists and I’m no longer forced to read 20 extremely dubious ANOVAs a day, I promise I’ll rewrite this section and dial down the vitriol. But until that day arrives, I stand by my claim that <em>default</em> Bayes factor methods are much more robust in the face of data analysis practices as they exist in the real world. <em>Default</em> orthodox methods suck, and we all know it.<a href="bayes.html#fnref268">↩</a></p></li>
<li id="fn269"><p>If you’re desperate to know, you can find all the gory details in Gunel and Dickey (1974). However, that’s a pretty technical paper. The help documentation to the <code>contingencyTableBF()</code> gives this explanation: “the argument <code>priorConcentration</code> indexes the expected deviation from the null hypothesis under the alternative, and corresponds to Gunel and Dickey’s (1974) <span class="math inline">\(a\)</span> parameter.” As I write this I’m about halfway through the Gunel and Dickey paper, and I agree that setting <span class="math inline">\(a=1\)</span> is a pretty sensible default choice, since it corresponds to an assumption that you have very little <em>a priori</em> knowledge about the contingency table.<a href="bayes.html#fnref269">↩</a></p></li>
<li id="fn270"><p>In some of the later examples, you’ll see that this number is not always 0%. This is because the <code>BayesFactor</code> package often has to run some simulations to compute approximate Bayes factors. So the answers you get won’t always be identical when you run the command a second time. That’s why the output of these functions tells you what the margin for error is.<a href="bayes.html#fnref270">↩</a></p></li>
<li id="fn271"><p>Apparently this omission is deliberate. I have this vague recollection that I spoke to Jeff Rouder about this once, and his opinion was that when homogeneity of variance is violated the results of a <span class="math inline">\(t\)</span>-test are uninterpretable. I can see the argument for this, but I’ve never really held a strong opinion myself. (Jeff, if you never said that, I’m sorry)<a href="bayes.html#fnref271">↩</a></p></li>
<li id="fn272"><p>Just in case you’re interested: the “JZS” part of the output relates to how the Bayesian test expresses the prior uncertainty about the variance <span class="math inline">\(\sigma^2\)</span>, and it’s short for the names of three people: “Jeffreys Zellner Siow”. See <span class="citation">Rouder et al. (<a href="#ref-Rouder2009">2009</a>)</span> for details.<a href="bayes.html#fnref272">↩</a></p></li>
<li id="fn273"><p>Again, in case you care … the null hypothesis here specifies an effect size of 0, since the two means are identical. The alternative hypothesis states that there <em>is</em> an effect, but it doesn’t specify exactly how big the effect will be. The <span class="math inline">\(r\)</span> value here relates to how big the effect is expected to be according to the alternative. You can type <code>?ttestBF</code> to get more details.<a href="bayes.html#fnref273">↩</a></p></li>
<li id="fn274"><p>Again, guys, sorry if I’ve misread you.<a href="bayes.html#fnref274">↩</a></p></li>
<li id="fn275"><p>I don’t even disagree with them: it’s <em>not</em> at all obvious why a Bayesian ANOVA should reproduce (say) the same set of model comparisons that the Type II testing strategy uses. It’s precisely because of the fact that I haven’t really come to any strong conclusions that I haven’t added anything to the <code>lsr</code> package to make Bayesian Type II tests easier to produce.<a href="bayes.html#fnref275">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="part-vi-endings-alternatives-and-prospects.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="epilogue.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
