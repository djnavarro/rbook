# Descriptive statistics {#descriptives}

```{r}
#| echo: false
#| message: false
#| results: asis
source(here::here("build", "common.R"))
status("drafting")
```


Any time that you get a new data set to look at, one of the first tasks that you have to do is find ways of summarising the data in a compact, easily understood fashion. This is what **descriptive statistics** (as opposed to inferential statistics) is all about. In fact, to many people the term "statistics" is synonymous with descriptive statistics. It is this topic that we'll consider in this chapter, but before going into any details, let's take a moment to get a sense of why we need descriptive statistics. To get started, let's take a look at the `southern_slam` data that is bundled as part of the lsrbook package. First, let's load the packages that we'll need:

```{r}
library(tibble)
library(lsrbook)
```

The reason for loading the lsrbook package is probably obvious: that's the package that provides the data set! The reason for loading tibble is probably less obvious, so I'd better explain: now that I've loaded the tibble package, the `southern_slam` data set will be interpreted as a tibble rather than a "vanilla" data frame. As a consequence, when I print the `southern_slam` data I get some slightly nicer looking output:

```{r}
southern_slam
```

So what is this data set anyway?

The `southern_slam` data set contains the results from 43 roller derby bouts that took place as part of the 2022 "Great Southern Slam" tournament held in Adelaide, Australia. I'll talk more about this data set later, but for now it's enough to note that we have columns specifying the names of the `home_team` and the `visitor_team`, columns storing the `home_score` and `visitor_score`, and another column called `margin` that contains the winning margin for each game. 

Let's have a look at the `margins` variable:

```{r}
southern_slam$margin
```

This output doesn't make it easy to get a sense of what the data are actually saying. Just "looking at the data" isn't a terribly effective way of understanding data. In order to get some idea about what's going on, we need to calculate some descriptive statistics (this chapter) and draw some nice pictures (Chapter \@ref(graphics). Since the descriptive statistics are the easier of the two topics, I'll start with those, but nevertheless I'll show you a histogram of the `southern_slam$margin` data, since it should help you get a sense of what the data we're trying to describe actually look like. We'll talk a lot more about how to draw histograms in Section \@ref(hist). For now, it's enough to look at the histogram and note that it provides a fairly interpretable representation of the margins data.

```{r}
#| label: southern_slam_margins
#| echo: false
ggplot2::ggplot(southern_slam, ggplot2::aes(margin)) +
	ggplot2::geom_histogram(breaks = seq(0, 600, 50)) + 
	ggplot2::labs(
		x = "Winning Margin",
		y = "Count"
	)
```

## Measures of central tendency {#centraltendency}

Drawing pictures of the data is an excellent way to convey the "gist" of what the data is trying to tell you. Another approach that can be helpful is to condense the data into a few simple "summary" statistics. In most situations, the first thing that you'll want to calculate is a measure of **central tendency**. That is, you'd like to know something about the "average" or "middle" of your data lies. The two most commonly used measures are the mean, median and mode; occasionally people will also report a trimmed mean. I'll explain each of these in turn, and then discuss when each of them is useful.

### The mean {#mean}

The **mean** of a set of observations is just a normal, old-fashioned average: add all of the values up, and then divide by the total number of values. The first five winning margins in the `southern_slam` data are 196, 51, 23, 106 and 219, so the mean of these observations is just:
$$
\frac{196 + 51 + 23 + 106 + 219}{5} = \frac{595}{5} = 119
$$
Of course, this definition of the mean isn't news to anyone: averages (i.e., means) are used so often in everyday life that this is pretty familiar stuff. However, since the concept of a mean is something that everyone already understands, I'll use this as an excuse to start introducing some of the mathematical notation that statisticians use to describe this calculation, and talk about how the calculations would be done in R. 

The first piece of notation to introduce is $N$, which we'll use to refer to the number of observations that we're averaging (in this case $N = 5$). Next, we need to attach a label to the observations themselves. It's traditional to use $X$ for this, and to use subscripts to indicate which observation we're actually talking about. That is, we'll use $X_1$ to refer to the first observation, $X_2$ to refer to the second observation, and so on, all the way up to $X_N$ for the last one. Or, to say the same thing in a slightly more abstract way, we use $X_i$ to refer to the $i$-th observation. Just to make sure we're clear on the notation, the following table lists the 5 observations in the `southern_slam$margins` variable, along with the mathematical symbol used to refer to it, and the actual value that the observation corresponds to:

| The observation        | Its symbol | The observed value |
|:-----------------------|:----------:|-------------------:|
| Winning margin, game 1 | $X_1$      |         196 points |
| Winning margin, game 2 | $X_2$      |          51 points |
| Winning margin, game 3 | $X_3$      |          23 points |
| Winning margin, game 4 | $X_4$      |         106 points |
| Winning margin, game 5 | $X_5$      |         219 points |


Okay, now let's try to write a formula for the mean. By tradition, we use $\bar{X}$ as the notation for the mean. So the calculation for the mean could be expressed using the following formula:
$$
\bar{X} = \frac{X_1 + X_2 + ... + X_{N-1} + X_N}{N}
$$
This formula is entirely correct, but it's terribly long, so we use a **summation symbol** -- written as $sum$ -- to shorten it.^[The choice to use $\sum$ to denote summation isn't entirely arbitrary. It's essentially identical the Greek upper case letter sigma, which is the analogue of the letter S. I do realise that "S is for sum" is a bit silly, but life is like that I'm afraid. Mathematical fonts will usually have some subtle differences between the summation symbol $\sum$ and the upper case sigma $\displaystyle\Sigma$, but as you can see those differences are minor. Along similar lines, there's a "product" symbol $\prod$ used to denote the multiplication of lots of numbers, and it very closely resembles the upper case Greek letter pi (the Greek analog of P). Again, there are usually minor differences in how they look in a font: $\prod$ is a product symbol, whereas $\displaystyle\Pi$ is the upper case pi... and oh my god this footnote is boring.] If I want to add up the first five observations, I could write out the sum the long way, $X_1 + X_2 + X_3 + X_4 +X_5$ or I could use the summation symbol to shorten it to this:
$$
\sum_{i=1}^5 X_i
$$
Taken literally, this could be read as "the sum, taken over all $i$ values from 1 to 5, of the value $X_i$". But basically, what it means is "add up the first five observations". In any case, we can use this notation to write out the formula for the mean, which looks like this:
$$
\bar{X} = \frac{1}{N} \sum_{i=1}^N X_i 
$$

In all honesty, I can't imagine that all this mathematical notation helps clarify the concept of the mean at all. In fact, it's really just a fancy way of writing out the same thing I said in words: add all the values up, and then divide by the total number of items. However, that's not really the reason I went into all that detail. My goal was to try to make sure that everyone reading this book is clear on the notation that we'll be using throughout the book: $\bar{X}$ for the mean, $\scriptstyle\sum$ for the idea of summation, $X_i$ for the $i$th observation, and $N$ for the total number of observations. We're going to be re-using these symbols a fair bit, so it's important that you understand them well enough to be able to "read" the equations, and to be able to see that it's just saying "add up lots of things and then divide by another thing".

### Calculating the mean in R

Okay that's the maths, how do we get the magic computing box to do the work for us? If you really wanted to, you could do this calculation directly in R. For the first `southern_slam` games, do this just by typing it in as if R were a calculator...

```{r}
(196 + 51 + 23 + 106 + 219) / 5
```

... in which case R outputs the answer `r sum(southern_slam$margin[1:5])`, just as if it were a calculator. However, that's not the only way to do the calculations, and when the number of observations starts to become large, it's easily the most tedious. Besides, in almost every real world scenario, you've already got the actual numbers stored in a variable of some kind, just like we have with the `afl.margins` variable. Under those circumstances, what you want is a function that will just add up all the values stored in a numeric vector. That's what the `sum()` function does. If we want to add up all `r nrow(southern_slam)` winning margins in the data set, we can do so using the following command:^[Note that, just as we saw with the combine function `c()` and the remove function `rm()`, the `sum()` function has unnamed arguments. I'll talk about unnamed arguments later in Section \@ref(dotsargument), but for now let's just ignore this detail.]

```{r}
sum(southern_slam$margin)
```

If we only want the sum of the first five observations, then we can use square brackets to pull out only the first five elements of the vector. So the command would now be: 

```{r}
sum(southern_slam$margin[1:5])
```

To calculate the mean, we now tell R to divide the output of this summation by five, so the command that we need to type now becomes the following:

```{r}
sum(southern_slam$margin[1:5]) / 5
```

Although it's pretty easy to calculate the mean using the `sum()` function, we can do it in an even easier way, since R also provides us with the `mean()` function. To calculate the mean for all 176 games, we would use the following command:

```{r}
mean(southern_slam$margin[1:5])
```

As you can see, this gives exactly the same answers as the previous calculations.

### The median {#median}

The second measure of central tendency that people use a lot is the **median**, and it's even easier to describe than the mean. The median of a set of observations is just the middle value. As before let's imagine we were interested only in the first five winning margins: 196, 51, 23, 106, and 219. To figure out the median, we sort these numbers into ascending order:

$$
23, 51, \mathbf{106}, 196, 219
$$

From inspection, it's obvious that the median value of these five observations is `r median(southern_slam$margin[1:5])`, since that's the middle one in the sorted list (I've put it in bold to make it even more obvious). Easy stuff. But what should we do if we were interested in the first *six* games rather than the first five? The sixth game in the tournament had a winning margin of 60 points, so our sorted list is now 
$$
23, 51, \mathbf{60}, \mathbf{106}, 196, 219 
$$

and there are *two* middle numbers, `r sort(southern_slam$margin[1:6])[3]` and `r sort(southern_slam$margin[1:6])[4]`. The median is defined as the average of those two numbers, which in this case is `r mean(sort(southern_slam$margin[1:6])[3:4])`. As before, it's very tedious to do this by hand when you've got lots of numbers. To illustrate this, here's what happens when you use R to sort all `r nrow(southern_slam)` winning margins. First, I'll use the `sort()` function (discussed in Chapter \@ref(datahandling)) to display the winning margins in increasing numerical order:

```{r}
sort(southern_slam$margin)
```

As we have `r nrow(southern_slam)` observations in our data, the median value corresponds to item `r (nrow(southern_slam) + 1)/2` in the sorted list of numbers. When we look up that observation in the list, we find that the median winning was `r sort(southern_slam$margin)[(nrow(southern_slam) + 1) / 2]`. In real life, of course, we don't actually calculate the median by sorting the data and then looking for the middle value by searching it manually. In real life, we use the `median()` command:

```{r}
median(southern_slam$margin)
```

Much nicer. 

### Mean or median? What's the difference?

Knowing how to calculate means and medians is only a part of the story. You also need to understand what each one is saying about the data, and what that implies for when you should use each one. The basic idea is illustrated in the figure below. The mean is basically the \"centre of gravity\" of the data set: if you imagine that the histogram of the data is a solid object, then the point on which you could balance it (as if on a see-saw) is the mean. In contrast, the median is the middle observation. Half of the observations are smaller, and half of the observations are larger. 

```{r}
#| label: mean-median-schematic
#| echo: false
knitr::include_graphics(here::here("images", "meanmedian.png"))
```

What this implies, as far as which one you should use, depends a little on what type of data you've got and what you're trying to achieve. As a rough guide:
 
- If your data are nominal scale, you probably shouldn't be using either the mean or the median. Both the mean and the median rely on the idea that the numbers assigned to values are meaningful. If the numbering scheme is arbitrary, then it's probably best to use the mode (Section \@ref(mode)) instead. 
- If your data are ordinal scale, you're more likely to want to use the median than the mean. The median only makes use of the order information in your data (i.e., which numbers are bigger), but doesn't depend on the precise numbers involved. That's exactly the situation that applies when your data are ordinal scale. The mean, on the other hand, makes use of the precise numeric values assigned to the observations, so it's not really appropriate for ordinal data.
- For interval and ratio scale data, either one is generally acceptable. Which one you pick depends a bit on what you're trying to achieve. The mean has the advantage that it uses all the information in the data (which is useful when you don't have a lot of data), but it's very sensitive to extreme values, as we'll see in Section \@ref(trimmedmean).  

Let's expand on that last part a little. One consequence is that there's systematic differences between the mean and the median when the histogram is asymmetric (skewed; see Section \@ref(skewandkurtosis)). This is illustrated in Figure \@ref(fig:meanmedian) notice that the median (right hand side) is located closer to the "body" of the histogram, whereas the mean (left hand side) gets dragged towards the "tail" (where the extreme values are).

To give a concrete example, suppose Bob (income \$50,000), Kate (income \$60,000) and Jane (income \$65,000) are sitting at a table: the average income at the table is \$58,333 and the median income is \$60,000. Then Bill sits down with them (income \$100,000,000). The average income has now jumped to \$25,043,750 but the median rises only to \$62,500. If you're interested in looking at the overall income at the table, the mean might be the right answer; but if you're interested in what counts as a typical income at the table, the median would be a better choice here.



### Trimmed mean {#trimmedmean} 

One of the fundamental rules of applied statistics is that the data are messy. Real life is never simple, and so the data sets that you obtain are never as straightforward as the statistical theory says.^[Or at least, the basic statistical theory -- these days there is a whole subfield of statistics called **robust statistics** that tries to grapple with the messiness of real data and develop theory that can cope with it.] This can have awkward consequences. To illustrate, consider this rather strange looking data set:

$$
-100, 2, 3, 4, 5, 6, 7, 8, 9, 10
$$

If you were to observe this in a real life data set, you'd probably suspect that something funny was going on with the $-100$ value. It's probably an **outlier**, a value that doesn't really belong with the others. You might consider removing it from the data set entirely, and in this particular case I'd probably agree with that course of action. In real life, however, you don't always get such cut-and-dried examples. For instance, you might get this instead:

$$
-15, 2, 3, 4, 5, 6, 7, 8, 9, 12
$$

The $-15$ looks a bit suspicious, but not anywhere near as much as that $-100$ did. In this case, it's a little trickier. It *might* be a legitimate observation, it might not.

When faced with a situation where some of the most extreme-valued observations might not be quite trustworthy, the mean is not necessarily a good measure of central tendency. It is highly sensitive to one or two extreme values, and is thus not considered to be a **robust** measure. One remedy that we've seen is to use the median. A more general solution is to use a "trimmed mean".  To calculate a trimmed mean, what you do is "discard" the most extreme examples on both ends (i.e., the largest and the smallest), and then take the mean of everything else. The goal is to preserve the best characteristics of the mean and the median: just like a median, you aren't highly influenced by extreme outliers, but like the mean, you "use" more than one of the observations. Generally, we describe a trimmed mean in terms of the percentage of observation on either side that are discarded. So, for instance, a 10% trimmed mean discards the largest 10% of the observations *and* the smallest 10% of the observations, and then takes the mean of the remaining 80% of the observations. Not surprisingly, the 0% trimmed mean is just the regular mean, and the 50% trimmed mean is the median. In that sense, trimmed means provide a whole family of central tendency measures that span the range from the mean to the median.

For our toy example above, we have 10 observations, and so a 10% trimmed mean is calculated by ignoring the largest value (i.e., 12) and the smallest value (i.e., -15) and taking the mean of the remaining values. First, let's enter the data:

```{r}
dataset <- c(-15, 2, 3, 4, 5, 6, 7, 8, 9, 12)
```

Next, let's calculate means and medians:

```{r}
mean(dataset)
median(dataset)
```

That's a fairly substantial difference, but I'm tempted to think that the mean is being influenced a bit too much by the extreme values at either end of the data set, especially the -15 one. So let's just try trimming the mean a bit. If I take a 10% trimmed mean, we'll drop the extreme values on either side, and take the mean of the rest: 

```{r}
mean(dataset, trim = .1)
```

which in this case gives exactly the same answer as the median. Note that, to get a 10% trimmed mean you write `trim = .1`, not `trim = 10`. In any case, let's finish up by calculating the 5% trimmed mean for the `southern_slam$margin` data, 

```{r}
mean(southern_slam$margin, trim = .05)  
```
 

### Mode {#mode}

The mode of a sample is very simple: it is the value that occurs most frequently. It's most often used for categorical variables, for which the mean and median don't really make sense. We'll use a different data set for this. The `archibald` data set supplied by the lsrbook package contains a table listing the winners of the Archibald Prize for Portraiture from 1921 to 2023. 

```{r}
archibald
```

There are actually three separate prizes included in this data set. In addition to the Archibald Prize itself, there's data for the Packing Room Prize (awarded by the staff who install the portraits in the gallery) and the People's Choice Award (voted on by the general public). Let's find the winners of the Archibald Prize only:

```{r}
archibald_winners <- archibald$artist[archibald$prize == "Archibald Prize"]
archibald_winners
```

Although the time period for the data spans 103 years, there are only 100 winners: no prizes were awarded in 1964 and 1980, and there was only a single combined prize awarded for the years 1991-1992.

We *could* read through all 100 entries, and count the number of occasions on which each artist appears in our list of winners, thereby producing a **frequency table**. However, that would be mindless and boring: exactly the sort of task that computers are great at. So let's use the `table()` function (discussed in more detail in Section \@ref(freqtables)) to do this task for us:
```{r}
archibald_counts <- table(archibald_winners)
archibald_counts
```

This is a bit hard to scan, so why don't we simplify it a bit? What I'll do is use the `sort()` function to arrange the table of winning artists order of frequency, and then show only the first few entries of the sorted table:

```{r}
sorted_archibald_counts <- sort(archibald_counts, decreasing = TRUE)
sorted_archibald_counts[1:4]
```

The winner of the most Archibald prizes in the 1921-2023 time frame of the data set was William Dargie. Or, to frame it in more statistical terms `"William Dargie"` is the modal value of the `archibald_winners` variable.



## Measures of variability {#var}

The statistics that we've discussed so far all relate to **central tendency**. That is, they all talk about which values are "in the middle" or "popular" in the data. However, central tendency is not the only type of summary statistic that we want to calculate. The second thing that we really want is a measure of the **variability** of the data. That is, how "spread out" are the data? How "far" away from the mean or median do the observed values tend to be? For now, let's assume that the data are interval or ratio scale, so we'll continue to use the `southern_slam` data.  We'll use this data to discuss several different measures of spread, each with different strengths and weaknesses. 

### Range {#range}

The **range** of a variable is very simple: it's the biggest value minus the smallest value. For the Southern Slam winning margins data, the maximum value is `r max(southern_slam$margin)`, and the minimum value is `r min(southern_slam$margin)`. We can calculate these values in R using the `max()` and `min()` functions:

```{r}
#| results: hide
max(southern_slam$margin)
min(southern_slam$margin)
```

where I've omitted the output because it's not interesting. The other possibility is to use the `range()` function; which outputs both the minimum value and the maximum value in a vector, like this:

```{r}
range(southern_slam$margin)
```

Although the range is the simplest way to quantify the notion of "variability", it's one of the worst. Recall from our discussion of the mean that we want our summary measure to be robust. If the data set has one or two extremely bad values in it, we'd like our statistics not to be unduly influenced by these cases. If we look once again at our toy example of a data set containing very extreme outliers...

$$
-100, 2, 3, 4, 5, 6, 7, 8, 9, 10
$$

...it is clear that the range is not robust, since this has a range of 110, but if the outlier were removed we would have a range of only 8.

### Interquartile range

The **interquartile range** is like the range, but instead of calculating the difference between the biggest and smallest value, it calculates the difference between the 25th quantile and the 75th quantile. Probably you already know what a **quantile** is (they're more commonly called percentiles), but if not: the 10th percentile of a data set is the smallest number $x$ such that 10% of the data is less than $x$. In fact, we've already come across the idea: the median of a data set is its 50th quantile / percentile! R actually provides you with a way of calculating quantiles, using the (surprise, surprise) `quantile()` function. Let's use it to calculate the median winning margin in the `southern_slam` data:

```{r}
quantile(southern_slam$margin, probs = .5)
```

Not surprisingly, this agrees with the answer that we saw earlier with the `median()` function. Now, we can actually input lots of quantiles at once, by specifying a vector for the `probs` argument. So lets do that, and get the 25th and 75th percentile:

```{r}
quantile(southern_slam$margin, probs = c(.25, .75))
```

And just like that, we have our interquartile range.^[There is also an `IQR()` function in R that will compute the difference between the 75th and 25th percentiles, and in earlier versions of this book I used it, but honestly the `quantile()` function does the job that needs doing so why complicate things?]

While it's obvious how to interpret the range, it's a little less obvious how to interpret the IQR. The simplest way to think about it is like this: the interquartile range is the range spanned by the "middle half" of the data. That is, one quarter of the data falls below the 25th percentile, one quarter of the data is above the 75th percentile, leaving the "middle half" of the data lying in between the two. And the IQR is the range covered by that middle half.

### Typical deviations from a typical value {#aad}

The two measures we've looked at so far both describe the spread of the data by examining the quantiles of the data. However, this isn't the only way to think about the problem. A different approach is to select a meaningful reference point to describe the "middle" of the data (i.e., a measure of central tendency like the mean or the median), and then ask how far away are the *actual* observations from this middle value?

This approach suggests the following approaches: if we decide that we're interested in averages, we could calculate the mean deviation from the mean value. Alternatively, if we want middle values, we could calcualte the median deviation from the median.^[To be honest, I don't see either of these measures used much in practice, but they're conceptually useful as stepping stones to understanding variance and standard deviations. (The median deviation from the mean is also a useful tool for constructing robust estimates for population standard deviations, but that's a very different topic)]

Since the previous paragraph might sound a little abstract, let's go through the **mean deviation from the mean** a little more slowly. One useful thing about this measure is that the name actually tells you exactly how to calculate it. Let's think about our AFL winning margins data, and once again we'll start by pretending that there's only 5 games in total, with winning margins of 56, 31, 56, 8 and 32. Since our calculations rely on an examination of the deviation from some reference point (in this case the mean), the first thing we need to calculate is the mean, $\bar{X}$. For these five observations, our mean is $\bar{X} = 36.6$. The next step is to convert each of our observations $X_i$ into a deviation score. We do this by calculating the difference between the observation $X_i$ and the mean $\bar{X}$. That is, the deviation score is defined to be $X_i - \bar{X}$. For the first observation in our sample, this is equal to $56 - 36.6 = 19.4$. Okay, that's simple enough. The next step in the process is to convert these deviations to absolute deviations. As we discussed earlier when talking about  the `abs()` function in R (Section \@ref(usingfunctions)), we do this by converting any negative values to positive ones. Mathematically, we would denote the absolute value of $-3$ as $|-3|$, and so we say that $|-3| = 3$. We use the absolute value function here because we don't really care whether the value is higher than the mean or lower than the mean, we're just interested in how *close* it is to the mean. To help make this process as obvious as possible, the table below shows these calculations for all five observations:

[Insert table, or possibly more helpfully, a figure]


Now that we have calculated the absolute deviation score for every observation in the data set, all that we have to do to calculate the mean of these scores. Let's do that:
$$
\frac{19.4 + 5.6 + 19.4 + 28.6 + 4.6}{5} = 15.52
$$
And we're done. The mean absolute deviation for these five scores is 15.52. 

For what it's worth, we can express this as a mathematical formula:
$$
\frac{1}{N} \sum_{i = 1}^N |X_i - \bar{X}|
$$

The last thing we need to talk about is how to calculate this in R. Doing it in a sequence of steps you get this:

```{r}
# some data!
value <- c(56, 31, 56, 8, 32)

# step 1: calculate the mean value
mean_value <- mean(value)

# step 2: calculate the absolute deviations from the mean
absolute_deviation <- abs(value - mean_value)

# step 3: calculate the average of these absolute deviations
average_absolute_deviation <- mean(absolute_deviation)

# profit!
average_absolute_deviation
```

But once you're comfortable writing R commands it's just easier to do this:

```{r}
mean(abs(value - mean(value)))
```

